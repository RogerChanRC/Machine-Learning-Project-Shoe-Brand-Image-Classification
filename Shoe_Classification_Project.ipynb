{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lHt1Lfe-kbpF",
        "outputId": "25e115ea-d3ef-4e60-ec52-fc55b2d99df4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "sLUZJTfTLpf4"
      },
      "outputs": [],
      "source": [
        "# ## Shoe Classification Project\n",
        "# ### Import Required Libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms, datasets, models\n",
        "from sklearn.model_selection import ParameterGrid\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import time\n",
        "import copy\n",
        "import os\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OO4YUpUzLrGM"
      },
      "outputs": [],
      "source": [
        "# ### Data Preprocessing and Augmentation\n",
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.RandomResizedCrop(224),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandomRotation(15),\n",
        "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'val': transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kRTbdhbALsri",
        "outputId": "55bc465b-5fe8-4da7-9cb9-9c6e1696ebed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classes confirmed: ['adidas', 'converse', 'nike']\n",
            "Training images: 603 (200 per class expected)\n",
            "Validation images: 90 (30 per class expected)\n"
          ]
        }
      ],
      "source": [
        "# ### Load Dataset with Custom Paths\n",
        "train_dir = '/content/drive/MyDrive/Nike_Adidas_converse_Shoes_image_dataset/train'\n",
        "val_dir = '/content/drive/MyDrive/Nike_Adidas_converse_Shoes_image_dataset/validate'\n",
        "\n",
        "image_datasets = {\n",
        "    'train': datasets.ImageFolder(train_dir, data_transforms['train']),\n",
        "    'val': datasets.ImageFolder(val_dir, data_transforms['val'])\n",
        "}\n",
        "\n",
        "dataloaders = {\n",
        "    'train': DataLoader(image_datasets['train'], batch_size=32, shuffle=True, num_workers=4),\n",
        "    'val': DataLoader(image_datasets['val'], batch_size=32, shuffle=False, num_workers=4)\n",
        "}\n",
        "\n",
        "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
        "class_names = image_datasets['train'].classes\n",
        "\n",
        "print(f\"Classes confirmed: {class_names}\")\n",
        "print(f\"Training images: {dataset_sizes['train']} (200 per class expected)\")\n",
        "print(f\"Validation images: {dataset_sizes['val']} (30 per class expected)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qvA3BZQFN_bd"
      },
      "outputs": [],
      "source": [
        "# ### Enhanced Model Configuration\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load pretrained EfficientNet\n",
        "model = models.efficientnet_b0(weights=models.EfficientNet_B0_Weights.IMAGENET1K_V1)\n",
        "num_ftrs = model.classifier[1].in_features\n",
        "\n",
        "# Modified classifier\n",
        "model.classifier = nn.Sequential(\n",
        "    nn.Dropout(p=0.3),\n",
        "    nn.Linear(num_ftrs, 256),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(256, len(class_names))\n",
        ")\n",
        "model = model.to(device)\n",
        "\n",
        "# Optimizer and scheduler with correct step count\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
        "\n",
        "# Calculate total steps explicitly\n",
        "num_epochs = 20\n",
        "steps_per_epoch = len(dataloaders['train'])\n",
        "total_steps = steps_per_epoch * num_epochs\n",
        "\n",
        "scheduler = optim.lr_scheduler.OneCycleLR(\n",
        "    optimizer,\n",
        "    max_lr=0.001,\n",
        "    total_steps=total_steps\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2K1YS6aq4-Zu",
        "outputId": "dbb81a18-3986-428b-c2d5-54ceade08d1b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "Train Loss: 1.0909 Acc: 0.3947\n",
            "Val Loss: 1.0639 Acc: 0.4889\n",
            "\n",
            "Epoch 2/20\n",
            "Train Loss: 1.0028 Acc: 0.6368\n",
            "Val Loss: 0.8017 Acc: 0.7667\n",
            "\n",
            "Epoch 3/20\n",
            "Train Loss: 0.6839 Acc: 0.7479\n",
            "Val Loss: 0.4040 Acc: 0.8111\n",
            "\n",
            "Epoch 4/20\n",
            "Train Loss: 0.4916 Acc: 0.7944\n",
            "Val Loss: 0.4471 Acc: 0.8556\n",
            "\n",
            "Epoch 5/20\n",
            "Train Loss: 0.4395 Acc: 0.8425\n",
            "Val Loss: 0.5538 Acc: 0.8111\n",
            "\n",
            "Epoch 6/20\n",
            "Train Loss: 0.4699 Acc: 0.8060\n",
            "Val Loss: 0.4632 Acc: 0.8778\n",
            "\n",
            "Epoch 7/20\n",
            "Train Loss: 0.3573 Acc: 0.8624\n",
            "Val Loss: 0.5904 Acc: 0.8333\n",
            "\n",
            "Epoch 8/20\n",
            "Train Loss: 0.4118 Acc: 0.8458\n",
            "Val Loss: 0.3700 Acc: 0.9000\n",
            "\n",
            "Epoch 9/20\n",
            "Train Loss: 0.3553 Acc: 0.8640\n",
            "Val Loss: 0.3219 Acc: 0.9111\n",
            "\n",
            "Epoch 10/20\n",
            "Train Loss: 0.3647 Acc: 0.8640\n",
            "Val Loss: 0.4190 Acc: 0.8556\n",
            "\n",
            "Epoch 11/20\n",
            "Train Loss: 0.2382 Acc: 0.9138\n",
            "Val Loss: 0.3656 Acc: 0.8889\n",
            "\n",
            "Epoch 12/20\n",
            "Train Loss: 0.2414 Acc: 0.8955\n",
            "Val Loss: 0.4133 Acc: 0.9222\n",
            "\n",
            "Epoch 13/20\n",
            "Train Loss: 0.2174 Acc: 0.9154\n",
            "Val Loss: 0.3825 Acc: 0.9111\n",
            "\n",
            "Epoch 14/20\n",
            "Train Loss: 0.2158 Acc: 0.9221\n",
            "Val Loss: 0.3456 Acc: 0.9111\n",
            "\n",
            "Epoch 15/20\n",
            "Train Loss: 0.1627 Acc: 0.9320\n",
            "Val Loss: 0.3948 Acc: 0.9000\n",
            "\n",
            "Epoch 16/20\n",
            "Train Loss: 0.1261 Acc: 0.9502\n",
            "Val Loss: 0.3295 Acc: 0.9222\n",
            "\n",
            "Epoch 17/20\n",
            "Train Loss: 0.1115 Acc: 0.9552\n",
            "Val Loss: 0.3092 Acc: 0.9222\n",
            "\n",
            "Epoch 18/20\n",
            "Train Loss: 0.1692 Acc: 0.9420\n",
            "Val Loss: 0.2925 Acc: 0.9000\n",
            "\n",
            "Epoch 19/20\n",
            "Train Loss: 0.1277 Acc: 0.9502\n",
            "Val Loss: 0.3037 Acc: 0.8778\n",
            "\n",
            "Epoch 20/20\n",
            "Train Loss: 0.1289 Acc: 0.9536\n",
            "Val Loss: 0.3074 Acc: 0.8778\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ### Training Execution\n",
        "def train_model(model, criterion, optimizer, scheduler, num_epochs=20):\n",
        "    best_acc = 0.0\n",
        "    train_losses, val_losses = [], []\n",
        "    train_accs, val_accs = [], []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}')\n",
        "\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        epoch_train_loss = 0.0\n",
        "        epoch_train_acc = 0.0\n",
        "        for inputs, labels in dataloaders['train']:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            epoch_train_loss += loss.item() * inputs.size(0)\n",
        "            epoch_train_acc += torch.sum(preds == labels.data)\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        epoch_val_loss = 0.0\n",
        "        epoch_val_acc = 0.0\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in dataloaders['val']:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "\n",
        "                epoch_val_loss += loss.item() * inputs.size(0)\n",
        "                epoch_val_acc += torch.sum(preds == labels.data)\n",
        "\n",
        "        # Calculate epoch metrics\n",
        "        train_loss = epoch_train_loss / dataset_sizes['train']\n",
        "        train_acc = epoch_train_acc.double() / dataset_sizes['train']\n",
        "        val_loss = epoch_val_loss / dataset_sizes['val']\n",
        "        val_acc = epoch_val_acc.double() / dataset_sizes['val']\n",
        "\n",
        "        # Store metrics\n",
        "        train_losses.append(train_loss)\n",
        "        train_accs.append(train_acc)\n",
        "        val_losses.append(val_loss)\n",
        "        val_accs.append(val_acc)\n",
        "\n",
        "        print(f'Train Loss: {train_loss:.4f} Acc: {train_acc:.4f}')\n",
        "        print(f'Val Loss: {val_loss:.4f} Acc: {val_acc:.4f}\\n')\n",
        "\n",
        "        # Save best model\n",
        "        if val_acc > best_acc:\n",
        "            best_acc = val_acc\n",
        "            torch.save(model.state_dict(), 'best_model.pth')\n",
        "\n",
        "    return model, train_losses, train_accs, val_losses, val_accs\n",
        "\n",
        "# Start training\n",
        "model, train_losses, train_accs, val_losses, val_accs = train_model(\n",
        "    model, criterion, optimizer, scheduler, num_epochs=20\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A6t2n8yjODcw",
        "outputId": "acb08c42-8e15-495c-b04b-6d0c41409a5d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Final Validation Accuracy: 92.22%\n"
          ]
        }
      ],
      "source": [
        "# ### Final Evaluation\n",
        "model.load_state_dict(torch.load('best_model.pth'))\n",
        "model.eval()\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in dataloaders['val']:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'\\nFinal Validation Accuracy: {100 * correct / total:.2f}%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 507
        },
        "id": "wfrCQsrr4RHF",
        "outputId": "527595cc-f28e-4407-bf3d-d786acedc2ea"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x500 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Plot training history\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Convert accuracy values to percentages\n",
        "train_accs = [acc.cpu().numpy() * 100 for acc in train_accs]\n",
        "val_accs = [acc.cpu().numpy() * 100 for acc in val_accs]\n",
        "\n",
        "# Loss curve\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(train_losses, label='Train Loss')\n",
        "plt.plot(val_losses, label='Val Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "# Accuracy curve\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(train_accs, label='Train Accuracy')\n",
        "plt.plot(val_accs, label='Val Accuracy')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "apXEyEWFmML1",
        "outputId": "ea5a5de6-6218-4b6b-914e-2eaafeb65726"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Hyperparameter Tuning...\n",
            "\n",
            "Testing parameters: {'batch_size': 16, 'dropout_rate': 0.1, 'epochs': 10, 'learning_rate': 0.1, 'weight_decay': 0}\n",
            "Epoch 1/10\n",
            "Train Loss: 1.2654 Acc: 0.3698\n",
            "Val Loss: 1651.0958 Acc: 0.3333\n",
            "\n",
            "Epoch 2/10\n",
            "Train Loss: 1.1233 Acc: 0.3499\n",
            "Val Loss: 4.3127 Acc: 0.3667\n",
            "\n",
            "Epoch 3/10\n",
            "Train Loss: 1.1049 Acc: 0.3549\n",
            "Val Loss: 1.1001 Acc: 0.3333\n",
            "\n",
            "Epoch 4/10\n",
            "Train Loss: 1.1065 Acc: 0.3134\n",
            "Val Loss: 1.1000 Acc: 0.3333\n",
            "\n",
            "Epoch 5/10\n",
            "Train Loss: 19.8067 Acc: 0.3217\n",
            "Val Loss: 740264.5361 Acc: 0.3444\n",
            "\n",
            "Epoch 6/10\n",
            "Train Loss: 1.1023 Acc: 0.3134\n",
            "Val Loss: 1.1006 Acc: 0.3333\n",
            "\n",
            "Epoch 7/10\n",
            "Train Loss: 1.1073 Acc: 0.2886\n",
            "Val Loss: 1.1001 Acc: 0.3333\n",
            "\n",
            "Epoch 8/10\n",
            "Train Loss: 1.1023 Acc: 0.3051\n",
            "Val Loss: 1.0990 Acc: 0.3333\n",
            "\n",
            "Epoch 9/10\n",
            "Train Loss: 1.1017 Acc: 0.3300\n",
            "Val Loss: 1.0994 Acc: 0.3333\n",
            "\n",
            "Epoch 10/10\n",
            "Train Loss: 1.0993 Acc: 0.3333\n",
            "Val Loss: 1.0991 Acc: 0.3333\n",
            "\n",
            "\n",
            "Testing parameters: {'batch_size': 16, 'dropout_rate': 0.1, 'epochs': 10, 'learning_rate': 0.1, 'weight_decay': 1e-05}\n",
            "Epoch 1/10\n",
            "Train Loss: 1.2129 Acc: 0.3648\n",
            "Val Loss: 98.9322 Acc: 0.3333\n",
            "\n",
            "Epoch 2/10\n",
            "Train Loss: 1.1148 Acc: 0.3151\n",
            "Val Loss: 1.1567 Acc: 0.3667\n",
            "\n",
            "Epoch 3/10\n",
            "Train Loss: 1.1043 Acc: 0.3234\n",
            "Val Loss: 1.1056 Acc: 0.3333\n",
            "\n",
            "Epoch 4/10\n",
            "Train Loss: 1.1044 Acc: 0.3118\n",
            "Val Loss: 1.1073 Acc: 0.3333\n",
            "\n",
            "Epoch 5/10\n",
            "Train Loss: 1.1050 Acc: 0.3217\n",
            "Val Loss: 1.0990 Acc: 0.3333\n",
            "\n",
            "Epoch 6/10\n",
            "Train Loss: 1.1038 Acc: 0.3367\n",
            "Val Loss: 1.1018 Acc: 0.3333\n",
            "\n",
            "Epoch 7/10\n",
            "Train Loss: 1.1020 Acc: 0.3300\n",
            "Val Loss: 1.0996 Acc: 0.3333\n",
            "\n",
            "Epoch 8/10\n",
            "Train Loss: 1.1004 Acc: 0.3151\n",
            "Val Loss: 1.0987 Acc: 0.3333\n",
            "\n",
            "Epoch 9/10\n",
            "Train Loss: 1.0994 Acc: 0.3217\n",
            "Val Loss: 1.0987 Acc: 0.3333\n",
            "\n",
            "Epoch 10/10\n",
            "Train Loss: 1.0987 Acc: 0.3333\n",
            "Val Loss: 1.0987 Acc: 0.3333\n",
            "\n",
            "\n",
            "Testing parameters: {'batch_size': 16, 'dropout_rate': 0.1, 'epochs': 10, 'learning_rate': 0.1, 'weight_decay': 0.0001}\n",
            "Epoch 1/10\n",
            "Train Loss: 1.1813 Acc: 0.3400\n",
            "Val Loss: 588.2764 Acc: 0.3333\n",
            "\n",
            "Epoch 2/10\n",
            "Train Loss: 1.4965 Acc: 0.3134\n",
            "Val Loss: 34699.8997 Acc: 0.3333\n",
            "\n",
            "Epoch 3/10\n",
            "Train Loss: 1.1166 Acc: 0.3134\n",
            "Val Loss: 1.1644 Acc: 0.3333\n",
            "\n",
            "Epoch 4/10\n",
            "Train Loss: 1.1050 Acc: 0.3184\n",
            "Val Loss: 1.1033 Acc: 0.3333\n",
            "\n",
            "Epoch 5/10\n",
            "Train Loss: 1.1106 Acc: 0.2836\n",
            "Val Loss: 1.0997 Acc: 0.3333\n",
            "\n",
            "Epoch 6/10\n",
            "Train Loss: 1.1077 Acc: 0.3201\n",
            "Val Loss: 1.1015 Acc: 0.3333\n",
            "\n",
            "Epoch 7/10\n",
            "Train Loss: 1.1018 Acc: 0.3284\n",
            "Val Loss: 1.0990 Acc: 0.3333\n",
            "\n",
            "Epoch 8/10\n",
            "Train Loss: 1.1113 Acc: 0.3333\n",
            "Val Loss: 1.0995 Acc: 0.3333\n",
            "\n",
            "Epoch 9/10\n",
            "Train Loss: 1.0991 Acc: 0.3267\n",
            "Val Loss: 1.0990 Acc: 0.3333\n",
            "\n",
            "Epoch 10/10\n",
            "Train Loss: 1.0990 Acc: 0.3333\n",
            "Val Loss: 1.0990 Acc: 0.3333\n",
            "\n",
            "\n",
            "Testing parameters: {'batch_size': 16, 'dropout_rate': 0.1, 'epochs': 10, 'learning_rate': 0.1, 'weight_decay': 0.001}\n",
            "Epoch 1/10\n",
            "Train Loss: 1.2175 Acc: 0.3648\n",
            "Val Loss: 1284.9981 Acc: 0.3333\n",
            "\n",
            "Epoch 2/10\n",
            "Train Loss: 1.1424 Acc: 0.3400\n",
            "Val Loss: 1.1007 Acc: 0.3333\n",
            "\n",
            "Epoch 3/10\n",
            "Train Loss: 1.1123 Acc: 0.3167\n",
            "Val Loss: 1.1044 Acc: 0.3333\n",
            "\n",
            "Epoch 4/10\n",
            "Train Loss: 1.1054 Acc: 0.3416\n",
            "Val Loss: 1.1071 Acc: 0.3333\n",
            "\n",
            "Epoch 5/10\n",
            "Train Loss: 1.1100 Acc: 0.2886\n",
            "Val Loss: 1.0994 Acc: 0.3333\n",
            "\n",
            "Epoch 6/10\n",
            "Train Loss: 1.1121 Acc: 0.3333\n",
            "Val Loss: 1.1043 Acc: 0.3333\n",
            "\n",
            "Epoch 7/10\n",
            "Train Loss: 1.1103 Acc: 0.3151\n",
            "Val Loss: 1.1018 Acc: 0.3333\n",
            "\n",
            "Epoch 8/10\n",
            "Train Loss: 1.1036 Acc: 0.3201\n",
            "Val Loss: 1.0987 Acc: 0.3333\n",
            "\n",
            "Epoch 9/10\n",
            "Train Loss: 1.0999 Acc: 0.3333\n",
            "Val Loss: 1.0988 Acc: 0.3333\n",
            "\n",
            "Epoch 10/10\n",
            "Train Loss: 1.0988 Acc: 0.3333\n",
            "Val Loss: 1.0987 Acc: 0.3333\n",
            "\n",
            "\n",
            "Testing parameters: {'batch_size': 16, 'dropout_rate': 0.1, 'epochs': 10, 'learning_rate': 0.01, 'weight_decay': 0}\n",
            "Epoch 1/10\n",
            "Train Loss: 0.9378 Acc: 0.5556\n",
            "Val Loss: 1.5873 Acc: 0.5222\n",
            "\n",
            "Epoch 2/10\n",
            "Train Loss: 1.0564 Acc: 0.5141\n",
            "Val Loss: 10.4182 Acc: 0.3333\n",
            "\n",
            "Epoch 3/10\n",
            "Train Loss: 1.1196 Acc: 0.3367\n",
            "Val Loss: 1.0983 Acc: 0.3556\n",
            "\n",
            "Epoch 4/10\n",
            "Train Loss: 1.0961 Acc: 0.3731\n",
            "Val Loss: 1.0891 Acc: 0.3889\n",
            "\n",
            "Epoch 5/10\n",
            "Train Loss: 1.1010 Acc: 0.3615\n",
            "Val Loss: 2.2410 Acc: 0.2778\n",
            "\n",
            "Epoch 6/10\n",
            "Train Loss: 1.0960 Acc: 0.3516\n",
            "Val Loss: 1.0641 Acc: 0.3889\n",
            "\n",
            "Epoch 7/10\n",
            "Train Loss: 1.1054 Acc: 0.3267\n",
            "Val Loss: 1.0926 Acc: 0.3444\n",
            "\n",
            "Epoch 8/10\n",
            "Train Loss: 1.0938 Acc: 0.3648\n",
            "Val Loss: 1.0575 Acc: 0.4222\n",
            "\n",
            "Epoch 9/10\n",
            "Train Loss: 1.0733 Acc: 0.4030\n",
            "Val Loss: 0.9555 Acc: 0.5778\n",
            "\n",
            "Epoch 10/10\n",
            "Train Loss: 1.0557 Acc: 0.4362\n",
            "Val Loss: 0.9508 Acc: 0.5556\n",
            "\n",
            "\n",
            "Testing parameters: {'batch_size': 16, 'dropout_rate': 0.1, 'epochs': 10, 'learning_rate': 0.01, 'weight_decay': 1e-05}\n",
            "Epoch 1/10\n",
            "Train Loss: 0.9469 Acc: 0.5987\n",
            "Val Loss: 1.8276 Acc: 0.6333\n",
            "\n",
            "Epoch 2/10\n",
            "Train Loss: 1.0874 Acc: 0.4610\n",
            "Val Loss: 12.7698 Acc: 0.4111\n",
            "\n",
            "Epoch 3/10\n",
            "Train Loss: 1.1398 Acc: 0.3582\n",
            "Val Loss: 1.0988 Acc: 0.3444\n",
            "\n",
            "Epoch 4/10\n",
            "Train Loss: 1.1005 Acc: 0.3184\n",
            "Val Loss: 1.0992 Acc: 0.3333\n",
            "\n",
            "Epoch 5/10\n",
            "Train Loss: 1.1054 Acc: 0.3400\n",
            "Val Loss: 1.0991 Acc: 0.3333\n",
            "\n",
            "Epoch 6/10\n",
            "Train Loss: 1.0999 Acc: 0.3317\n",
            "Val Loss: 1.0987 Acc: 0.3333\n",
            "\n",
            "Epoch 7/10\n",
            "Train Loss: 1.0989 Acc: 0.3333\n",
            "Val Loss: 1.0987 Acc: 0.3333\n",
            "\n",
            "Epoch 8/10\n",
            "Train Loss: 1.0988 Acc: 0.3134\n",
            "Val Loss: 1.0986 Acc: 0.3333\n",
            "\n",
            "Epoch 9/10\n",
            "Train Loss: 1.0987 Acc: 0.3333\n",
            "Val Loss: 1.0986 Acc: 0.3333\n",
            "\n",
            "Epoch 10/10\n",
            "Train Loss: 1.0986 Acc: 0.3333\n",
            "Val Loss: 1.0986 Acc: 0.3333\n",
            "\n",
            "\n",
            "Testing parameters: {'batch_size': 16, 'dropout_rate': 0.1, 'epochs': 10, 'learning_rate': 0.01, 'weight_decay': 0.0001}\n",
            "Epoch 1/10\n",
            "Train Loss: 0.9369 Acc: 0.5721\n",
            "Val Loss: 1.7610 Acc: 0.5667\n",
            "\n",
            "Epoch 2/10\n",
            "Train Loss: 1.0963 Acc: 0.4677\n",
            "Val Loss: 118.8743 Acc: 0.3333\n",
            "\n",
            "Epoch 3/10\n",
            "Train Loss: 1.1394 Acc: 0.3599\n",
            "Val Loss: 1.0961 Acc: 0.4000\n",
            "\n",
            "Epoch 4/10\n",
            "Train Loss: 1.1143 Acc: 0.3300\n",
            "Val Loss: 1.0966 Acc: 0.2889\n",
            "\n",
            "Epoch 5/10\n",
            "Train Loss: 1.1010 Acc: 0.3118\n",
            "Val Loss: 1.0993 Acc: 0.3333\n",
            "\n",
            "Epoch 6/10\n",
            "Train Loss: 1.0997 Acc: 0.3134\n",
            "Val Loss: 1.0969 Acc: 0.3333\n",
            "\n",
            "Epoch 7/10\n",
            "Train Loss: 1.0997 Acc: 0.2886\n",
            "Val Loss: 1.0986 Acc: 0.3333\n",
            "\n",
            "Epoch 8/10\n",
            "Train Loss: 1.1009 Acc: 0.3400\n",
            "Val Loss: 1.0958 Acc: 0.3778\n",
            "\n",
            "Epoch 9/10\n",
            "Train Loss: 1.0996 Acc: 0.3151\n",
            "Val Loss: 1.0956 Acc: 0.4000\n",
            "\n",
            "Epoch 10/10\n",
            "Train Loss: 1.0983 Acc: 0.3648\n",
            "Val Loss: 1.0951 Acc: 0.3889\n",
            "\n",
            "\n",
            "Testing parameters: {'batch_size': 16, 'dropout_rate': 0.1, 'epochs': 10, 'learning_rate': 0.01, 'weight_decay': 0.001}\n",
            "Epoch 1/10\n",
            "Train Loss: 0.8918 Acc: 0.5970\n",
            "Val Loss: 1.0678 Acc: 0.6778\n",
            "\n",
            "Epoch 2/10\n",
            "Train Loss: 1.1186 Acc: 0.4594\n",
            "Val Loss: 240.8207 Acc: 0.3333\n",
            "\n",
            "Epoch 3/10\n",
            "Train Loss: 1.1289 Acc: 0.3317\n",
            "Val Loss: 1.2430 Acc: 0.3778\n",
            "\n",
            "Epoch 4/10\n",
            "Train Loss: 1.1011 Acc: 0.3350\n",
            "Val Loss: 1.0961 Acc: 0.3333\n",
            "\n",
            "Epoch 5/10\n",
            "Train Loss: 1.1005 Acc: 0.3151\n",
            "Val Loss: 1.0943 Acc: 0.4222\n",
            "\n",
            "Epoch 6/10\n",
            "Train Loss: 1.1012 Acc: 0.3350\n",
            "Val Loss: 1.1067 Acc: 0.3333\n",
            "\n",
            "Epoch 7/10\n",
            "Train Loss: 1.0938 Acc: 0.3648\n",
            "Val Loss: 1.0832 Acc: 0.4000\n",
            "\n",
            "Epoch 8/10\n",
            "Train Loss: 1.0962 Acc: 0.3499\n",
            "Val Loss: 1.0935 Acc: 0.3778\n",
            "\n",
            "Epoch 9/10\n",
            "Train Loss: 1.0875 Acc: 0.3715\n",
            "Val Loss: 1.0912 Acc: 0.3667\n",
            "\n",
            "Epoch 10/10\n",
            "Train Loss: 1.0862 Acc: 0.3765\n",
            "Val Loss: 1.0858 Acc: 0.3778\n",
            "\n",
            "\n",
            "Testing parameters: {'batch_size': 16, 'dropout_rate': 0.1, 'epochs': 10, 'learning_rate': 0.001, 'weight_decay': 0}\n",
            "Epoch 1/10\n",
            "Train Loss: 1.0685 Acc: 0.4478\n",
            "Val Loss: 0.9252 Acc: 0.7667\n",
            "\n",
            "Epoch 2/10\n",
            "Train Loss: 0.7713 Acc: 0.6683\n",
            "Val Loss: 0.4840 Acc: 0.8222\n",
            "\n",
            "Epoch 3/10\n",
            "Train Loss: 0.7450 Acc: 0.6965\n",
            "Val Loss: 0.7134 Acc: 0.6889\n",
            "\n",
            "Epoch 4/10\n",
            "Train Loss: 0.6380 Acc: 0.7330\n",
            "Val Loss: 0.5404 Acc: 0.8111\n",
            "\n",
            "Epoch 5/10\n",
            "Train Loss: 0.5084 Acc: 0.7811\n",
            "Val Loss: 0.3854 Acc: 0.8778\n",
            "\n",
            "Epoch 6/10\n",
            "Train Loss: 0.4106 Acc: 0.8441\n",
            "Val Loss: 0.4231 Acc: 0.8222\n",
            "\n",
            "Epoch 7/10\n",
            "Train Loss: 0.3501 Acc: 0.8690\n",
            "Val Loss: 0.3937 Acc: 0.9111\n",
            "\n",
            "Epoch 8/10\n",
            "Train Loss: 0.3547 Acc: 0.8624\n",
            "Val Loss: 0.4325 Acc: 0.8778\n",
            "\n",
            "Epoch 9/10\n",
            "Train Loss: 0.3068 Acc: 0.8756\n",
            "Val Loss: 0.4121 Acc: 0.8556\n",
            "\n",
            "Epoch 10/10\n",
            "Train Loss: 0.2522 Acc: 0.9088\n",
            "Val Loss: 0.3745 Acc: 0.8556\n",
            "\n",
            "\n",
            "Testing parameters: {'batch_size': 16, 'dropout_rate': 0.1, 'epochs': 10, 'learning_rate': 0.001, 'weight_decay': 1e-05}\n",
            "Epoch 1/10\n",
            "Train Loss: 1.0686 Acc: 0.4876\n",
            "Val Loss: 0.8884 Acc: 0.6333\n",
            "\n",
            "Epoch 2/10\n",
            "Train Loss: 0.7196 Acc: 0.6866\n",
            "Val Loss: 0.8858 Acc: 0.7222\n",
            "\n",
            "Epoch 3/10\n",
            "Train Loss: 0.7068 Acc: 0.7214\n",
            "Val Loss: 0.3745 Acc: 0.8667\n",
            "\n",
            "Epoch 4/10\n",
            "Train Loss: 0.6515 Acc: 0.7380\n",
            "Val Loss: 0.5429 Acc: 0.7556\n",
            "\n",
            "Epoch 5/10\n",
            "Train Loss: 0.5811 Acc: 0.7811\n",
            "Val Loss: 0.3557 Acc: 0.8778\n",
            "\n",
            "Epoch 6/10\n",
            "Train Loss: 0.4692 Acc: 0.8209\n",
            "Val Loss: 0.3370 Acc: 0.8333\n",
            "\n",
            "Epoch 7/10\n",
            "Train Loss: 0.3422 Acc: 0.8706\n",
            "Val Loss: 0.3079 Acc: 0.8778\n",
            "\n",
            "Epoch 8/10\n",
            "Train Loss: 0.2701 Acc: 0.8905\n",
            "Val Loss: 0.2820 Acc: 0.8667\n",
            "\n",
            "Epoch 9/10\n",
            "Train Loss: 0.2958 Acc: 0.8972\n",
            "Val Loss: 0.3080 Acc: 0.8778\n",
            "\n",
            "Epoch 10/10\n",
            "Train Loss: 0.2277 Acc: 0.9154\n",
            "Val Loss: 0.3147 Acc: 0.8889\n",
            "\n",
            "\n",
            "Testing parameters: {'batch_size': 16, 'dropout_rate': 0.1, 'epochs': 10, 'learning_rate': 0.001, 'weight_decay': 0.0001}\n",
            "Epoch 1/10\n",
            "Train Loss: 1.0681 Acc: 0.4527\n",
            "Val Loss: 0.9293 Acc: 0.7000\n",
            "\n",
            "Epoch 2/10\n",
            "Train Loss: 0.7192 Acc: 0.7396\n",
            "Val Loss: 0.6577 Acc: 0.7444\n",
            "\n",
            "Epoch 3/10\n",
            "Train Loss: 0.7536 Acc: 0.6965\n",
            "Val Loss: 0.6000 Acc: 0.7333\n",
            "\n",
            "Epoch 4/10\n",
            "Train Loss: 0.5945 Acc: 0.7562\n",
            "Val Loss: 0.4758 Acc: 0.8111\n",
            "\n",
            "Epoch 5/10\n",
            "Train Loss: 0.5197 Acc: 0.7844\n",
            "Val Loss: 0.3951 Acc: 0.8111\n",
            "\n",
            "Epoch 6/10\n",
            "Train Loss: 0.4620 Acc: 0.8043\n",
            "Val Loss: 0.4870 Acc: 0.8222\n",
            "\n",
            "Epoch 7/10\n",
            "Train Loss: 0.4331 Acc: 0.8325\n",
            "Val Loss: 0.3552 Acc: 0.8222\n",
            "\n",
            "Epoch 8/10\n",
            "Train Loss: 0.2763 Acc: 0.8972\n",
            "Val Loss: 0.3775 Acc: 0.8667\n",
            "\n",
            "Epoch 9/10\n",
            "Train Loss: 0.2646 Acc: 0.9071\n",
            "Val Loss: 0.2989 Acc: 0.8778\n",
            "\n",
            "Epoch 10/10\n",
            "Train Loss: 0.2060 Acc: 0.9171\n",
            "Val Loss: 0.3424 Acc: 0.8667\n",
            "\n",
            "\n",
            "Testing parameters: {'batch_size': 16, 'dropout_rate': 0.1, 'epochs': 10, 'learning_rate': 0.001, 'weight_decay': 0.001}\n",
            "Epoch 1/10\n",
            "Train Loss: 1.0719 Acc: 0.4643\n",
            "Val Loss: 0.9280 Acc: 0.8000\n",
            "\n",
            "Epoch 2/10\n",
            "Train Loss: 0.7879 Acc: 0.6799\n",
            "Val Loss: 0.5422 Acc: 0.8111\n",
            "\n",
            "Epoch 3/10\n",
            "Train Loss: 0.7235 Acc: 0.6949\n",
            "Val Loss: 0.4311 Acc: 0.8333\n",
            "\n",
            "Epoch 4/10\n",
            "Train Loss: 0.6588 Acc: 0.7280\n",
            "Val Loss: 0.4924 Acc: 0.8000\n",
            "\n",
            "Epoch 5/10\n",
            "Train Loss: 0.5228 Acc: 0.7977\n",
            "Val Loss: 0.3782 Acc: 0.8556\n",
            "\n",
            "Epoch 6/10\n",
            "Train Loss: 0.4680 Acc: 0.8027\n",
            "Val Loss: 0.4466 Acc: 0.8222\n",
            "\n",
            "Epoch 7/10\n",
            "Train Loss: 0.3991 Acc: 0.8474\n",
            "Val Loss: 0.2831 Acc: 0.9000\n",
            "\n",
            "Epoch 8/10\n",
            "Train Loss: 0.2517 Acc: 0.9022\n",
            "Val Loss: 0.3385 Acc: 0.8778\n",
            "\n",
            "Epoch 9/10\n",
            "Train Loss: 0.2982 Acc: 0.8955\n",
            "Val Loss: 0.2656 Acc: 0.9000\n",
            "\n",
            "Epoch 10/10\n",
            "Train Loss: 0.2287 Acc: 0.9154\n",
            "Val Loss: 0.2858 Acc: 0.8889\n",
            "\n",
            "\n",
            "Testing parameters: {'batch_size': 16, 'dropout_rate': 0.3, 'epochs': 10, 'learning_rate': 0.1, 'weight_decay': 0}\n",
            "Epoch 1/10\n",
            "Train Loss: 1.2357 Acc: 0.3831\n",
            "Val Loss: 1524.5308 Acc: 0.3333\n",
            "\n",
            "Epoch 2/10\n",
            "Train Loss: 1.1183 Acc: 0.3317\n",
            "Val Loss: 1.0994 Acc: 0.3333\n",
            "\n",
            "Epoch 3/10\n",
            "Train Loss: 1.1070 Acc: 0.3300\n",
            "Val Loss: 1.1137 Acc: 0.3333\n",
            "\n",
            "Epoch 4/10\n",
            "Train Loss: 1.1067 Acc: 0.2786\n",
            "Val Loss: 1.0989 Acc: 0.3333\n",
            "\n",
            "Epoch 5/10\n",
            "Train Loss: 1.1038 Acc: 0.3267\n",
            "Val Loss: 1.0995 Acc: 0.3333\n",
            "\n",
            "Epoch 6/10\n",
            "Train Loss: 1.1043 Acc: 0.3085\n",
            "Val Loss: 1.1031 Acc: 0.3333\n",
            "\n",
            "Epoch 7/10\n",
            "Train Loss: 1.1011 Acc: 0.3300\n",
            "Val Loss: 1.0987 Acc: 0.3333\n",
            "\n",
            "Epoch 8/10\n",
            "Train Loss: 1.1035 Acc: 0.3217\n",
            "Val Loss: 1.0988 Acc: 0.3333\n",
            "\n",
            "Epoch 9/10\n",
            "Train Loss: 1.1007 Acc: 0.3333\n",
            "Val Loss: 1.0995 Acc: 0.3333\n",
            "\n",
            "Epoch 10/10\n",
            "Train Loss: 1.0993 Acc: 0.3333\n",
            "Val Loss: 1.0992 Acc: 0.3333\n",
            "\n",
            "\n",
            "Testing parameters: {'batch_size': 16, 'dropout_rate': 0.3, 'epochs': 10, 'learning_rate': 0.1, 'weight_decay': 1e-05}\n",
            "Epoch 1/10\n",
            "Train Loss: 1.2159 Acc: 0.3665\n",
            "Val Loss: 547.4725 Acc: 0.3333\n",
            "\n",
            "Epoch 2/10\n",
            "Train Loss: 1.1425 Acc: 0.3566\n",
            "Val Loss: 1.4037 Acc: 0.2667\n",
            "\n",
            "Epoch 3/10\n",
            "Train Loss: 1.1065 Acc: 0.2985\n",
            "Val Loss: 1.1125 Acc: 0.3333\n",
            "\n",
            "Epoch 4/10\n",
            "Train Loss: 1.1063 Acc: 0.3416\n",
            "Val Loss: 1.1210 Acc: 0.3333\n",
            "\n",
            "Epoch 5/10\n",
            "Train Loss: 1.1119 Acc: 0.3217\n",
            "Val Loss: 1.1075 Acc: 0.3333\n",
            "\n",
            "Epoch 6/10\n",
            "Train Loss: 1.1012 Acc: 0.3217\n",
            "Val Loss: 1.1006 Acc: 0.3333\n",
            "\n",
            "Epoch 7/10\n",
            "Train Loss: 1.1095 Acc: 0.3018\n",
            "Val Loss: 1.0987 Acc: 0.3333\n",
            "\n",
            "Epoch 8/10\n",
            "Train Loss: 1.1012 Acc: 0.3085\n",
            "Val Loss: 1.0993 Acc: 0.3333\n",
            "\n",
            "Epoch 9/10\n",
            "Train Loss: 1.0998 Acc: 0.3333\n",
            "Val Loss: 1.0990 Acc: 0.3333\n",
            "\n",
            "Epoch 10/10\n",
            "Train Loss: 1.0990 Acc: 0.3333\n",
            "Val Loss: 1.0989 Acc: 0.3333\n",
            "\n",
            "\n",
            "Testing parameters: {'batch_size': 16, 'dropout_rate': 0.3, 'epochs': 10, 'learning_rate': 0.1, 'weight_decay': 0.0001}\n",
            "Epoch 1/10\n",
            "Train Loss: 1.2022 Acc: 0.3765\n",
            "Val Loss: 114.4590 Acc: 0.3556\n",
            "\n",
            "Epoch 2/10\n",
            "Train Loss: 1.1202 Acc: 0.3101\n",
            "Val Loss: 1.0994 Acc: 0.3333\n",
            "\n",
            "Epoch 3/10\n",
            "Train Loss: 1.1085 Acc: 0.3234\n",
            "Val Loss: 1.1005 Acc: 0.3333\n",
            "\n",
            "Epoch 4/10\n",
            "Train Loss: 1.1073 Acc: 0.3201\n",
            "Val Loss: 1.1056 Acc: 0.3333\n",
            "\n",
            "Epoch 5/10\n",
            "Train Loss: 1.1058 Acc: 0.3151\n",
            "Val Loss: 1.0988 Acc: 0.3333\n",
            "\n",
            "Epoch 6/10\n",
            "Train Loss: 1.1013 Acc: 0.3284\n",
            "Val Loss: 1.1002 Acc: 0.3333\n",
            "\n",
            "Epoch 7/10\n",
            "Train Loss: 1.1077 Acc: 0.2985\n",
            "Val Loss: 1.1017 Acc: 0.3333\n",
            "\n",
            "Epoch 8/10\n",
            "Train Loss: 1.1027 Acc: 0.3002\n",
            "Val Loss: 1.0987 Acc: 0.3333\n",
            "\n",
            "Epoch 9/10\n",
            "Train Loss: 1.1009 Acc: 0.3118\n",
            "Val Loss: 1.0992 Acc: 0.3333\n",
            "\n",
            "Epoch 10/10\n",
            "Train Loss: 1.0991 Acc: 0.3333\n",
            "Val Loss: 1.0989 Acc: 0.3333\n",
            "\n",
            "\n",
            "Testing parameters: {'batch_size': 16, 'dropout_rate': 0.3, 'epochs': 10, 'learning_rate': 0.1, 'weight_decay': 0.001}\n",
            "Epoch 1/10\n",
            "Train Loss: 1.2853 Acc: 0.3416\n",
            "Val Loss: 654.3980 Acc: 0.3333\n",
            "\n",
            "Epoch 2/10\n",
            "Train Loss: 1.1203 Acc: 0.3217\n",
            "Val Loss: 1.0901 Acc: 0.3333\n",
            "\n",
            "Epoch 3/10\n",
            "Train Loss: 1.1101 Acc: 0.2886\n",
            "Val Loss: 1.1097 Acc: 0.3333\n",
            "\n",
            "Epoch 4/10\n",
            "Train Loss: 1.1045 Acc: 0.3499\n",
            "Val Loss: 1.1014 Acc: 0.3333\n",
            "\n",
            "Epoch 5/10\n",
            "Train Loss: 1.1041 Acc: 0.3217\n",
            "Val Loss: 1.1023 Acc: 0.3333\n",
            "\n",
            "Epoch 6/10\n",
            "Train Loss: 1.1069 Acc: 0.3118\n",
            "Val Loss: 1.1018 Acc: 0.3333\n",
            "\n",
            "Epoch 7/10\n",
            "Train Loss: 1.1034 Acc: 0.3051\n",
            "Val Loss: 1.0986 Acc: 0.3333\n",
            "\n",
            "Epoch 8/10\n",
            "Train Loss: 1.1008 Acc: 0.3035\n",
            "Val Loss: 1.0990 Acc: 0.3333\n",
            "\n",
            "Epoch 9/10\n",
            "Train Loss: 1.0997 Acc: 0.3333\n",
            "Val Loss: 1.0987 Acc: 0.3333\n",
            "\n",
            "Epoch 10/10\n",
            "Train Loss: 1.0988 Acc: 0.3333\n",
            "Val Loss: 1.0986 Acc: 0.3333\n",
            "\n",
            "\n",
            "Testing parameters: {'batch_size': 16, 'dropout_rate': 0.3, 'epochs': 10, 'learning_rate': 0.01, 'weight_decay': 0}\n",
            "Epoch 1/10\n",
            "Train Loss: 0.9494 Acc: 0.5572\n",
            "Val Loss: 1.1571 Acc: 0.5111\n",
            "\n",
            "Epoch 2/10\n",
            "Train Loss: 1.0480 Acc: 0.4428\n",
            "Val Loss: 2.2233 Acc: 0.3222\n",
            "\n",
            "Epoch 3/10\n",
            "Train Loss: 1.1326 Acc: 0.3516\n",
            "Val Loss: 1.1016 Acc: 0.3333\n",
            "\n",
            "Epoch 4/10\n",
            "Train Loss: 1.1148 Acc: 0.3416\n",
            "Val Loss: 1.0991 Acc: 0.3111\n",
            "\n",
            "Epoch 5/10\n",
            "Train Loss: 1.1034 Acc: 0.3068\n",
            "Val Loss: 1.0989 Acc: 0.3333\n",
            "\n",
            "Epoch 6/10\n",
            "Train Loss: 1.0997 Acc: 0.3234\n",
            "Val Loss: 1.0987 Acc: 0.3333\n",
            "\n",
            "Epoch 7/10\n",
            "Train Loss: 1.0997 Acc: 0.3002\n",
            "Val Loss: 1.0987 Acc: 0.3333\n",
            "\n",
            "Epoch 8/10\n",
            "Train Loss: 1.0989 Acc: 0.3333\n",
            "Val Loss: 1.0986 Acc: 0.3333\n",
            "\n",
            "Epoch 9/10\n",
            "Train Loss: 1.0987 Acc: 0.3101\n",
            "Val Loss: 1.0986 Acc: 0.3333\n",
            "\n",
            "Epoch 10/10\n",
            "Train Loss: 1.0986 Acc: 0.3333\n",
            "Val Loss: 1.0986 Acc: 0.3333\n",
            "\n",
            "\n",
            "Testing parameters: {'batch_size': 16, 'dropout_rate': 0.3, 'epochs': 10, 'learning_rate': 0.01, 'weight_decay': 1e-05}\n",
            "Epoch 1/10\n",
            "Train Loss: 0.9430 Acc: 0.5589\n",
            "Val Loss: 3.2177 Acc: 0.4111\n",
            "\n",
            "Epoch 2/10\n",
            "Train Loss: 1.0156 Acc: 0.5323\n",
            "Val Loss: 1.9083 Acc: 0.4000\n",
            "\n",
            "Epoch 3/10\n",
            "Train Loss: 1.1361 Acc: 0.3317\n",
            "Val Loss: 1.3038 Acc: 0.4111\n",
            "\n",
            "Epoch 4/10\n",
            "Train Loss: 1.1023 Acc: 0.3698\n",
            "Val Loss: 1.1076 Acc: 0.3111\n",
            "\n",
            "Epoch 5/10\n",
            "Train Loss: 1.1049 Acc: 0.3615\n",
            "Val Loss: 1.1024 Acc: 0.3556\n",
            "\n",
            "Epoch 6/10\n",
            "Train Loss: 1.0842 Acc: 0.4013\n",
            "Val Loss: 1.0904 Acc: 0.4667\n",
            "\n",
            "Epoch 7/10\n",
            "Train Loss: 1.0806 Acc: 0.4113\n",
            "Val Loss: 1.0100 Acc: 0.4778\n",
            "\n",
            "Epoch 8/10\n",
            "Train Loss: 0.9774 Acc: 0.5207\n",
            "Val Loss: 0.9040 Acc: 0.5556\n",
            "\n",
            "Epoch 9/10\n",
            "Train Loss: 0.9438 Acc: 0.5274\n",
            "Val Loss: 0.7758 Acc: 0.6444\n",
            "\n",
            "Epoch 10/10\n",
            "Train Loss: 0.8715 Acc: 0.5837\n",
            "Val Loss: 0.7721 Acc: 0.6778\n",
            "\n",
            "\n",
            "Testing parameters: {'batch_size': 16, 'dropout_rate': 0.3, 'epochs': 10, 'learning_rate': 0.01, 'weight_decay': 0.0001}\n",
            "Epoch 1/10\n",
            "Train Loss: 0.9352 Acc: 0.5406\n",
            "Val Loss: 1.3794 Acc: 0.6222\n",
            "\n",
            "Epoch 2/10\n",
            "Train Loss: 1.1003 Acc: 0.4627\n",
            "Val Loss: 1.7701 Acc: 0.3778\n",
            "\n",
            "Epoch 3/10\n",
            "Train Loss: 1.1096 Acc: 0.3400\n",
            "Val Loss: 1.3848 Acc: 0.4111\n",
            "\n",
            "Epoch 4/10\n",
            "Train Loss: 1.1369 Acc: 0.3798\n",
            "Val Loss: 1.0997 Acc: 0.3333\n",
            "\n",
            "Epoch 5/10\n",
            "Train Loss: 1.1018 Acc: 0.3267\n",
            "Val Loss: 1.0989 Acc: 0.3333\n",
            "\n",
            "Epoch 6/10\n",
            "Train Loss: 1.0997 Acc: 0.3333\n",
            "Val Loss: 1.0988 Acc: 0.3333\n",
            "\n",
            "Epoch 7/10\n",
            "Train Loss: 1.0995 Acc: 0.3051\n",
            "Val Loss: 1.0987 Acc: 0.3333\n",
            "\n",
            "Epoch 8/10\n",
            "Train Loss: 1.0989 Acc: 0.3284\n",
            "Val Loss: 1.0986 Acc: 0.3333\n",
            "\n",
            "Epoch 9/10\n",
            "Train Loss: 1.0988 Acc: 0.3333\n",
            "Val Loss: 1.0986 Acc: 0.3333\n",
            "\n",
            "Epoch 10/10\n",
            "Train Loss: 1.0986 Acc: 0.3333\n",
            "Val Loss: 1.0986 Acc: 0.3333\n",
            "\n",
            "\n",
            "Testing parameters: {'batch_size': 16, 'dropout_rate': 0.3, 'epochs': 10, 'learning_rate': 0.01, 'weight_decay': 0.001}\n",
            "Epoch 1/10\n",
            "Train Loss: 0.8979 Acc: 0.5837\n",
            "Val Loss: 0.8135 Acc: 0.7444\n",
            "\n",
            "Epoch 2/10\n",
            "Train Loss: 1.0748 Acc: 0.4411\n",
            "Val Loss: 4.2137 Acc: 0.3778\n",
            "\n",
            "Epoch 3/10\n",
            "Train Loss: 1.1311 Acc: 0.3151\n",
            "Val Loss: 1.1157 Acc: 0.3444\n",
            "\n",
            "Epoch 4/10\n",
            "Train Loss: 1.1065 Acc: 0.3765\n",
            "Val Loss: 1.1099 Acc: 0.3000\n",
            "\n",
            "Epoch 5/10\n",
            "Train Loss: 1.1000 Acc: 0.3068\n",
            "Val Loss: 1.0877 Acc: 0.3333\n",
            "\n",
            "Epoch 6/10\n",
            "Train Loss: 1.0996 Acc: 0.3284\n",
            "Val Loss: 1.0880 Acc: 0.3667\n",
            "\n",
            "Epoch 7/10\n",
            "Train Loss: 1.0935 Acc: 0.3814\n",
            "Val Loss: 1.0710 Acc: 0.4556\n",
            "\n",
            "Epoch 8/10\n",
            "Train Loss: 1.0978 Acc: 0.3698\n",
            "Val Loss: 1.0790 Acc: 0.4556\n",
            "\n",
            "Epoch 9/10\n",
            "Train Loss: 1.0893 Acc: 0.3731\n",
            "Val Loss: 1.0444 Acc: 0.4333\n",
            "\n",
            "Epoch 10/10\n",
            "Train Loss: 1.0830 Acc: 0.3897\n",
            "Val Loss: 1.0373 Acc: 0.4444\n",
            "\n",
            "\n",
            "Testing parameters: {'batch_size': 16, 'dropout_rate': 0.3, 'epochs': 10, 'learning_rate': 0.001, 'weight_decay': 0}\n",
            "Epoch 1/10\n",
            "Train Loss: 1.0828 Acc: 0.4295\n",
            "Val Loss: 0.9883 Acc: 0.6444\n",
            "\n",
            "Epoch 2/10\n",
            "Train Loss: 0.8219 Acc: 0.6866\n",
            "Val Loss: 0.9636 Acc: 0.6333\n",
            "\n",
            "Epoch 3/10\n",
            "Train Loss: 0.7402 Acc: 0.6915\n",
            "Val Loss: 0.5706 Acc: 0.7111\n",
            "\n",
            "Epoch 4/10\n",
            "Train Loss: 0.5909 Acc: 0.7844\n",
            "Val Loss: 0.7241 Acc: 0.7889\n",
            "\n",
            "Epoch 5/10\n",
            "Train Loss: 0.5484 Acc: 0.7993\n",
            "Val Loss: 0.4733 Acc: 0.8222\n",
            "\n",
            "Epoch 6/10\n",
            "Train Loss: 0.4614 Acc: 0.8192\n",
            "Val Loss: 0.3388 Acc: 0.8444\n",
            "\n",
            "Epoch 7/10\n",
            "Train Loss: 0.3778 Acc: 0.8557\n",
            "Val Loss: 0.2718 Acc: 0.9111\n",
            "\n",
            "Epoch 8/10\n",
            "Train Loss: 0.3330 Acc: 0.8872\n",
            "Val Loss: 0.2833 Acc: 0.9000\n",
            "\n",
            "Epoch 9/10\n",
            "Train Loss: 0.2641 Acc: 0.9138\n",
            "Val Loss: 0.2564 Acc: 0.8889\n",
            "\n",
            "Epoch 10/10\n",
            "Train Loss: 0.3007 Acc: 0.8856\n",
            "Val Loss: 0.2301 Acc: 0.9000\n",
            "\n",
            "\n",
            "Testing parameters: {'batch_size': 16, 'dropout_rate': 0.3, 'epochs': 10, 'learning_rate': 0.001, 'weight_decay': 1e-05}\n",
            "Epoch 1/10\n",
            "Train Loss: 1.0804 Acc: 0.4080\n",
            "Val Loss: 0.9510 Acc: 0.7444\n",
            "\n",
            "Epoch 2/10\n",
            "Train Loss: 0.7555 Acc: 0.7098\n",
            "Val Loss: 0.9831 Acc: 0.7333\n",
            "\n",
            "Epoch 3/10\n",
            "Train Loss: 0.7130 Acc: 0.7032\n",
            "Val Loss: 0.8493 Acc: 0.8000\n",
            "\n",
            "Epoch 4/10\n",
            "Train Loss: 0.6865 Acc: 0.7181\n",
            "Val Loss: 0.5320 Acc: 0.8333\n",
            "\n",
            "Epoch 5/10\n",
            "Train Loss: 0.5160 Acc: 0.7944\n",
            "Val Loss: 0.3067 Acc: 0.8889\n",
            "\n",
            "Epoch 6/10\n",
            "Train Loss: 0.4360 Acc: 0.8242\n",
            "Val Loss: 0.3339 Acc: 0.8778\n",
            "\n",
            "Epoch 7/10\n",
            "Train Loss: 0.3690 Acc: 0.8507\n",
            "Val Loss: 0.2929 Acc: 0.9111\n",
            "\n",
            "Epoch 8/10\n",
            "Train Loss: 0.3552 Acc: 0.8640\n",
            "Val Loss: 0.2665 Acc: 0.9222\n",
            "\n",
            "Epoch 9/10\n",
            "Train Loss: 0.3151 Acc: 0.8756\n",
            "Val Loss: 0.3394 Acc: 0.8889\n",
            "\n",
            "Epoch 10/10\n",
            "Train Loss: 0.2531 Acc: 0.9138\n",
            "Val Loss: 0.2815 Acc: 0.9000\n",
            "\n",
            "\n",
            "Testing parameters: {'batch_size': 16, 'dropout_rate': 0.3, 'epochs': 10, 'learning_rate': 0.001, 'weight_decay': 0.0001}\n",
            "Epoch 1/10\n",
            "Train Loss: 1.0766 Acc: 0.4411\n",
            "Val Loss: 0.9489 Acc: 0.6889\n",
            "\n",
            "Epoch 2/10\n",
            "Train Loss: 0.7139 Acc: 0.7280\n",
            "Val Loss: 0.5490 Acc: 0.7889\n",
            "\n",
            "Epoch 3/10\n",
            "Train Loss: 0.7589 Acc: 0.6667\n",
            "Val Loss: 0.7749 Acc: 0.7333\n",
            "\n",
            "Epoch 4/10\n",
            "Train Loss: 0.6555 Acc: 0.7363\n",
            "Val Loss: 0.5168 Acc: 0.7778\n",
            "\n",
            "Epoch 5/10\n",
            "Train Loss: 0.5615 Acc: 0.7745\n",
            "Val Loss: 0.2568 Acc: 0.8778\n",
            "\n",
            "Epoch 6/10\n",
            "Train Loss: 0.4442 Acc: 0.8275\n",
            "Val Loss: 0.2854 Acc: 0.9222\n",
            "\n",
            "Epoch 7/10\n",
            "Train Loss: 0.3828 Acc: 0.8574\n",
            "Val Loss: 0.3470 Acc: 0.9000\n",
            "\n",
            "Epoch 8/10\n",
            "Train Loss: 0.3437 Acc: 0.8590\n",
            "Val Loss: 0.2598 Acc: 0.9000\n",
            "\n",
            "Epoch 9/10\n",
            "Train Loss: 0.2731 Acc: 0.8905\n",
            "Val Loss: 0.3003 Acc: 0.9222\n",
            "\n",
            "Epoch 10/10\n",
            "Train Loss: 0.2619 Acc: 0.9022\n",
            "Val Loss: 0.2504 Acc: 0.9000\n",
            "\n",
            "\n",
            "Testing parameters: {'batch_size': 16, 'dropout_rate': 0.3, 'epochs': 10, 'learning_rate': 0.001, 'weight_decay': 0.001}\n",
            "Epoch 1/10\n",
            "Train Loss: 1.0680 Acc: 0.4428\n",
            "Val Loss: 0.9235 Acc: 0.7556\n",
            "\n",
            "Epoch 2/10\n",
            "Train Loss: 0.7757 Acc: 0.6998\n",
            "Val Loss: 0.5174 Acc: 0.8222\n",
            "\n",
            "Epoch 3/10\n",
            "Train Loss: 0.7051 Acc: 0.7297\n",
            "Val Loss: 0.4762 Acc: 0.8444\n",
            "\n",
            "Epoch 4/10\n",
            "Train Loss: 0.5767 Acc: 0.7678\n",
            "Val Loss: 0.4122 Acc: 0.8667\n",
            "\n",
            "Epoch 5/10\n",
            "Train Loss: 0.5629 Acc: 0.7894\n",
            "Val Loss: 0.5775 Acc: 0.8000\n",
            "\n",
            "Epoch 6/10\n",
            "Train Loss: 0.5164 Acc: 0.7877\n",
            "Val Loss: 0.3121 Acc: 0.8667\n",
            "\n",
            "Epoch 7/10\n",
            "Train Loss: 0.4010 Acc: 0.8441\n",
            "Val Loss: 0.2326 Acc: 0.8889\n",
            "\n",
            "Epoch 8/10\n",
            "Train Loss: 0.3119 Acc: 0.8806\n",
            "Val Loss: 0.2400 Acc: 0.9000\n",
            "\n",
            "Epoch 9/10\n",
            "Train Loss: 0.3105 Acc: 0.8657\n",
            "Val Loss: 0.2526 Acc: 0.8889\n",
            "\n",
            "Epoch 10/10\n",
            "Train Loss: 0.2330 Acc: 0.9254\n",
            "Val Loss: 0.2497 Acc: 0.8889\n",
            "\n",
            "\n",
            "Testing parameters: {'batch_size': 16, 'dropout_rate': 0.5, 'epochs': 10, 'learning_rate': 0.1, 'weight_decay': 0}\n",
            "Epoch 1/10\n",
            "Train Loss: 1.2667 Acc: 0.3715\n",
            "Val Loss: 1915.0223 Acc: 0.3889\n",
            "\n",
            "Epoch 2/10\n",
            "Train Loss: 1.1740 Acc: 0.3018\n",
            "Val Loss: 146.1702 Acc: 0.3333\n",
            "\n",
            "Epoch 3/10\n",
            "Train Loss: 1.1165 Acc: 0.3317\n",
            "Val Loss: 1.1006 Acc: 0.3333\n",
            "\n",
            "Epoch 4/10\n",
            "Train Loss: 1.1035 Acc: 0.3416\n",
            "Val Loss: 1.1194 Acc: 0.3333\n",
            "\n",
            "Epoch 5/10\n",
            "Train Loss: 1.1060 Acc: 0.3018\n",
            "Val Loss: 1.0995 Acc: 0.3333\n",
            "\n",
            "Epoch 6/10\n",
            "Train Loss: 1.1068 Acc: 0.2886\n",
            "Val Loss: 1.1011 Acc: 0.3333\n",
            "\n",
            "Epoch 7/10\n",
            "Train Loss: 1.1055 Acc: 0.3134\n",
            "Val Loss: 1.1002 Acc: 0.3333\n",
            "\n",
            "Epoch 8/10\n",
            "Train Loss: 1.1035 Acc: 0.2919\n",
            "Val Loss: 1.0989 Acc: 0.3333\n",
            "\n",
            "Epoch 9/10\n",
            "Train Loss: 1.1001 Acc: 0.3085\n",
            "Val Loss: 1.0987 Acc: 0.3333\n",
            "\n",
            "Epoch 10/10\n",
            "Train Loss: 1.0988 Acc: 0.3333\n",
            "Val Loss: 1.0987 Acc: 0.3333\n",
            "\n",
            "\n",
            "Testing parameters: {'batch_size': 16, 'dropout_rate': 0.5, 'epochs': 10, 'learning_rate': 0.1, 'weight_decay': 1e-05}\n",
            "Epoch 1/10\n",
            "Train Loss: 1.1823 Acc: 0.3814\n",
            "Val Loss: 3588.9540 Acc: 0.3333\n",
            "\n",
            "Epoch 2/10\n",
            "Train Loss: 1.1340 Acc: 0.3516\n",
            "Val Loss: 24.6624 Acc: 0.3333\n",
            "\n",
            "Epoch 3/10\n",
            "Train Loss: 1.1094 Acc: 0.3134\n",
            "Val Loss: 1.1092 Acc: 0.3333\n",
            "\n",
            "Epoch 4/10\n",
            "Train Loss: 1.1169 Acc: 0.3134\n",
            "Val Loss: 1.0994 Acc: 0.3333\n",
            "\n",
            "Epoch 5/10\n",
            "Train Loss: 1.1040 Acc: 0.3002\n",
            "Val Loss: 1.1016 Acc: 0.3333\n",
            "\n",
            "Epoch 6/10\n",
            "Train Loss: 1.1064 Acc: 0.3151\n",
            "Val Loss: 1.1061 Acc: 0.3333\n",
            "\n",
            "Epoch 7/10\n",
            "Train Loss: 1.1023 Acc: 0.3250\n",
            "Val Loss: 1.1000 Acc: 0.3333\n",
            "\n",
            "Epoch 8/10\n",
            "Train Loss: 1.1025 Acc: 0.2902\n",
            "Val Loss: 1.0987 Acc: 0.3333\n",
            "\n",
            "Epoch 9/10\n",
            "Train Loss: 1.1022 Acc: 0.3167\n",
            "Val Loss: 1.0992 Acc: 0.3333\n",
            "\n",
            "Epoch 10/10\n",
            "Train Loss: 1.0991 Acc: 0.3333\n",
            "Val Loss: 1.0989 Acc: 0.3333\n",
            "\n",
            "\n",
            "Testing parameters: {'batch_size': 16, 'dropout_rate': 0.5, 'epochs': 10, 'learning_rate': 0.1, 'weight_decay': 0.0001}\n",
            "Epoch 1/10\n",
            "Train Loss: 1.2098 Acc: 0.4229\n",
            "Val Loss: 2762.7765 Acc: 0.3333\n",
            "\n",
            "Epoch 2/10\n",
            "Train Loss: 1.1088 Acc: 0.3018\n",
            "Val Loss: 7.7205 Acc: 0.3333\n",
            "\n",
            "Epoch 3/10\n",
            "Train Loss: 1.1073 Acc: 0.3051\n",
            "Val Loss: 2.6504 Acc: 0.3222\n",
            "\n",
            "Epoch 4/10\n",
            "Train Loss: 1.1084 Acc: 0.3267\n",
            "Val Loss: 1.6274 Acc: 0.3222\n",
            "\n",
            "Epoch 5/10\n",
            "Train Loss: 1.1054 Acc: 0.3350\n",
            "Val Loss: 1.2521 Acc: 0.3222\n",
            "\n",
            "Epoch 6/10\n",
            "Train Loss: 1.1027 Acc: 0.3350\n",
            "Val Loss: 2.5900 Acc: 0.3333\n",
            "\n",
            "Epoch 7/10\n",
            "Train Loss: 1.1066 Acc: 0.3118\n",
            "Val Loss: 1.7026 Acc: 0.3222\n",
            "\n",
            "Epoch 8/10\n",
            "Train Loss: 1.1051 Acc: 0.3051\n",
            "Val Loss: 1.3995 Acc: 0.3333\n",
            "\n",
            "Epoch 9/10\n",
            "Train Loss: 1.0988 Acc: 0.3383\n",
            "Val Loss: 3.0893 Acc: 0.3222\n",
            "\n",
            "Epoch 10/10\n",
            "Train Loss: 1.0989 Acc: 0.3333\n",
            "Val Loss: 1.1882 Acc: 0.3222\n",
            "\n",
            "\n",
            "Testing parameters: {'batch_size': 16, 'dropout_rate': 0.5, 'epochs': 10, 'learning_rate': 0.1, 'weight_decay': 0.001}\n",
            "Epoch 1/10\n",
            "Train Loss: 1.2495 Acc: 0.3698\n",
            "Val Loss: 1060.8011 Acc: 0.3333\n",
            "\n",
            "Epoch 2/10\n",
            "Train Loss: 1.1525 Acc: 0.3018\n",
            "Val Loss: 84.2223 Acc: 0.3222\n",
            "\n",
            "Epoch 3/10\n",
            "Train Loss: 1.1309 Acc: 0.3184\n",
            "Val Loss: 1.1026 Acc: 0.3333\n",
            "\n",
            "Epoch 4/10\n",
            "Train Loss: 1.1075 Acc: 0.3184\n",
            "Val Loss: 1.1008 Acc: 0.3333\n",
            "\n",
            "Epoch 5/10\n",
            "Train Loss: 1.1043 Acc: 0.3483\n",
            "Val Loss: 1.1000 Acc: 0.3333\n",
            "\n",
            "Epoch 6/10\n",
            "Train Loss: 1.1024 Acc: 0.3416\n",
            "Val Loss: 1.1071 Acc: 0.3333\n",
            "\n",
            "Epoch 7/10\n",
            "Train Loss: 1.1038 Acc: 0.3300\n",
            "Val Loss: 1.0989 Acc: 0.3333\n",
            "\n",
            "Epoch 8/10\n",
            "Train Loss: 1.1009 Acc: 0.2902\n",
            "Val Loss: 1.0988 Acc: 0.3333\n",
            "\n",
            "Epoch 9/10\n",
            "Train Loss: 1.0996 Acc: 0.3051\n",
            "Val Loss: 1.0987 Acc: 0.3333\n",
            "\n",
            "Epoch 10/10\n",
            "Train Loss: 1.0996 Acc: 0.3333\n",
            "Val Loss: 1.0987 Acc: 0.3333\n",
            "\n",
            "\n",
            "Testing parameters: {'batch_size': 16, 'dropout_rate': 0.5, 'epochs': 10, 'learning_rate': 0.01, 'weight_decay': 0}\n",
            "Epoch 1/10\n",
            "Train Loss: 0.9180 Acc: 0.5887\n",
            "Val Loss: 1.4693 Acc: 0.5333\n",
            "\n",
            "Epoch 2/10\n",
            "Train Loss: 1.0956 Acc: 0.4643\n",
            "Val Loss: 1.3559 Acc: 0.3778\n",
            "\n",
            "Epoch 3/10\n",
            "Train Loss: 1.1365 Acc: 0.3449\n",
            "Val Loss: 1.0954 Acc: 0.2778\n",
            "\n",
            "Epoch 4/10\n",
            "Train Loss: 1.1008 Acc: 0.2952\n",
            "Val Loss: 1.1205 Acc: 0.3333\n",
            "\n",
            "Epoch 5/10\n",
            "Train Loss: 1.1020 Acc: 0.3317\n",
            "Val Loss: 1.1000 Acc: 0.3444\n",
            "\n",
            "Epoch 6/10\n",
            "Train Loss: 1.1020 Acc: 0.3549\n",
            "Val Loss: 1.0983 Acc: 0.3333\n",
            "\n",
            "Epoch 7/10\n",
            "Train Loss: 1.0993 Acc: 0.3184\n",
            "Val Loss: 1.0974 Acc: 0.3444\n",
            "\n",
            "Epoch 8/10\n",
            "Train Loss: 1.0995 Acc: 0.3350\n",
            "Val Loss: 1.0977 Acc: 0.3333\n",
            "\n",
            "Epoch 9/10\n",
            "Train Loss: 1.0984 Acc: 0.3333\n",
            "Val Loss: 1.0962 Acc: 0.3333\n",
            "\n",
            "Epoch 10/10\n",
            "Train Loss: 1.0970 Acc: 0.3333\n",
            "Val Loss: 1.0949 Acc: 0.3333\n",
            "\n",
            "\n",
            "Testing parameters: {'batch_size': 16, 'dropout_rate': 0.5, 'epochs': 10, 'learning_rate': 0.01, 'weight_decay': 1e-05}\n",
            "Epoch 1/10\n",
            "Train Loss: 0.9713 Acc: 0.5721\n",
            "Val Loss: 1.4107 Acc: 0.5444\n",
            "\n",
            "Epoch 2/10\n",
            "Train Loss: 1.0725 Acc: 0.4776\n",
            "Val Loss: 49.7800 Acc: 0.4000\n",
            "\n",
            "Epoch 3/10\n",
            "Train Loss: 1.1208 Acc: 0.3134\n",
            "Val Loss: 1.1247 Acc: 0.3778\n",
            "\n",
            "Epoch 4/10\n",
            "Train Loss: 1.1042 Acc: 0.3134\n",
            "Val Loss: 1.0978 Acc: 0.3222\n",
            "\n",
            "Epoch 5/10\n",
            "Train Loss: 1.1016 Acc: 0.3051\n",
            "Val Loss: 1.0988 Acc: 0.3444\n",
            "\n",
            "Epoch 6/10\n",
            "Train Loss: 1.1016 Acc: 0.3018\n",
            "Val Loss: 1.0987 Acc: 0.3333\n",
            "\n",
            "Epoch 7/10\n",
            "Train Loss: 1.0991 Acc: 0.3035\n",
            "Val Loss: 1.0986 Acc: 0.3333\n",
            "\n",
            "Epoch 8/10\n",
            "Train Loss: 1.0987 Acc: 0.3333\n",
            "Val Loss: 1.0986 Acc: 0.3333\n",
            "\n",
            "Epoch 9/10\n",
            "Train Loss: 1.0988 Acc: 0.3035\n",
            "Val Loss: 1.0986 Acc: 0.3333\n",
            "\n",
            "Epoch 10/10\n",
            "Train Loss: 1.0986 Acc: 0.3350\n",
            "Val Loss: 1.0986 Acc: 0.3333\n",
            "\n",
            "\n",
            "Testing parameters: {'batch_size': 16, 'dropout_rate': 0.5, 'epochs': 10, 'learning_rate': 0.01, 'weight_decay': 0.0001}\n",
            "Epoch 1/10\n",
            "Train Loss: 0.9801 Acc: 0.5439\n",
            "Val Loss: 0.8514 Acc: 0.7222\n",
            "\n",
            "Epoch 2/10\n",
            "Train Loss: 1.0144 Acc: 0.5307\n",
            "Val Loss: 8.5642 Acc: 0.4222\n",
            "\n",
            "Epoch 3/10\n",
            "Train Loss: 1.1403 Acc: 0.3582\n",
            "Val Loss: 1.0939 Acc: 0.3444\n",
            "\n",
            "Epoch 4/10\n",
            "Train Loss: 1.1044 Acc: 0.2985\n",
            "Val Loss: 1.0997 Acc: 0.3444\n",
            "\n",
            "Epoch 5/10\n",
            "Train Loss: 1.1060 Acc: 0.2919\n",
            "Val Loss: 1.0988 Acc: 0.3222\n",
            "\n",
            "Epoch 6/10\n",
            "Train Loss: 1.0998 Acc: 0.3333\n",
            "Val Loss: 1.0987 Acc: 0.3333\n",
            "\n",
            "Epoch 7/10\n",
            "Train Loss: 1.0990 Acc: 0.3234\n",
            "Val Loss: 1.0986 Acc: 0.3333\n",
            "\n",
            "Epoch 8/10\n",
            "Train Loss: 1.0989 Acc: 0.3333\n",
            "Val Loss: 1.0986 Acc: 0.3333\n",
            "\n",
            "Epoch 9/10\n",
            "Train Loss: 1.0987 Acc: 0.3333\n",
            "Val Loss: 1.0985 Acc: 0.3333\n",
            "\n",
            "Epoch 10/10\n",
            "Train Loss: 1.0986 Acc: 0.3333\n",
            "Val Loss: 1.0984 Acc: 0.3556\n",
            "\n",
            "\n",
            "Testing parameters: {'batch_size': 16, 'dropout_rate': 0.5, 'epochs': 10, 'learning_rate': 0.01, 'weight_decay': 0.001}\n",
            "Epoch 1/10\n",
            "Train Loss: 0.9562 Acc: 0.5406\n",
            "Val Loss: 1.1877 Acc: 0.5889\n",
            "\n",
            "Epoch 2/10\n",
            "Train Loss: 1.0367 Acc: 0.5091\n",
            "Val Loss: 1.2267 Acc: 0.2778\n",
            "\n",
            "Epoch 3/10\n",
            "Train Loss: 1.1431 Acc: 0.3765\n",
            "Val Loss: 1.1255 Acc: 0.3444\n",
            "\n",
            "Epoch 4/10\n",
            "Train Loss: 1.1038 Acc: 0.3085\n",
            "Val Loss: 1.1010 Acc: 0.3667\n",
            "\n",
            "Epoch 5/10\n",
            "Train Loss: 1.1002 Acc: 0.3250\n",
            "Val Loss: 1.0997 Acc: 0.3333\n",
            "\n",
            "Epoch 6/10\n",
            "Train Loss: 1.1003 Acc: 0.3151\n",
            "Val Loss: 1.1046 Acc: 0.3222\n",
            "\n",
            "Epoch 7/10\n",
            "Train Loss: 1.1063 Acc: 0.3051\n",
            "Val Loss: 1.0983 Acc: 0.3333\n",
            "\n",
            "Epoch 8/10\n",
            "Train Loss: 1.0988 Acc: 0.3317\n",
            "Val Loss: 1.0986 Acc: 0.3333\n",
            "\n",
            "Epoch 9/10\n",
            "Train Loss: 1.0985 Acc: 0.3367\n",
            "Val Loss: 1.0986 Acc: 0.3333\n",
            "\n",
            "Epoch 10/10\n",
            "Train Loss: 1.0987 Acc: 0.3300\n",
            "Val Loss: 1.0986 Acc: 0.3333\n",
            "\n",
            "\n",
            "Testing parameters: {'batch_size': 16, 'dropout_rate': 0.5, 'epochs': 10, 'learning_rate': 0.001, 'weight_decay': 0}\n",
            "Epoch 1/10\n",
            "Train Loss: 1.0837 Acc: 0.3964\n",
            "Val Loss: 0.9945 Acc: 0.6889\n",
            "\n",
            "Epoch 2/10\n",
            "Train Loss: 0.7953 Acc: 0.7164\n",
            "Val Loss: 0.6929 Acc: 0.7778\n",
            "\n",
            "Epoch 3/10\n",
            "Train Loss: 0.7330 Acc: 0.7065\n",
            "Val Loss: 0.6995 Acc: 0.7000\n",
            "\n",
            "Epoch 4/10\n",
            "Train Loss: 0.6505 Acc: 0.7247\n",
            "Val Loss: 0.5718 Acc: 0.7667\n",
            "\n",
            "Epoch 5/10\n",
            "Train Loss: 0.5540 Acc: 0.8043\n",
            "Val Loss: 0.3821 Acc: 0.8667\n",
            "\n",
            "Epoch 6/10\n",
            "Train Loss: 0.4258 Acc: 0.8242\n",
            "Val Loss: 0.3049 Acc: 0.8889\n",
            "\n",
            "Epoch 7/10\n",
            "Train Loss: 0.4185 Acc: 0.8325\n",
            "Val Loss: 0.3714 Acc: 0.8556\n",
            "\n",
            "Epoch 8/10\n",
            "Train Loss: 0.3293 Acc: 0.8723\n",
            "Val Loss: 0.3158 Acc: 0.8556\n",
            "\n",
            "Epoch 9/10\n",
            "Train Loss: 0.2871 Acc: 0.8872\n",
            "Val Loss: 0.2791 Acc: 0.9000\n",
            "\n",
            "Epoch 10/10\n",
            "Train Loss: 0.2416 Acc: 0.9071\n",
            "Val Loss: 0.3073 Acc: 0.8778\n",
            "\n",
            "\n",
            "Testing parameters: {'batch_size': 16, 'dropout_rate': 0.5, 'epochs': 10, 'learning_rate': 0.001, 'weight_decay': 1e-05}\n",
            "Epoch 1/10\n",
            "Train Loss: 1.0744 Acc: 0.4196\n",
            "Val Loss: 0.9886 Acc: 0.7333\n",
            "\n",
            "Epoch 2/10\n",
            "Train Loss: 0.8387 Acc: 0.6683\n",
            "Val Loss: 0.7433 Acc: 0.6667\n",
            "\n",
            "Epoch 3/10\n",
            "Train Loss: 0.7276 Acc: 0.6998\n",
            "Val Loss: 0.5569 Acc: 0.7667\n",
            "\n",
            "Epoch 4/10\n",
            "Train Loss: 0.6314 Acc: 0.7380\n",
            "Val Loss: 0.4658 Acc: 0.8111\n",
            "\n",
            "Epoch 5/10\n",
            "Train Loss: 0.5673 Acc: 0.7794\n",
            "Val Loss: 0.6254 Acc: 0.7667\n",
            "\n",
            "Epoch 6/10\n",
            "Train Loss: 0.4294 Acc: 0.8441\n",
            "Val Loss: 0.3997 Acc: 0.8444\n",
            "\n",
            "Epoch 7/10\n",
            "Train Loss: 0.3967 Acc: 0.8474\n",
            "Val Loss: 0.3251 Acc: 0.8667\n",
            "\n",
            "Epoch 8/10\n",
            "Train Loss: 0.3351 Acc: 0.8789\n",
            "Val Loss: 0.2238 Acc: 0.9222\n",
            "\n",
            "Epoch 9/10\n",
            "Train Loss: 0.2928 Acc: 0.8889\n",
            "Val Loss: 0.2556 Acc: 0.9333\n",
            "\n",
            "Epoch 10/10\n",
            "Train Loss: 0.2554 Acc: 0.9104\n",
            "Val Loss: 0.2167 Acc: 0.9333\n",
            "\n",
            "\n",
            "Testing parameters: {'batch_size': 16, 'dropout_rate': 0.5, 'epochs': 10, 'learning_rate': 0.001, 'weight_decay': 0.0001}\n",
            "Epoch 1/10\n",
            "Train Loss: 1.0787 Acc: 0.4113\n",
            "Val Loss: 0.9773 Acc: 0.7222\n",
            "\n",
            "Epoch 2/10\n",
            "Train Loss: 0.8225 Acc: 0.6849\n",
            "Val Loss: 0.5597 Acc: 0.8111\n",
            "\n",
            "Epoch 3/10\n",
            "Train Loss: 0.6821 Acc: 0.7214\n",
            "Val Loss: 0.7277 Acc: 0.7444\n",
            "\n",
            "Epoch 4/10\n",
            "Train Loss: 0.6454 Acc: 0.7330\n",
            "Val Loss: 0.6621 Acc: 0.7556\n",
            "\n",
            "Epoch 5/10\n",
            "Train Loss: 0.6128 Acc: 0.7595\n",
            "Val Loss: 0.4436 Acc: 0.7889\n",
            "\n",
            "Epoch 6/10\n",
            "Train Loss: 0.4999 Acc: 0.8093\n",
            "Val Loss: 0.3731 Acc: 0.8333\n",
            "\n",
            "Epoch 7/10\n",
            "Train Loss: 0.4549 Acc: 0.8308\n",
            "Val Loss: 0.6451 Acc: 0.7889\n",
            "\n",
            "Epoch 8/10\n",
            "Train Loss: 0.4150 Acc: 0.8159\n",
            "Val Loss: 0.4419 Acc: 0.8222\n",
            "\n",
            "Epoch 9/10\n",
            "Train Loss: 0.2704 Acc: 0.9005\n",
            "Val Loss: 0.3682 Acc: 0.8667\n",
            "\n",
            "Epoch 10/10\n",
            "Train Loss: 0.3006 Acc: 0.8922\n",
            "Val Loss: 0.3906 Acc: 0.8556\n",
            "\n",
            "\n",
            "Testing parameters: {'batch_size': 16, 'dropout_rate': 0.5, 'epochs': 10, 'learning_rate': 0.001, 'weight_decay': 0.001}\n",
            "Epoch 1/10\n",
            "Train Loss: 1.0817 Acc: 0.4378\n",
            "Val Loss: 0.9875 Acc: 0.6556\n",
            "\n",
            "Epoch 2/10\n",
            "Train Loss: 0.8130 Acc: 0.6667\n",
            "Val Loss: 0.4492 Acc: 0.8444\n",
            "\n",
            "Epoch 3/10\n",
            "Train Loss: 0.7328 Acc: 0.6899\n",
            "Val Loss: 1.1798 Acc: 0.6333\n",
            "\n",
            "Epoch 4/10\n",
            "Train Loss: 0.6911 Acc: 0.7247\n",
            "Val Loss: 0.5012 Acc: 0.8000\n",
            "\n",
            "Epoch 5/10\n",
            "Train Loss: 0.5379 Acc: 0.7678\n",
            "Val Loss: 0.3017 Acc: 0.8778\n",
            "\n",
            "Epoch 6/10\n",
            "Train Loss: 0.4527 Acc: 0.8375\n",
            "Val Loss: 0.5469 Acc: 0.8778\n",
            "\n",
            "Epoch 7/10\n",
            "Train Loss: 0.4007 Acc: 0.8507\n",
            "Val Loss: 0.3551 Acc: 0.8667\n",
            "\n",
            "Epoch 8/10\n",
            "Train Loss: 0.3109 Acc: 0.8889\n",
            "Val Loss: 0.3000 Acc: 0.9111\n",
            "\n",
            "Epoch 9/10\n",
            "Train Loss: 0.2768 Acc: 0.8972\n",
            "Val Loss: 0.2894 Acc: 0.9111\n",
            "\n",
            "Epoch 10/10\n",
            "Train Loss: 0.2515 Acc: 0.9005\n",
            "Val Loss: 0.2933 Acc: 0.9111\n",
            "\n",
            "\n",
            "Testing parameters: {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 10, 'learning_rate': 0.1, 'weight_decay': 0}\n",
            "Epoch 1/10\n",
            "Train Loss: 1.2059 Acc: 0.4096\n",
            "Val Loss: 6701.6072 Acc: 0.3667\n",
            "\n",
            "Epoch 2/10\n",
            "Train Loss: 1.1444 Acc: 0.3350\n",
            "Val Loss: 1070356.3167 Acc: 0.3333\n",
            "\n",
            "Epoch 3/10\n",
            "Train Loss: 1.1878 Acc: 0.3499\n",
            "Val Loss: 5.0157 Acc: 0.3222\n",
            "\n",
            "Epoch 4/10\n",
            "Train Loss: 1.1598 Acc: 0.3317\n",
            "Val Loss: 22.3388 Acc: 0.3333\n",
            "\n",
            "Epoch 5/10\n",
            "Train Loss: 1.1053 Acc: 0.3317\n",
            "Val Loss: 1.1021 Acc: 0.3333\n",
            "\n",
            "Epoch 6/10\n",
            "Train Loss: 1.1029 Acc: 0.3134\n",
            "Val Loss: 1.0988 Acc: 0.3333\n",
            "\n",
            "Epoch 7/10\n",
            "Train Loss: 1.1005 Acc: 0.3333\n",
            "Val Loss: 1.0995 Acc: 0.3333\n",
            "\n",
            "Epoch 8/10\n",
            "Train Loss: 1.0993 Acc: 0.3416\n",
            "Val Loss: 1.0991 Acc: 0.3333\n",
            "\n",
            "Epoch 9/10\n",
            "Train Loss: 1.0992 Acc: 0.3333\n",
            "Val Loss: 1.0989 Acc: 0.3333\n",
            "\n",
            "Epoch 10/10\n",
            "Train Loss: 1.0988 Acc: 0.3333\n",
            "Val Loss: 1.0988 Acc: 0.3333\n",
            "\n",
            "\n",
            "Testing parameters: {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 10, 'learning_rate': 0.1, 'weight_decay': 1e-05}\n",
            "Epoch 1/10\n",
            "Train Loss: 1.1929 Acc: 0.4096\n",
            "Val Loss: 4541.5629 Acc: 0.3333\n",
            "\n",
            "Epoch 2/10\n",
            "Train Loss: 1.1679 Acc: 0.3317\n",
            "Val Loss: 1.1003 Acc: 0.3333\n",
            "\n",
            "Epoch 3/10\n",
            "Train Loss: 1.1026 Acc: 0.3118\n",
            "Val Loss: 1.0998 Acc: 0.3333\n",
            "\n",
            "Epoch 4/10\n",
            "Train Loss: 1.1039 Acc: 0.3101\n",
            "Val Loss: 1.0992 Acc: 0.3333\n",
            "\n",
            "Epoch 5/10\n",
            "Train Loss: 1.1006 Acc: 0.3300\n",
            "Val Loss: 1.0987 Acc: 0.3333\n",
            "\n",
            "Epoch 6/10\n",
            "Train Loss: 1.1038 Acc: 0.3134\n",
            "Val Loss: 1.0991 Acc: 0.3333\n",
            "\n",
            "Epoch 7/10\n",
            "Train Loss: 1.1033 Acc: 0.3234\n",
            "Val Loss: 1.0991 Acc: 0.3333\n",
            "\n",
            "Epoch 8/10\n",
            "Train Loss: 1.0985 Acc: 0.3250\n",
            "Val Loss: 1.0987 Acc: 0.3333\n",
            "\n",
            "Epoch 9/10\n",
            "Train Loss: 1.0990 Acc: 0.3333\n",
            "Val Loss: 1.0989 Acc: 0.3333\n",
            "\n",
            "Epoch 10/10\n",
            "Train Loss: 1.0990 Acc: 0.3333\n",
            "Val Loss: 1.0989 Acc: 0.3333\n",
            "\n",
            "\n",
            "Testing parameters: {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 10, 'learning_rate': 0.1, 'weight_decay': 0.0001}\n",
            "Epoch 1/10\n",
            "Train Loss: 1.1394 Acc: 0.4312\n",
            "Val Loss: 2480.9962 Acc: 0.2889\n",
            "\n",
            "Epoch 2/10\n",
            "Train Loss: 1.2141 Acc: 0.3167\n",
            "Val Loss: 22844.2667 Acc: 0.3333\n",
            "\n",
            "Epoch 3/10\n",
            "Train Loss: 1.3345 Acc: 0.3300\n",
            "Val Loss: 1.1048 Acc: 0.3333\n",
            "\n",
            "Epoch 4/10\n",
            "Train Loss: 1.1065 Acc: 0.3118\n",
            "Val Loss: 1.1001 Acc: 0.3333\n",
            "\n",
            "Epoch 5/10\n",
            "Train Loss: 1.1011 Acc: 0.3118\n",
            "Val Loss: 1.0993 Acc: 0.3333\n",
            "\n",
            "Epoch 6/10\n",
            "Train Loss: 1.1030 Acc: 0.3068\n",
            "Val Loss: 1.0988 Acc: 0.3333\n",
            "\n",
            "Epoch 7/10\n",
            "Train Loss: 1.1040 Acc: 0.3300\n",
            "Val Loss: 1.1001 Acc: 0.3333\n",
            "\n",
            "Epoch 8/10\n",
            "Train Loss: 1.1009 Acc: 0.3167\n",
            "Val Loss: 1.0998 Acc: 0.3333\n",
            "\n",
            "Epoch 9/10\n",
            "Train Loss: 1.0996 Acc: 0.3333\n",
            "Val Loss: 1.0991 Acc: 0.3333\n",
            "\n",
            "Epoch 10/10\n",
            "Train Loss: 1.0991 Acc: 0.3333\n",
            "Val Loss: 1.0990 Acc: 0.3333\n",
            "\n",
            "\n",
            "Testing parameters: {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 10, 'learning_rate': 0.1, 'weight_decay': 0.001}\n",
            "Epoch 1/10\n",
            "Train Loss: 1.2259 Acc: 0.3914\n",
            "Val Loss: 13078.6974 Acc: 0.3333\n",
            "\n",
            "Epoch 2/10\n",
            "Train Loss: 1.1565 Acc: 0.2902\n",
            "Val Loss: 48155.9264 Acc: 0.3333\n",
            "\n",
            "Epoch 3/10\n",
            "Train Loss: 1.1184 Acc: 0.3151\n",
            "Val Loss: 214.3729 Acc: 0.3333\n",
            "\n",
            "Epoch 4/10\n",
            "Train Loss: 1.5435 Acc: 0.3383\n",
            "Val Loss: 5243.4988 Acc: 0.3333\n",
            "\n",
            "Epoch 5/10\n",
            "Train Loss: 1.1029 Acc: 0.3267\n",
            "Val Loss: 1.1005 Acc: 0.3333\n",
            "\n",
            "Epoch 6/10\n",
            "Train Loss: 1.1083 Acc: 0.3333\n",
            "Val Loss: 1.0987 Acc: 0.3333\n",
            "\n",
            "Epoch 7/10\n",
            "Train Loss: 1.1014 Acc: 0.3333\n",
            "Val Loss: 1.0995 Acc: 0.3333\n",
            "\n",
            "Epoch 8/10\n",
            "Train Loss: 1.0994 Acc: 0.3333\n",
            "Val Loss: 1.0988 Acc: 0.3333\n",
            "\n",
            "Epoch 9/10\n",
            "Train Loss: 1.0985 Acc: 0.3333\n",
            "Val Loss: 1.0986 Acc: 0.3333\n",
            "\n",
            "Epoch 10/10\n",
            "Train Loss: 1.0986 Acc: 0.3367\n",
            "Val Loss: 1.0986 Acc: 0.3333\n",
            "\n",
            "\n",
            "Testing parameters: {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 10, 'learning_rate': 0.01, 'weight_decay': 0}\n",
            "Epoch 1/10\n",
            "Train Loss: 0.9646 Acc: 0.5589\n",
            "Val Loss: 0.7885 Acc: 0.6889\n",
            "\n",
            "Epoch 2/10\n",
            "Train Loss: 1.0149 Acc: 0.5456\n",
            "Val Loss: 1.2168 Acc: 0.2889\n",
            "\n",
            "Epoch 3/10\n",
            "Train Loss: 1.1116 Acc: 0.4196\n",
            "Val Loss: 8.3774 Acc: 0.3556\n",
            "\n",
            "Epoch 4/10\n",
            "Train Loss: 1.0463 Acc: 0.4279\n",
            "Val Loss: 1.2886 Acc: 0.4444\n",
            "\n",
            "Epoch 5/10\n",
            "Train Loss: 1.0011 Acc: 0.4876\n",
            "Val Loss: 1.2412 Acc: 0.3889\n",
            "\n",
            "Epoch 6/10\n",
            "Train Loss: 1.0108 Acc: 0.4677\n",
            "Val Loss: 2.8101 Acc: 0.3000\n",
            "\n",
            "Epoch 7/10\n",
            "Train Loss: 0.9623 Acc: 0.4760\n",
            "Val Loss: 1.6233 Acc: 0.4889\n",
            "\n",
            "Epoch 8/10\n",
            "Train Loss: 0.8711 Acc: 0.5439\n",
            "Val Loss: 0.7962 Acc: 0.5889\n",
            "\n",
            "Epoch 9/10\n",
            "Train Loss: 0.8754 Acc: 0.5323\n",
            "Val Loss: 0.7560 Acc: 0.5778\n",
            "\n",
            "Epoch 10/10\n",
            "Train Loss: 0.8191 Acc: 0.5556\n",
            "Val Loss: 0.7450 Acc: 0.5778\n",
            "\n",
            "\n",
            "Testing parameters: {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 10, 'learning_rate': 0.01, 'weight_decay': 1e-05}\n",
            "Epoch 1/10\n",
            "Train Loss: 0.9229 Acc: 0.5456\n",
            "Val Loss: 2.4594 Acc: 0.5889\n",
            "\n",
            "Epoch 2/10\n",
            "Train Loss: 0.9007 Acc: 0.6136\n",
            "Val Loss: 4.1298 Acc: 0.5000\n",
            "\n",
            "Epoch 3/10\n",
            "Train Loss: 1.0620 Acc: 0.4677\n",
            "Val Loss: 33.9546 Acc: 0.3333\n",
            "\n",
            "Epoch 4/10\n",
            "Train Loss: 0.9539 Acc: 0.4643\n",
            "Val Loss: 1.9345 Acc: 0.5333\n",
            "\n",
            "Epoch 5/10\n",
            "Train Loss: 0.9606 Acc: 0.4776\n",
            "Val Loss: 1.5724 Acc: 0.3889\n",
            "\n",
            "Epoch 6/10\n",
            "Train Loss: 0.8464 Acc: 0.5091\n",
            "Val Loss: 0.8034 Acc: 0.5556\n",
            "\n",
            "Epoch 7/10\n",
            "Train Loss: 0.8592 Acc: 0.5158\n",
            "Val Loss: 0.7616 Acc: 0.6222\n",
            "\n",
            "Epoch 8/10\n",
            "Train Loss: 0.8298 Acc: 0.5622\n",
            "Val Loss: 0.7085 Acc: 0.7000\n",
            "\n",
            "Epoch 9/10\n",
            "Train Loss: 0.7656 Acc: 0.5771\n",
            "Val Loss: 0.6871 Acc: 0.5778\n",
            "\n",
            "Epoch 10/10\n",
            "Train Loss: 0.7417 Acc: 0.5987\n",
            "Val Loss: 0.6790 Acc: 0.5889\n",
            "\n",
            "\n",
            "Testing parameters: {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 10, 'learning_rate': 0.01, 'weight_decay': 0.0001}\n",
            "Epoch 1/10\n",
            "Train Loss: 0.9118 Acc: 0.5638\n",
            "Val Loss: 1.3602 Acc: 0.6444\n",
            "\n",
            "Epoch 2/10\n",
            "Train Loss: 1.0526 Acc: 0.5904\n",
            "Val Loss: 2.4619 Acc: 0.3444\n",
            "\n",
            "Epoch 3/10\n",
            "Train Loss: 1.0472 Acc: 0.4992\n",
            "Val Loss: 2.4353 Acc: 0.3556\n",
            "\n",
            "Epoch 4/10\n",
            "Train Loss: 1.1037 Acc: 0.3814\n",
            "Val Loss: 9.3442 Acc: 0.3556\n",
            "\n",
            "Epoch 5/10\n",
            "Train Loss: 1.1149 Acc: 0.3416\n",
            "Val Loss: 1.0800 Acc: 0.4222\n",
            "\n",
            "Epoch 6/10\n",
            "Train Loss: 1.0951 Acc: 0.3499\n",
            "Val Loss: 1.0073 Acc: 0.3444\n",
            "\n",
            "Epoch 7/10\n",
            "Train Loss: 1.0793 Acc: 0.4030\n",
            "Val Loss: 1.0937 Acc: 0.3778\n",
            "\n",
            "Epoch 8/10\n",
            "Train Loss: 1.0523 Acc: 0.4312\n",
            "Val Loss: 1.1203 Acc: 0.4444\n",
            "\n",
            "Epoch 9/10\n",
            "Train Loss: 1.0348 Acc: 0.4527\n",
            "Val Loss: 0.9849 Acc: 0.4333\n",
            "\n",
            "Epoch 10/10\n",
            "Train Loss: 1.0174 Acc: 0.4660\n",
            "Val Loss: 0.9755 Acc: 0.4667\n",
            "\n",
            "\n",
            "Testing parameters: {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 10, 'learning_rate': 0.01, 'weight_decay': 0.001}\n",
            "Epoch 1/10\n",
            "Train Loss: 0.9506 Acc: 0.5340\n",
            "Val Loss: 0.9499 Acc: 0.7667\n",
            "\n",
            "Epoch 2/10\n",
            "Train Loss: 0.9349 Acc: 0.6036\n",
            "Val Loss: 1.6384 Acc: 0.3889\n",
            "\n",
            "Epoch 3/10\n",
            "Train Loss: 1.0797 Acc: 0.4428\n",
            "Val Loss: 251.9755 Acc: 0.3333\n",
            "\n",
            "Epoch 4/10\n",
            "Train Loss: 1.0367 Acc: 0.4743\n",
            "Val Loss: 4.2876 Acc: 0.3111\n",
            "\n",
            "Epoch 5/10\n",
            "Train Loss: 1.0764 Acc: 0.4262\n",
            "Val Loss: 1.0163 Acc: 0.4889\n",
            "\n",
            "Epoch 6/10\n",
            "Train Loss: 1.0104 Acc: 0.4146\n",
            "Val Loss: 0.9117 Acc: 0.5000\n",
            "\n",
            "Epoch 7/10\n",
            "Train Loss: 0.9455 Acc: 0.5091\n",
            "Val Loss: 0.8820 Acc: 0.5000\n",
            "\n",
            "Epoch 8/10\n",
            "Train Loss: 0.8776 Acc: 0.5274\n",
            "Val Loss: 0.8024 Acc: 0.5444\n",
            "\n",
            "Epoch 9/10\n",
            "Train Loss: 0.8521 Acc: 0.5456\n",
            "Val Loss: 0.7729 Acc: 0.5444\n",
            "\n",
            "Epoch 10/10\n",
            "Train Loss: 0.8340 Acc: 0.5439\n",
            "Val Loss: 0.7525 Acc: 0.5778\n",
            "\n",
            "\n",
            "Testing parameters: {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 10, 'learning_rate': 0.001, 'weight_decay': 0}\n",
            "Epoch 1/10\n",
            "Train Loss: 1.0849 Acc: 0.4129\n",
            "Val Loss: 1.0034 Acc: 0.6778\n",
            "\n",
            "Epoch 2/10\n",
            "Train Loss: 0.7888 Acc: 0.7098\n",
            "Val Loss: 0.5415 Acc: 0.8111\n",
            "\n",
            "Epoch 3/10\n",
            "Train Loss: 0.6402 Acc: 0.7512\n",
            "Val Loss: 0.5263 Acc: 0.7778\n",
            "\n",
            "Epoch 4/10\n",
            "Train Loss: 0.5410 Acc: 0.7877\n",
            "Val Loss: 0.4512 Acc: 0.8444\n",
            "\n",
            "Epoch 5/10\n",
            "Train Loss: 0.4598 Acc: 0.8192\n",
            "Val Loss: 0.6192 Acc: 0.7667\n",
            "\n",
            "Epoch 6/10\n",
            "Train Loss: 0.3390 Acc: 0.8740\n",
            "Val Loss: 0.5059 Acc: 0.8444\n",
            "\n",
            "Epoch 7/10\n",
            "Train Loss: 0.2331 Acc: 0.9138\n",
            "Val Loss: 0.5009 Acc: 0.8556\n",
            "\n",
            "Epoch 8/10\n",
            "Train Loss: 0.2580 Acc: 0.8988\n",
            "Val Loss: 0.3778 Acc: 0.8889\n",
            "\n",
            "Epoch 9/10\n",
            "Train Loss: 0.1829 Acc: 0.9237\n",
            "Val Loss: 0.3621 Acc: 0.9000\n",
            "\n",
            "Epoch 10/10\n",
            "Train Loss: 0.1638 Acc: 0.9370\n",
            "Val Loss: 0.3776 Acc: 0.9000\n",
            "\n",
            "\n",
            "Testing parameters: {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 10, 'learning_rate': 0.001, 'weight_decay': 1e-05}\n",
            "Epoch 1/10\n",
            "Train Loss: 1.0852 Acc: 0.4262\n",
            "Val Loss: 1.0083 Acc: 0.6667\n",
            "\n",
            "Epoch 2/10\n",
            "Train Loss: 0.8044 Acc: 0.7131\n",
            "Val Loss: 0.4496 Acc: 0.8222\n",
            "\n",
            "Epoch 3/10\n",
            "Train Loss: 0.6430 Acc: 0.7396\n",
            "Val Loss: 0.7257 Acc: 0.7778\n",
            "\n",
            "Epoch 4/10\n",
            "Train Loss: 0.6209 Acc: 0.7529\n",
            "Val Loss: 0.4623 Acc: 0.8222\n",
            "\n",
            "Epoch 5/10\n",
            "Train Loss: 0.4635 Acc: 0.8093\n",
            "Val Loss: 0.4261 Acc: 0.8333\n",
            "\n",
            "Epoch 6/10\n",
            "Train Loss: 0.3430 Acc: 0.8557\n",
            "Val Loss: 0.3801 Acc: 0.8444\n",
            "\n",
            "Epoch 7/10\n",
            "Train Loss: 0.2908 Acc: 0.8823\n",
            "Val Loss: 0.3726 Acc: 0.8556\n",
            "\n",
            "Epoch 8/10\n",
            "Train Loss: 0.2177 Acc: 0.9121\n",
            "Val Loss: 0.3502 Acc: 0.8444\n",
            "\n",
            "Epoch 9/10\n",
            "Train Loss: 0.1875 Acc: 0.9370\n",
            "Val Loss: 0.3448 Acc: 0.8778\n",
            "\n",
            "Epoch 10/10\n",
            "Train Loss: 0.2368 Acc: 0.9088\n",
            "Val Loss: 0.3252 Acc: 0.8889\n",
            "\n",
            "\n",
            "Testing parameters: {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 10, 'learning_rate': 0.001, 'weight_decay': 0.0001}\n",
            "Epoch 1/10\n",
            "Train Loss: 1.0819 Acc: 0.4046\n",
            "Val Loss: 1.0041 Acc: 0.6333\n",
            "\n",
            "Epoch 2/10\n",
            "Train Loss: 0.8129 Acc: 0.7264\n",
            "Val Loss: 0.5556 Acc: 0.8444\n",
            "\n",
            "Epoch 3/10\n",
            "Train Loss: 0.5571 Acc: 0.7761\n",
            "Val Loss: 0.5136 Acc: 0.8222\n",
            "\n",
            "Epoch 4/10\n",
            "Train Loss: 0.5424 Acc: 0.8027\n",
            "Val Loss: 0.5146 Acc: 0.8000\n",
            "\n",
            "Epoch 5/10\n",
            "Train Loss: 0.4338 Acc: 0.8308\n",
            "Val Loss: 0.5658 Acc: 0.8333\n",
            "\n",
            "Epoch 6/10\n",
            "Train Loss: 0.3151 Acc: 0.8706\n",
            "Val Loss: 0.4302 Acc: 0.8556\n",
            "\n",
            "Epoch 7/10\n",
            "Train Loss: 0.2375 Acc: 0.9088\n",
            "Val Loss: 0.3026 Acc: 0.8667\n",
            "\n",
            "Epoch 8/10\n",
            "Train Loss: 0.2832 Acc: 0.8889\n",
            "Val Loss: 0.4091 Acc: 0.8444\n",
            "\n",
            "Epoch 9/10\n",
            "Train Loss: 0.2131 Acc: 0.9154\n",
            "Val Loss: 0.3862 Acc: 0.8556\n",
            "\n",
            "Epoch 10/10\n",
            "Train Loss: 0.1971 Acc: 0.9204\n",
            "Val Loss: 0.3425 Acc: 0.8778\n",
            "\n",
            "\n",
            "Testing parameters: {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 10, 'learning_rate': 0.001, 'weight_decay': 0.001}\n",
            "Epoch 1/10\n",
            "Train Loss: 1.0796 Acc: 0.4395\n",
            "Val Loss: 0.9752 Acc: 0.7111\n",
            "\n",
            "Epoch 2/10\n",
            "Train Loss: 0.7601 Acc: 0.7463\n",
            "Val Loss: 0.4217 Acc: 0.8667\n",
            "\n",
            "Epoch 3/10\n",
            "Train Loss: 0.6196 Acc: 0.7629\n",
            "Val Loss: 0.6313 Acc: 0.7778\n",
            "\n",
            "Epoch 4/10\n",
            "Train Loss: 0.5524 Acc: 0.8027\n",
            "Val Loss: 0.6570 Acc: 0.8000\n",
            "\n",
            "Epoch 5/10\n",
            "Train Loss: 0.4414 Acc: 0.8176\n",
            "Val Loss: 0.4847 Acc: 0.8111\n",
            "\n",
            "Epoch 6/10\n",
            "Train Loss: 0.2921 Acc: 0.8856\n",
            "Val Loss: 0.4204 Acc: 0.8778\n",
            "\n",
            "Epoch 7/10\n",
            "Train Loss: 0.3075 Acc: 0.8905\n",
            "Val Loss: 0.4987 Acc: 0.8111\n",
            "\n",
            "Epoch 8/10\n",
            "Train Loss: 0.2718 Acc: 0.9104\n",
            "Val Loss: 0.4879 Acc: 0.8000\n",
            "\n",
            "Epoch 9/10\n",
            "Train Loss: 0.2388 Acc: 0.9022\n",
            "Val Loss: 0.4147 Acc: 0.8000\n",
            "\n",
            "Epoch 10/10\n",
            "Train Loss: 0.2140 Acc: 0.9088\n",
            "Val Loss: 0.3847 Acc: 0.8444\n",
            "\n",
            "\n",
            "Testing parameters: {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 10, 'learning_rate': 0.1, 'weight_decay': 0}\n",
            "Epoch 1/10\n",
            "Train Loss: 1.2085 Acc: 0.4129\n",
            "Val Loss: 542.1702 Acc: 0.3333\n",
            "\n",
            "Epoch 2/10\n",
            "Train Loss: 1.2792 Acc: 0.3217\n",
            "Val Loss: 1.1020 Acc: 0.3333\n",
            "\n",
            "Epoch 3/10\n",
            "Train Loss: 1.1056 Acc: 0.3167\n",
            "Val Loss: 1.1002 Acc: 0.3333\n",
            "\n",
            "Epoch 4/10\n",
            "Train Loss: 1.1031 Acc: 0.3085\n",
            "Val Loss: 1.0997 Acc: 0.3333\n",
            "\n",
            "Epoch 5/10\n",
            "Train Loss: 1.1034 Acc: 0.2919\n",
            "Val Loss: 1.1003 Acc: 0.3333\n",
            "\n",
            "Epoch 6/10\n",
            "Train Loss: 1.1032 Acc: 0.3400\n",
            "Val Loss: 1.0989 Acc: 0.3333\n",
            "\n",
            "Epoch 7/10\n",
            "Train Loss: 1.1035 Acc: 0.3234\n",
            "Val Loss: 1.0996 Acc: 0.3333\n",
            "\n",
            "Epoch 8/10\n",
            "Train Loss: 1.1014 Acc: 0.3167\n",
            "Val Loss: 1.0996 Acc: 0.3333\n",
            "\n",
            "Epoch 9/10\n",
            "Train Loss: 1.0993 Acc: 0.3333\n",
            "Val Loss: 1.0990 Acc: 0.3333\n",
            "\n",
            "Epoch 10/10\n",
            "Train Loss: 1.0990 Acc: 0.3333\n",
            "Val Loss: 1.0989 Acc: 0.3333\n",
            "\n",
            "\n",
            "Testing parameters: {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 10, 'learning_rate': 0.1, 'weight_decay': 1e-05}\n",
            "Epoch 1/10\n",
            "Train Loss: 1.2608 Acc: 0.4395\n",
            "Val Loss: 41365.4834 Acc: 0.3333\n",
            "\n",
            "Epoch 2/10\n",
            "Train Loss: 1.2590 Acc: 0.3532\n",
            "Val Loss: 5004.1719 Acc: 0.3556\n",
            "\n",
            "Epoch 3/10\n",
            "Train Loss: 1.1042 Acc: 0.2803\n",
            "Val Loss: 5.5828 Acc: 0.3222\n",
            "\n",
            "Epoch 4/10\n",
            "Train Loss: 1.1426 Acc: 0.3118\n",
            "Val Loss: 9.4475 Acc: 0.3667\n",
            "\n",
            "Epoch 5/10\n",
            "Train Loss: 1.1702 Acc: 0.3101\n",
            "Val Loss: 49.8044 Acc: 0.3333\n",
            "\n",
            "Epoch 6/10\n",
            "Train Loss: 1.1059 Acc: 0.3300\n",
            "Val Loss: 1.0993 Acc: 0.3333\n",
            "\n",
            "Epoch 7/10\n",
            "Train Loss: 1.1015 Acc: 0.3118\n",
            "Val Loss: 1.0991 Acc: 0.3333\n",
            "\n",
            "Epoch 8/10\n",
            "Train Loss: 1.0997 Acc: 0.3250\n",
            "Val Loss: 1.0996 Acc: 0.3333\n",
            "\n",
            "Epoch 9/10\n",
            "Train Loss: 1.0995 Acc: 0.3333\n",
            "Val Loss: 1.0991 Acc: 0.3333\n",
            "\n",
            "Epoch 10/10\n",
            "Train Loss: 1.0991 Acc: 0.3333\n",
            "Val Loss: 1.0990 Acc: 0.3333\n",
            "\n",
            "\n",
            "Testing parameters: {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 10, 'learning_rate': 0.1, 'weight_decay': 0.0001}\n",
            "Epoch 1/10\n",
            "Train Loss: 1.1852 Acc: 0.3731\n",
            "Val Loss: 20784.8521 Acc: 0.3333\n",
            "\n",
            "Epoch 2/10\n",
            "Train Loss: 1.1388 Acc: 0.3317\n",
            "Val Loss: 596398.1264 Acc: 0.3333\n",
            "\n",
            "Epoch 3/10\n",
            "Train Loss: 1.1026 Acc: 0.3085\n",
            "Val Loss: 171.7424 Acc: 0.3333\n",
            "\n",
            "Epoch 4/10\n",
            "Train Loss: 1.1023 Acc: 0.3234\n",
            "Val Loss: 1.1012 Acc: 0.3333\n",
            "\n",
            "Epoch 5/10\n",
            "Train Loss: 1.1097 Acc: 0.3184\n",
            "Val Loss: 1.1025 Acc: 0.3333\n",
            "\n",
            "Epoch 6/10\n",
            "Train Loss: 1.1073 Acc: 0.3416\n",
            "Val Loss: 46.1641 Acc: 0.3111\n",
            "\n",
            "Epoch 7/10\n",
            "Train Loss: 1.1004 Acc: 0.3284\n",
            "Val Loss: 1.0965 Acc: 0.3444\n",
            "\n",
            "Epoch 8/10\n",
            "Train Loss: 1.1012 Acc: 0.3333\n",
            "Val Loss: 1.0995 Acc: 0.3333\n",
            "\n",
            "Epoch 9/10\n",
            "Train Loss: 1.1001 Acc: 0.3068\n",
            "Val Loss: 1.0988 Acc: 0.3333\n",
            "\n",
            "Epoch 10/10\n",
            "Train Loss: 1.0988 Acc: 0.3333\n",
            "Val Loss: 1.0987 Acc: 0.3333\n",
            "\n",
            "\n",
            "Testing parameters: {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 10, 'learning_rate': 0.1, 'weight_decay': 0.001}\n",
            "Epoch 1/10\n",
            "Train Loss: 1.1794 Acc: 0.3914\n",
            "Val Loss: 18142.1041 Acc: 0.3778\n",
            "\n",
            "Epoch 2/10\n",
            "Train Loss: 1.1204 Acc: 0.3532\n",
            "Val Loss: 243942.0299 Acc: 0.3333\n",
            "\n",
            "Epoch 3/10\n",
            "Train Loss: 1.1818 Acc: 0.2902\n",
            "Val Loss: 15452.5605 Acc: 0.3333\n",
            "\n",
            "Epoch 4/10\n",
            "Train Loss: 1.2338 Acc: 0.3367\n",
            "Val Loss: 2419.1402 Acc: 0.3556\n",
            "\n",
            "Epoch 5/10\n",
            "Train Loss: 1.1121 Acc: 0.3167\n",
            "Val Loss: 8.5600 Acc: 0.3222\n",
            "\n",
            "Epoch 6/10\n",
            "Train Loss: 1.1539 Acc: 0.3383\n",
            "Val Loss: 2.1481 Acc: 0.3556\n",
            "\n",
            "Epoch 7/10\n",
            "Train Loss: 1.1022 Acc: 0.3234\n",
            "Val Loss: 1.0991 Acc: 0.3333\n",
            "\n",
            "Epoch 8/10\n",
            "Train Loss: 1.1004 Acc: 0.3118\n",
            "Val Loss: 1.0987 Acc: 0.3333\n",
            "\n",
            "Epoch 9/10\n",
            "Train Loss: 1.0987 Acc: 0.3333\n",
            "Val Loss: 1.0987 Acc: 0.3333\n",
            "\n",
            "Epoch 10/10\n",
            "Train Loss: 1.0988 Acc: 0.3333\n",
            "Val Loss: 1.0987 Acc: 0.3333\n",
            "\n",
            "\n",
            "Testing parameters: {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 10, 'learning_rate': 0.01, 'weight_decay': 0}\n",
            "Epoch 1/10\n",
            "Train Loss: 0.9305 Acc: 0.5473\n",
            "Val Loss: 2.2452 Acc: 0.6889\n",
            "\n",
            "Epoch 2/10\n",
            "Train Loss: 1.0318 Acc: 0.5357\n",
            "Val Loss: 193.7801 Acc: 0.3333\n",
            "\n",
            "Epoch 3/10\n",
            "Train Loss: 1.1017 Acc: 0.4063\n",
            "Val Loss: 16.3746 Acc: 0.3444\n",
            "\n",
            "Epoch 4/10\n",
            "Train Loss: 1.0768 Acc: 0.4030\n",
            "Val Loss: 2.1450 Acc: 0.3667\n",
            "\n",
            "Epoch 5/10\n",
            "Train Loss: 0.9985 Acc: 0.4809\n",
            "Val Loss: 1.0211 Acc: 0.4556\n",
            "\n",
            "Epoch 6/10\n",
            "Train Loss: 0.9666 Acc: 0.5141\n",
            "Val Loss: 5.7665 Acc: 0.3556\n",
            "\n",
            "Epoch 7/10\n",
            "Train Loss: 0.9941 Acc: 0.5141\n",
            "Val Loss: 0.8102 Acc: 0.6889\n",
            "\n",
            "Epoch 8/10\n",
            "Train Loss: 0.9445 Acc: 0.5473\n",
            "Val Loss: 0.7356 Acc: 0.7000\n",
            "\n",
            "Epoch 9/10\n",
            "Train Loss: 0.8151 Acc: 0.6302\n",
            "Val Loss: 0.6922 Acc: 0.6889\n",
            "\n",
            "Epoch 10/10\n",
            "Train Loss: 0.8190 Acc: 0.5837\n",
            "Val Loss: 0.6890 Acc: 0.6778\n",
            "\n",
            "\n",
            "Testing parameters: {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 10, 'learning_rate': 0.01, 'weight_decay': 1e-05}\n",
            "Epoch 1/10\n",
            "Train Loss: 0.9387 Acc: 0.5439\n",
            "Val Loss: 1.3660 Acc: 0.6778\n",
            "\n",
            "Epoch 2/10\n",
            "Train Loss: 1.0377 Acc: 0.5091\n",
            "Val Loss: 20.0503 Acc: 0.2778\n",
            "\n",
            "Epoch 3/10\n",
            "Train Loss: 1.1499 Acc: 0.3101\n",
            "Val Loss: 6.5467 Acc: 0.3000\n",
            "\n",
            "Epoch 4/10\n",
            "Train Loss: 1.1230 Acc: 0.3416\n",
            "Val Loss: 1.0957 Acc: 0.3889\n",
            "\n",
            "Epoch 5/10\n",
            "Train Loss: 1.0911 Acc: 0.3648\n",
            "Val Loss: 1.0896 Acc: 0.3667\n",
            "\n",
            "Epoch 6/10\n",
            "Train Loss: 1.0823 Acc: 0.3864\n",
            "Val Loss: 1.3244 Acc: 0.4222\n",
            "\n",
            "Epoch 7/10\n",
            "Train Loss: 1.0532 Acc: 0.4229\n",
            "Val Loss: 1.0616 Acc: 0.3778\n",
            "\n",
            "Epoch 8/10\n",
            "Train Loss: 1.0261 Acc: 0.4461\n",
            "Val Loss: 0.9500 Acc: 0.5667\n",
            "\n",
            "Epoch 9/10\n",
            "Train Loss: 0.9830 Acc: 0.4826\n",
            "Val Loss: 0.9573 Acc: 0.5333\n",
            "\n",
            "Epoch 10/10\n",
            "Train Loss: 0.9647 Acc: 0.4793\n",
            "Val Loss: 0.9507 Acc: 0.5333\n",
            "\n",
            "\n",
            "Testing parameters: {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 10, 'learning_rate': 0.01, 'weight_decay': 0.0001}\n",
            "Epoch 1/10\n",
            "Train Loss: 0.9311 Acc: 0.5672\n",
            "Val Loss: 1.2577 Acc: 0.7778\n",
            "\n",
            "Epoch 2/10\n",
            "Train Loss: 1.0990 Acc: 0.5556\n",
            "Val Loss: 3.5707 Acc: 0.3556\n",
            "\n",
            "Epoch 3/10\n",
            "Train Loss: 1.0615 Acc: 0.4610\n",
            "Val Loss: 60.2687 Acc: 0.2778\n",
            "\n",
            "Epoch 4/10\n",
            "Train Loss: 1.1111 Acc: 0.3831\n",
            "Val Loss: 1.0783 Acc: 0.3667\n",
            "\n",
            "Epoch 5/10\n",
            "Train Loss: 1.0644 Acc: 0.4577\n",
            "Val Loss: 0.9313 Acc: 0.5667\n",
            "\n",
            "Epoch 6/10\n",
            "Train Loss: 0.9991 Acc: 0.5108\n",
            "Val Loss: 0.9078 Acc: 0.5333\n",
            "\n",
            "Epoch 7/10\n",
            "Train Loss: 0.9833 Acc: 0.5141\n",
            "Val Loss: 0.8562 Acc: 0.5778\n",
            "\n",
            "Epoch 8/10\n",
            "Train Loss: 0.8810 Acc: 0.5605\n",
            "Val Loss: 0.7268 Acc: 0.6556\n",
            "\n",
            "Epoch 9/10\n",
            "Train Loss: 0.8029 Acc: 0.6401\n",
            "Val Loss: 0.6433 Acc: 0.7000\n",
            "\n",
            "Epoch 10/10\n",
            "Train Loss: 0.7740 Acc: 0.6285\n",
            "Val Loss: 0.6191 Acc: 0.7000\n",
            "\n",
            "\n",
            "Testing parameters: {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 10, 'learning_rate': 0.01, 'weight_decay': 0.001}\n",
            "Epoch 1/10\n",
            "Train Loss: 0.8721 Acc: 0.6136\n",
            "Val Loss: 1.6364 Acc: 0.7889\n",
            "\n",
            "Epoch 2/10\n",
            "Train Loss: 1.0878 Acc: 0.5373\n",
            "Val Loss: 4.5252 Acc: 0.3000\n",
            "\n",
            "Epoch 3/10\n",
            "Train Loss: 1.1015 Acc: 0.4511\n",
            "Val Loss: 2.7939 Acc: 0.2000\n",
            "\n",
            "Epoch 4/10\n",
            "Train Loss: 1.0637 Acc: 0.4279\n",
            "Val Loss: 2.4739 Acc: 0.3444\n",
            "\n",
            "Epoch 5/10\n",
            "Train Loss: 1.0858 Acc: 0.3781\n",
            "Val Loss: 1.2769 Acc: 0.2778\n",
            "\n",
            "Epoch 6/10\n",
            "Train Loss: 1.0257 Acc: 0.5224\n",
            "Val Loss: 0.9943 Acc: 0.5000\n",
            "\n",
            "Epoch 7/10\n",
            "Train Loss: 0.9621 Acc: 0.5174\n",
            "Val Loss: 0.9996 Acc: 0.5000\n",
            "\n",
            "Epoch 8/10\n",
            "Train Loss: 0.8836 Acc: 0.5622\n",
            "Val Loss: 0.7887 Acc: 0.5778\n",
            "\n",
            "Epoch 9/10\n",
            "Train Loss: 0.8404 Acc: 0.6003\n",
            "Val Loss: 0.7164 Acc: 0.5556\n",
            "\n",
            "Epoch 10/10\n",
            "Train Loss: 0.8031 Acc: 0.6153\n",
            "Val Loss: 0.7332 Acc: 0.5667\n",
            "\n",
            "\n",
            "Testing parameters: {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 10, 'learning_rate': 0.001, 'weight_decay': 0}\n",
            "Epoch 1/10\n",
            "Train Loss: 1.0896 Acc: 0.3715\n",
            "Val Loss: 1.0228 Acc: 0.7556\n",
            "\n",
            "Epoch 2/10\n",
            "Train Loss: 0.8968 Acc: 0.6385\n",
            "Val Loss: 0.4740 Acc: 0.8000\n",
            "\n",
            "Epoch 3/10\n",
            "Train Loss: 0.5785 Acc: 0.7761\n",
            "Val Loss: 0.3706 Acc: 0.8556\n",
            "\n",
            "Epoch 4/10\n",
            "Train Loss: 0.5805 Acc: 0.7612\n",
            "Val Loss: 0.6970 Acc: 0.7889\n",
            "\n",
            "Epoch 5/10\n",
            "Train Loss: 0.5404 Acc: 0.7794\n",
            "Val Loss: 0.5363 Acc: 0.7889\n",
            "\n",
            "Epoch 6/10\n",
            "Train Loss: 0.3605 Acc: 0.8607\n",
            "Val Loss: 0.3351 Acc: 0.8333\n",
            "\n",
            "Epoch 7/10\n",
            "Train Loss: 0.2790 Acc: 0.8889\n",
            "Val Loss: 0.3636 Acc: 0.8778\n",
            "\n",
            "Epoch 8/10\n",
            "Train Loss: 0.2620 Acc: 0.8955\n",
            "Val Loss: 0.4026 Acc: 0.8444\n",
            "\n",
            "Epoch 9/10\n",
            "Train Loss: 0.1832 Acc: 0.9420\n",
            "Val Loss: 0.3778 Acc: 0.8556\n",
            "\n",
            "Epoch 10/10\n",
            "Train Loss: 0.2052 Acc: 0.9154\n",
            "Val Loss: 0.3732 Acc: 0.8667\n",
            "\n",
            "\n",
            "Testing parameters: {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 10, 'learning_rate': 0.001, 'weight_decay': 1e-05}\n",
            "Epoch 1/10\n",
            "Train Loss: 1.0816 Acc: 0.4129\n",
            "Val Loss: 1.0061 Acc: 0.6556\n",
            "\n",
            "Epoch 2/10\n",
            "Train Loss: 0.8399 Acc: 0.6716\n",
            "Val Loss: 0.5370 Acc: 0.7444\n",
            "\n",
            "Epoch 3/10\n",
            "Train Loss: 0.5936 Acc: 0.7529\n",
            "Val Loss: 1.1694 Acc: 0.6000\n",
            "\n",
            "Epoch 4/10\n",
            "Train Loss: 0.5541 Acc: 0.7761\n",
            "Val Loss: 0.5044 Acc: 0.8556\n",
            "\n",
            "Epoch 5/10\n",
            "Train Loss: 0.5359 Acc: 0.7944\n",
            "Val Loss: 0.6382 Acc: 0.7333\n",
            "\n",
            "Epoch 6/10\n",
            "Train Loss: 0.3854 Acc: 0.8574\n",
            "Val Loss: 0.4035 Acc: 0.8333\n",
            "\n",
            "Epoch 7/10\n",
            "Train Loss: 0.2722 Acc: 0.8972\n",
            "Val Loss: 0.3498 Acc: 0.8778\n",
            "\n",
            "Epoch 8/10\n",
            "Train Loss: 0.2520 Acc: 0.8955\n",
            "Val Loss: 0.3622 Acc: 0.8667\n",
            "\n",
            "Epoch 9/10\n",
            "Train Loss: 0.2069 Acc: 0.9237\n",
            "Val Loss: 0.3696 Acc: 0.8667\n",
            "\n",
            "Epoch 10/10\n",
            "Train Loss: 0.2723 Acc: 0.9171\n",
            "Val Loss: 0.4003 Acc: 0.8444\n",
            "\n",
            "\n",
            "Testing parameters: {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 10, 'learning_rate': 0.001, 'weight_decay': 0.0001}\n",
            "Epoch 1/10\n",
            "Train Loss: 1.0860 Acc: 0.3864\n",
            "Val Loss: 1.0031 Acc: 0.6444\n",
            "\n",
            "Epoch 2/10\n",
            "Train Loss: 0.8808 Acc: 0.6584\n",
            "Val Loss: 0.5073 Acc: 0.8000\n",
            "\n",
            "Epoch 3/10\n",
            "Train Loss: 0.5330 Acc: 0.7927\n",
            "Val Loss: 1.2535 Acc: 0.6889\n",
            "\n",
            "Epoch 4/10\n",
            "Train Loss: 0.5745 Acc: 0.7761\n",
            "Val Loss: 0.4780 Acc: 0.8000\n",
            "\n",
            "Epoch 5/10\n",
            "Train Loss: 0.4525 Acc: 0.8209\n",
            "Val Loss: 0.6173 Acc: 0.8111\n",
            "\n",
            "Epoch 6/10\n",
            "Train Loss: 0.4134 Acc: 0.8391\n",
            "Val Loss: 0.4850 Acc: 0.8333\n",
            "\n",
            "Epoch 7/10\n",
            "Train Loss: 0.2978 Acc: 0.8856\n",
            "Val Loss: 0.2580 Acc: 0.8778\n",
            "\n",
            "Epoch 8/10\n",
            "Train Loss: 0.3203 Acc: 0.8955\n",
            "Val Loss: 0.2299 Acc: 0.9333\n",
            "\n",
            "Epoch 9/10\n",
            "Train Loss: 0.2207 Acc: 0.9287\n",
            "Val Loss: 0.2554 Acc: 0.9000\n",
            "\n",
            "Epoch 10/10\n",
            "Train Loss: 0.2558 Acc: 0.9104\n",
            "Val Loss: 0.2632 Acc: 0.9000\n",
            "\n",
            "\n",
            "Testing parameters: {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 10, 'learning_rate': 0.001, 'weight_decay': 0.001}\n",
            "Epoch 1/10\n",
            "Train Loss: 1.0839 Acc: 0.3748\n",
            "Val Loss: 0.9966 Acc: 0.6222\n",
            "\n",
            "Epoch 2/10\n",
            "Train Loss: 0.8494 Acc: 0.7098\n",
            "Val Loss: 0.6729 Acc: 0.7889\n",
            "\n",
            "Epoch 3/10\n",
            "Train Loss: 0.6373 Acc: 0.7297\n",
            "Val Loss: 0.6146 Acc: 0.7778\n",
            "\n",
            "Epoch 4/10\n",
            "Train Loss: 0.5266 Acc: 0.8027\n",
            "Val Loss: 0.5860 Acc: 0.8111\n",
            "\n",
            "Epoch 5/10\n",
            "Train Loss: 0.4583 Acc: 0.8325\n",
            "Val Loss: 0.3271 Acc: 0.8667\n",
            "\n",
            "Epoch 6/10\n",
            "Train Loss: 0.3802 Acc: 0.8590\n",
            "Val Loss: 0.3026 Acc: 0.9111\n",
            "\n",
            "Epoch 7/10\n",
            "Train Loss: 0.2830 Acc: 0.8905\n",
            "Val Loss: 0.2755 Acc: 0.9111\n",
            "\n",
            "Epoch 8/10\n",
            "Train Loss: 0.2646 Acc: 0.8955\n",
            "Val Loss: 0.2552 Acc: 0.9222\n",
            "\n",
            "Epoch 9/10\n",
            "Train Loss: 0.2335 Acc: 0.9121\n",
            "Val Loss: 0.3198 Acc: 0.9222\n",
            "\n",
            "Epoch 10/10\n",
            "Train Loss: 0.2066 Acc: 0.9171\n",
            "Val Loss: 0.3038 Acc: 0.9222\n",
            "\n",
            "\n",
            "Testing parameters: {'batch_size': 32, 'dropout_rate': 0.5, 'epochs': 10, 'learning_rate': 0.1, 'weight_decay': 0}\n",
            "Epoch 1/10\n",
            "Train Loss: 1.2572 Acc: 0.4063\n",
            "Val Loss: 4587.0767 Acc: 0.3333\n",
            "\n",
            "Epoch 2/10\n",
            "Train Loss: 1.1505 Acc: 0.3300\n",
            "Val Loss: 4700.1833 Acc: 0.3333\n",
            "\n",
            "Epoch 3/10\n",
            "Train Loss: 1.1781 Acc: 0.3201\n",
            "Val Loss: 14493.6083 Acc: 0.3333\n",
            "\n",
            "Epoch 4/10\n",
            "Train Loss: 1.1024 Acc: 0.3317\n",
            "Val Loss: 16.5484 Acc: 0.3333\n",
            "\n",
            "Epoch 5/10\n",
            "Train Loss: 1.1018 Acc: 0.3018\n",
            "Val Loss: 1.0997 Acc: 0.3333\n",
            "\n",
            "Epoch 6/10\n",
            "Train Loss: 1.1029 Acc: 0.2935\n",
            "Val Loss: 1.0989 Acc: 0.3333\n",
            "\n",
            "Epoch 7/10\n",
            "Train Loss: 1.0993 Acc: 0.3134\n",
            "Val Loss: 1.0991 Acc: 0.3333\n",
            "\n",
            "Epoch 8/10\n",
            "Train Loss: 1.0993 Acc: 0.3333\n",
            "Val Loss: 1.0986 Acc: 0.3333\n",
            "\n",
            "Epoch 9/10\n",
            "Train Loss: 1.0992 Acc: 0.3051\n",
            "Val Loss: 1.0987 Acc: 0.3333\n",
            "\n",
            "Epoch 10/10\n",
            "Train Loss: 1.0988 Acc: 0.3333\n",
            "Val Loss: 1.0987 Acc: 0.3333\n",
            "\n",
            "\n",
            "Testing parameters: {'batch_size': 32, 'dropout_rate': 0.5, 'epochs': 10, 'learning_rate': 0.1, 'weight_decay': 1e-05}\n",
            "Epoch 1/10\n",
            "Train Loss: 1.2842 Acc: 0.3964\n",
            "Val Loss: 138292.2642 Acc: 0.3333\n",
            "\n",
            "Epoch 2/10\n",
            "Train Loss: 1.1465 Acc: 0.2703\n",
            "Val Loss: 434.6913 Acc: 0.3333\n",
            "\n",
            "Epoch 3/10\n",
            "Train Loss: 1.1051 Acc: 0.3217\n",
            "Val Loss: 1.3013 Acc: 0.3222\n",
            "\n",
            "Epoch 4/10\n",
            "Train Loss: 1.1052 Acc: 0.3035\n",
            "Val Loss: 1.1016 Acc: 0.3333\n",
            "\n",
            "Epoch 5/10\n",
            "Train Loss: 1.1060 Acc: 0.3317\n",
            "Val Loss: 1.0987 Acc: 0.3333\n",
            "\n",
            "Epoch 6/10\n",
            "Train Loss: 1.1037 Acc: 0.3217\n",
            "Val Loss: 1.0989 Acc: 0.3333\n",
            "\n",
            "Epoch 7/10\n",
            "Train Loss: 1.1030 Acc: 0.3350\n",
            "Val Loss: 1.0991 Acc: 0.3333\n",
            "\n",
            "Epoch 8/10\n",
            "Train Loss: 1.1000 Acc: 0.3201\n",
            "Val Loss: 1.0990 Acc: 0.3333\n",
            "\n",
            "Epoch 9/10\n",
            "Train Loss: 1.0991 Acc: 0.3333\n",
            "Val Loss: 1.0989 Acc: 0.3333\n",
            "\n",
            "Epoch 10/10\n",
            "Train Loss: 1.0991 Acc: 0.3333\n",
            "Val Loss: 1.0988 Acc: 0.3333\n",
            "\n",
            "\n",
            "Testing parameters: {'batch_size': 32, 'dropout_rate': 0.5, 'epochs': 10, 'learning_rate': 0.1, 'weight_decay': 0.0001}\n",
            "Epoch 1/10\n",
            "Train Loss: 1.1936 Acc: 0.3831\n",
            "Val Loss: 4215.0600 Acc: 0.3333\n",
            "\n",
            "Epoch 2/10\n",
            "Train Loss: 1.2152 Acc: 0.3284\n",
            "Val Loss: 52968.1817 Acc: 0.3333\n",
            "\n",
            "Epoch 3/10\n",
            "Train Loss: 1.1038 Acc: 0.3184\n",
            "Val Loss: 1.4750 Acc: 0.3444\n",
            "\n",
            "Epoch 4/10\n",
            "Train Loss: 1.1029 Acc: 0.2935\n",
            "Val Loss: 1.1000 Acc: 0.3333\n",
            "\n",
            "Epoch 5/10\n",
            "Train Loss: 1.1060 Acc: 0.3134\n",
            "Val Loss: 1.1029 Acc: 0.3333\n",
            "\n",
            "Epoch 6/10\n",
            "Train Loss: 1.1044 Acc: 0.3101\n",
            "Val Loss: 1.0986 Acc: 0.3333\n",
            "\n",
            "Epoch 7/10\n",
            "Train Loss: 1.1014 Acc: 0.3101\n",
            "Val Loss: 1.0998 Acc: 0.3333\n",
            "\n",
            "Epoch 8/10\n",
            "Train Loss: 1.0995 Acc: 0.3333\n",
            "Val Loss: 1.0986 Acc: 0.3333\n",
            "\n",
            "Epoch 9/10\n",
            "Train Loss: 1.0993 Acc: 0.3350\n",
            "Val Loss: 1.0988 Acc: 0.3333\n",
            "\n",
            "Epoch 10/10\n",
            "Train Loss: 1.0988 Acc: 0.3333\n",
            "Val Loss: 1.0987 Acc: 0.3333\n",
            "\n",
            "\n",
            "Testing parameters: {'batch_size': 32, 'dropout_rate': 0.5, 'epochs': 10, 'learning_rate': 0.1, 'weight_decay': 0.001}\n",
            "Epoch 1/10\n",
            "Train Loss: 1.2021 Acc: 0.4262\n",
            "Val Loss: 545.3758 Acc: 0.2667\n",
            "\n",
            "Epoch 2/10\n",
            "Train Loss: 1.2121 Acc: 0.3184\n",
            "Val Loss: 2097577.3444 Acc: 0.3333\n",
            "\n",
            "Epoch 3/10\n",
            "Train Loss: 1.2127 Acc: 0.3582\n",
            "Val Loss: 980.4558 Acc: 0.3222\n",
            "\n",
            "Epoch 4/10\n",
            "Train Loss: 8.2241 Acc: 0.3267\n",
            "Val Loss: 3308865399.4667 Acc: 0.3333\n",
            "\n",
            "Epoch 5/10\n",
            "Train Loss: 1.5411 Acc: 0.3051\n",
            "Val Loss: 266.5387 Acc: 0.3333\n",
            "\n",
            "Epoch 6/10\n",
            "Train Loss: 1.0973 Acc: 0.3267\n",
            "Val Loss: 1.0888 Acc: 0.3333\n",
            "\n",
            "Epoch 7/10\n",
            "Train Loss: 1.0999 Acc: 0.3284\n",
            "Val Loss: 1.0987 Acc: 0.3333\n",
            "\n",
            "Epoch 8/10\n",
            "Train Loss: 1.0996 Acc: 0.3151\n",
            "Val Loss: 1.0988 Acc: 0.3333\n",
            "\n",
            "Epoch 9/10\n",
            "Train Loss: 1.0988 Acc: 0.3333\n",
            "Val Loss: 1.0987 Acc: 0.3333\n",
            "\n",
            "Epoch 10/10\n",
            "Train Loss: 1.0987 Acc: 0.3333\n",
            "Val Loss: 1.0987 Acc: 0.3333\n",
            "\n",
            "\n",
            "Testing parameters: {'batch_size': 32, 'dropout_rate': 0.5, 'epochs': 10, 'learning_rate': 0.01, 'weight_decay': 0}\n",
            "Epoch 1/10\n",
            "Train Loss: 0.9165 Acc: 0.5207\n",
            "Val Loss: 0.9579 Acc: 0.7111\n",
            "\n",
            "Epoch 2/10\n",
            "Train Loss: 1.0581 Acc: 0.5456\n",
            "Val Loss: 156.4740 Acc: 0.3444\n",
            "\n",
            "Epoch 3/10\n",
            "Train Loss: 1.1862 Acc: 0.3167\n",
            "Val Loss: 3.1482 Acc: 0.3333\n",
            "\n",
            "Epoch 4/10\n",
            "Train Loss: 1.1117 Acc: 0.3466\n",
            "Val Loss: 1.0855 Acc: 0.4111\n",
            "\n",
            "Epoch 5/10\n",
            "Train Loss: 1.1232 Acc: 0.3831\n",
            "Val Loss: 1.1013 Acc: 0.3222\n",
            "\n",
            "Epoch 6/10\n",
            "Train Loss: 1.0939 Acc: 0.3748\n",
            "Val Loss: 1.0866 Acc: 0.3000\n",
            "\n",
            "Epoch 7/10\n",
            "Train Loss: 1.0792 Acc: 0.3847\n",
            "Val Loss: 0.9680 Acc: 0.4889\n",
            "\n",
            "Epoch 8/10\n",
            "Train Loss: 1.0387 Acc: 0.4295\n",
            "Val Loss: 0.9667 Acc: 0.5333\n",
            "\n",
            "Epoch 9/10\n",
            "Train Loss: 1.0374 Acc: 0.4411\n",
            "Val Loss: 0.9521 Acc: 0.4556\n",
            "\n",
            "Epoch 10/10\n",
            "Train Loss: 0.9743 Acc: 0.4992\n",
            "Val Loss: 0.9402 Acc: 0.5333\n",
            "\n",
            "\n",
            "Testing parameters: {'batch_size': 32, 'dropout_rate': 0.5, 'epochs': 10, 'learning_rate': 0.01, 'weight_decay': 1e-05}\n",
            "Epoch 1/10\n",
            "Train Loss: 0.9461 Acc: 0.5506\n",
            "Val Loss: 1.3500 Acc: 0.6556\n",
            "\n",
            "Epoch 2/10\n",
            "Train Loss: 1.0040 Acc: 0.5423\n",
            "Val Loss: 1.2282 Acc: 0.3111\n",
            "\n",
            "Epoch 3/10\n",
            "Train Loss: 1.1278 Acc: 0.3416\n",
            "Val Loss: 1.3440 Acc: 0.3000\n",
            "\n",
            "Epoch 4/10\n",
            "Train Loss: 1.1081 Acc: 0.3167\n",
            "Val Loss: 1.1072 Acc: 0.3556\n",
            "\n",
            "Epoch 5/10\n",
            "Train Loss: 1.1023 Acc: 0.3333\n",
            "Val Loss: 1.0970 Acc: 0.3333\n",
            "\n",
            "Epoch 6/10\n",
            "Train Loss: 1.0966 Acc: 0.3400\n",
            "Val Loss: 1.6849 Acc: 0.3222\n",
            "\n",
            "Epoch 7/10\n",
            "Train Loss: 1.1091 Acc: 0.3333\n",
            "Val Loss: 1.0979 Acc: 0.4111\n",
            "\n",
            "Epoch 8/10\n",
            "Train Loss: 1.0990 Acc: 0.3483\n",
            "Val Loss: 1.0928 Acc: 0.4333\n",
            "\n",
            "Epoch 9/10\n",
            "Train Loss: 1.0943 Acc: 0.3632\n",
            "Val Loss: 1.0950 Acc: 0.4111\n",
            "\n",
            "Epoch 10/10\n",
            "Train Loss: 1.0960 Acc: 0.3483\n",
            "Val Loss: 1.0940 Acc: 0.4000\n",
            "\n",
            "\n",
            "Testing parameters: {'batch_size': 32, 'dropout_rate': 0.5, 'epochs': 10, 'learning_rate': 0.01, 'weight_decay': 0.0001}\n",
            "Epoch 1/10\n",
            "Train Loss: 0.9586 Acc: 0.5091\n",
            "Val Loss: 0.7007 Acc: 0.7778\n",
            "\n",
            "Epoch 2/10\n",
            "Train Loss: 1.0217 Acc: 0.5439\n",
            "Val Loss: 22.0863 Acc: 0.3556\n",
            "\n",
            "Epoch 3/10\n",
            "Train Loss: 1.0692 Acc: 0.4693\n",
            "Val Loss: 1.1585 Acc: 0.3889\n",
            "\n",
            "Epoch 4/10\n",
            "Train Loss: 1.0732 Acc: 0.4345\n",
            "Val Loss: 12.9639 Acc: 0.3444\n",
            "\n",
            "Epoch 5/10\n",
            "Train Loss: 0.9862 Acc: 0.4461\n",
            "Val Loss: 3.2146 Acc: 0.4000\n",
            "\n",
            "Epoch 6/10\n",
            "Train Loss: 0.9778 Acc: 0.4892\n",
            "Val Loss: 0.7714 Acc: 0.6000\n",
            "\n",
            "Epoch 7/10\n",
            "Train Loss: 0.9071 Acc: 0.5357\n",
            "Val Loss: 0.7603 Acc: 0.6000\n",
            "\n",
            "Epoch 8/10\n",
            "Train Loss: 0.8205 Acc: 0.5638\n",
            "Val Loss: 0.7927 Acc: 0.6000\n",
            "\n",
            "Epoch 9/10\n",
            "Train Loss: 0.8146 Acc: 0.5290\n",
            "Val Loss: 0.7064 Acc: 0.6111\n",
            "\n",
            "Epoch 10/10\n",
            "Train Loss: 0.7849 Acc: 0.5622\n",
            "Val Loss: 0.7029 Acc: 0.5889\n",
            "\n",
            "\n",
            "Testing parameters: {'batch_size': 32, 'dropout_rate': 0.5, 'epochs': 10, 'learning_rate': 0.01, 'weight_decay': 0.001}\n",
            "Epoch 1/10\n",
            "Train Loss: 0.9332 Acc: 0.5522\n",
            "Val Loss: 1.9460 Acc: 0.6889\n",
            "\n",
            "Epoch 2/10\n",
            "Train Loss: 1.0036 Acc: 0.5821\n",
            "Val Loss: 2.7278 Acc: 0.2889\n",
            "\n",
            "Epoch 3/10\n",
            "Train Loss: 1.1100 Acc: 0.4096\n",
            "Val Loss: 1.0679 Acc: 0.3333\n",
            "\n",
            "Epoch 4/10\n",
            "Train Loss: 0.9903 Acc: 0.5340\n",
            "Val Loss: 1.2082 Acc: 0.4111\n",
            "\n",
            "Epoch 5/10\n",
            "Train Loss: 0.9952 Acc: 0.4909\n",
            "Val Loss: 41.1858 Acc: 0.4222\n",
            "\n",
            "Epoch 6/10\n",
            "Train Loss: 1.0865 Acc: 0.4378\n",
            "Val Loss: 0.9578 Acc: 0.4778\n",
            "\n",
            "Epoch 7/10\n",
            "Train Loss: 1.0098 Acc: 0.4494\n",
            "Val Loss: 0.8656 Acc: 0.6222\n",
            "\n",
            "Epoch 8/10\n",
            "Train Loss: 0.9304 Acc: 0.5539\n",
            "Val Loss: 0.7689 Acc: 0.6222\n",
            "\n",
            "Epoch 9/10\n",
            "Train Loss: 0.8845 Acc: 0.5489\n",
            "Val Loss: 0.7379 Acc: 0.7000\n",
            "\n",
            "Epoch 10/10\n",
            "Train Loss: 0.9011 Acc: 0.5804\n",
            "Val Loss: 0.7380 Acc: 0.7000\n",
            "\n",
            "\n",
            "Testing parameters: {'batch_size': 32, 'dropout_rate': 0.5, 'epochs': 10, 'learning_rate': 0.001, 'weight_decay': 0}\n",
            "Epoch 1/10\n",
            "Train Loss: 1.0971 Acc: 0.3665\n",
            "Val Loss: 1.0372 Acc: 0.6444\n",
            "\n",
            "Epoch 2/10\n",
            "Train Loss: 0.8771 Acc: 0.6633\n",
            "Val Loss: 0.4726 Acc: 0.8000\n",
            "\n",
            "Epoch 3/10\n",
            "Train Loss: 0.6518 Acc: 0.7512\n",
            "Val Loss: 0.5477 Acc: 0.7889\n",
            "\n",
            "Epoch 4/10\n",
            "Train Loss: 0.5701 Acc: 0.7711\n",
            "Val Loss: 0.6267 Acc: 0.7444\n",
            "\n",
            "Epoch 5/10\n",
            "Train Loss: 0.4904 Acc: 0.8027\n",
            "Val Loss: 0.3375 Acc: 0.8889\n",
            "\n",
            "Epoch 6/10\n",
            "Train Loss: 0.3879 Acc: 0.8342\n",
            "Val Loss: 0.2766 Acc: 0.9222\n",
            "\n",
            "Epoch 7/10\n",
            "Train Loss: 0.3024 Acc: 0.8806\n",
            "Val Loss: 0.3358 Acc: 0.9222\n",
            "\n",
            "Epoch 8/10\n",
            "Train Loss: 0.2518 Acc: 0.8988\n",
            "Val Loss: 0.3547 Acc: 0.8889\n",
            "\n",
            "Epoch 9/10\n",
            "Train Loss: 0.2490 Acc: 0.9055\n",
            "Val Loss: 0.3253 Acc: 0.9000\n",
            "\n",
            "Epoch 10/10\n",
            "Train Loss: 0.2227 Acc: 0.9187\n",
            "Val Loss: 0.3341 Acc: 0.8889\n",
            "\n",
            "\n",
            "Testing parameters: {'batch_size': 32, 'dropout_rate': 0.5, 'epochs': 10, 'learning_rate': 0.001, 'weight_decay': 1e-05}\n",
            "Epoch 1/10\n",
            "Train Loss: 1.0877 Acc: 0.3698\n",
            "Val Loss: 1.0265 Acc: 0.7778\n",
            "\n",
            "Epoch 2/10\n",
            "Train Loss: 0.8939 Acc: 0.6318\n",
            "Val Loss: 0.3828 Acc: 0.8444\n",
            "\n",
            "Epoch 3/10\n",
            "Train Loss: 0.6812 Acc: 0.7280\n",
            "Val Loss: 0.7152 Acc: 0.7000\n",
            "\n",
            "Epoch 4/10\n",
            "Train Loss: 0.5385 Acc: 0.7877\n",
            "Val Loss: 0.4367 Acc: 0.8778\n",
            "\n",
            "Epoch 5/10\n",
            "Train Loss: 0.4198 Acc: 0.8143\n",
            "Val Loss: 0.4148 Acc: 0.8444\n",
            "\n",
            "Epoch 6/10\n",
            "Train Loss: 0.3456 Acc: 0.8657\n",
            "Val Loss: 0.5015 Acc: 0.8556\n",
            "\n",
            "Epoch 7/10\n",
            "Train Loss: 0.3102 Acc: 0.8872\n",
            "Val Loss: 0.4207 Acc: 0.8667\n",
            "\n",
            "Epoch 8/10\n",
            "Train Loss: 0.2082 Acc: 0.9221\n",
            "Val Loss: 0.3878 Acc: 0.8889\n",
            "\n",
            "Epoch 9/10\n",
            "Train Loss: 0.2020 Acc: 0.9270\n",
            "Val Loss: 0.3818 Acc: 0.9111\n",
            "\n",
            "Epoch 10/10\n",
            "Train Loss: 0.2006 Acc: 0.9221\n",
            "Val Loss: 0.3829 Acc: 0.9111\n",
            "\n",
            "\n",
            "Testing parameters: {'batch_size': 32, 'dropout_rate': 0.5, 'epochs': 10, 'learning_rate': 0.001, 'weight_decay': 0.0001}\n",
            "Epoch 1/10\n",
            "Train Loss: 1.0949 Acc: 0.3566\n",
            "Val Loss: 1.0346 Acc: 0.6333\n",
            "\n",
            "Epoch 2/10\n",
            "Train Loss: 0.8772 Acc: 0.6667\n",
            "Val Loss: 0.5832 Acc: 0.7556\n",
            "\n",
            "Epoch 3/10\n",
            "Train Loss: 0.5565 Acc: 0.7745\n",
            "Val Loss: 1.1816 Acc: 0.6889\n",
            "\n",
            "Epoch 4/10\n",
            "Train Loss: 0.5406 Acc: 0.7828\n",
            "Val Loss: 0.5594 Acc: 0.8111\n",
            "\n",
            "Epoch 5/10\n",
            "Train Loss: 0.5207 Acc: 0.7828\n",
            "Val Loss: 0.4873 Acc: 0.8222\n",
            "\n",
            "Epoch 6/10\n",
            "Train Loss: 0.2980 Acc: 0.8889\n",
            "Val Loss: 0.5690 Acc: 0.7778\n",
            "\n",
            "Epoch 7/10\n",
            "Train Loss: 0.3164 Acc: 0.8756\n",
            "Val Loss: 0.5617 Acc: 0.7889\n",
            "\n",
            "Epoch 8/10\n",
            "Train Loss: 0.2679 Acc: 0.8856\n",
            "Val Loss: 0.4694 Acc: 0.8222\n",
            "\n",
            "Epoch 9/10\n",
            "Train Loss: 0.2309 Acc: 0.9071\n",
            "Val Loss: 0.4529 Acc: 0.8444\n",
            "\n",
            "Epoch 10/10\n",
            "Train Loss: 0.2183 Acc: 0.9171\n",
            "Val Loss: 0.4443 Acc: 0.8444\n",
            "\n",
            "\n",
            "Testing parameters: {'batch_size': 32, 'dropout_rate': 0.5, 'epochs': 10, 'learning_rate': 0.001, 'weight_decay': 0.001}\n",
            "Epoch 1/10\n",
            "Train Loss: 1.0822 Acc: 0.3997\n",
            "Val Loss: 1.0285 Acc: 0.7222\n",
            "\n",
            "Epoch 2/10\n",
            "Train Loss: 0.8786 Acc: 0.6733\n",
            "Val Loss: 0.4924 Acc: 0.8222\n",
            "\n",
            "Epoch 3/10\n",
            "Train Loss: 0.6352 Acc: 0.7330\n",
            "Val Loss: 0.4937 Acc: 0.8000\n",
            "\n",
            "Epoch 4/10\n",
            "Train Loss: 0.4782 Acc: 0.8226\n",
            "Val Loss: 1.0119 Acc: 0.7111\n",
            "\n",
            "Epoch 5/10\n",
            "Train Loss: 0.5086 Acc: 0.7977\n",
            "Val Loss: 0.7553 Acc: 0.8000\n",
            "\n",
            "Epoch 6/10\n",
            "Train Loss: 0.3499 Acc: 0.8640\n",
            "Val Loss: 0.4340 Acc: 0.8556\n",
            "\n",
            "Epoch 7/10\n",
            "Train Loss: 0.3251 Acc: 0.8756\n",
            "Val Loss: 0.4320 Acc: 0.8333\n",
            "\n",
            "Epoch 8/10\n",
            "Train Loss: 0.2039 Acc: 0.9320\n",
            "Val Loss: 0.3847 Acc: 0.8556\n",
            "\n",
            "Epoch 9/10\n",
            "Train Loss: 0.2507 Acc: 0.9038\n",
            "Val Loss: 0.3809 Acc: 0.8667\n",
            "\n",
            "Epoch 10/10\n",
            "Train Loss: 0.1847 Acc: 0.9370\n",
            "Val Loss: 0.3732 Acc: 0.8667\n",
            "\n",
            "\n",
            "Testing parameters: {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 10, 'learning_rate': 0.1, 'weight_decay': 0}\n",
            "Epoch 1/10\n",
            "Train Loss: 1.1852 Acc: 0.4975\n",
            "Val Loss: 11641.8348 Acc: 0.3333\n",
            "\n",
            "Epoch 2/10\n",
            "Train Loss: 1.2591 Acc: 0.3018\n",
            "Val Loss: 1567729982.5778 Acc: 0.3333\n",
            "\n",
            "Epoch 3/10\n",
            "Train Loss: 1.1170 Acc: 0.3234\n",
            "Val Loss: 1.0992 Acc: 0.3333\n",
            "\n",
            "Epoch 4/10\n",
            "Train Loss: 1.0992 Acc: 0.3400\n",
            "Val Loss: 1.0995 Acc: 0.3333\n",
            "\n",
            "Epoch 5/10\n",
            "Train Loss: 1.1007 Acc: 0.3118\n",
            "Val Loss: 1.0989 Acc: 0.3333\n",
            "\n",
            "Epoch 6/10\n",
            "Train Loss: 1.0994 Acc: 0.3333\n",
            "Val Loss: 1.0987 Acc: 0.3333\n",
            "\n",
            "Epoch 7/10\n",
            "Train Loss: 1.1001 Acc: 0.3333\n",
            "Val Loss: 1.0991 Acc: 0.3333\n",
            "\n",
            "Epoch 8/10\n",
            "Train Loss: 1.1000 Acc: 0.3333\n",
            "Val Loss: 1.0991 Acc: 0.3333\n",
            "\n",
            "Epoch 9/10\n",
            "Train Loss: 1.0989 Acc: 0.3333\n",
            "Val Loss: 1.0989 Acc: 0.3333\n",
            "\n",
            "Epoch 10/10\n",
            "Train Loss: 1.0989 Acc: 0.3333\n",
            "Val Loss: 1.0989 Acc: 0.3333\n",
            "\n",
            "\n",
            "Testing parameters: {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 10, 'learning_rate': 0.1, 'weight_decay': 1e-05}\n",
            "Epoch 1/10\n",
            "Train Loss: 1.2172 Acc: 0.3864\n",
            "Val Loss: 41163.8528 Acc: 0.2889\n",
            "\n",
            "Epoch 2/10\n",
            "Train Loss: 1.1695 Acc: 0.3068\n",
            "Val Loss: 1141036.9833 Acc: 0.3667\n",
            "\n",
            "Epoch 3/10\n",
            "Train Loss: 1.1093 Acc: 0.3350\n",
            "Val Loss: 6554.1219 Acc: 0.3333\n",
            "\n",
            "Epoch 4/10\n",
            "Train Loss: 1.1074 Acc: 0.3068\n",
            "Val Loss: 1.3882 Acc: 0.3333\n",
            "\n",
            "Epoch 5/10\n",
            "Train Loss: 1.1022 Acc: 0.3234\n",
            "Val Loss: 1.1000 Acc: 0.3333\n",
            "\n",
            "Epoch 6/10\n",
            "Train Loss: 1.1023 Acc: 0.3333\n",
            "Val Loss: 1.0991 Acc: 0.3333\n",
            "\n",
            "Epoch 7/10\n",
            "Train Loss: 1.0997 Acc: 0.3167\n",
            "Val Loss: 1.0995 Acc: 0.3333\n",
            "\n",
            "Epoch 8/10\n",
            "Train Loss: 1.0998 Acc: 0.3333\n",
            "Val Loss: 1.0993 Acc: 0.3333\n",
            "\n",
            "Epoch 9/10\n",
            "Train Loss: 1.0994 Acc: 0.3333\n",
            "Val Loss: 1.0990 Acc: 0.3333\n",
            "\n",
            "Epoch 10/10\n",
            "Train Loss: 1.0990 Acc: 0.3333\n",
            "Val Loss: 1.0990 Acc: 0.3333\n",
            "\n",
            "\n",
            "Testing parameters: {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 10, 'learning_rate': 0.1, 'weight_decay': 0.0001}\n",
            "Epoch 1/10\n",
            "Train Loss: 1.3720 Acc: 0.4660\n",
            "Val Loss: 23774.9995 Acc: 0.3333\n",
            "\n",
            "Epoch 2/10\n",
            "Train Loss: 1.5106 Acc: 0.3085\n",
            "Val Loss: 14744626494.5778 Acc: 0.3333\n",
            "\n",
            "Epoch 3/10\n",
            "Train Loss: 1.1386 Acc: 0.3234\n",
            "Val Loss: 10299207.8222 Acc: 0.3333\n",
            "\n",
            "Epoch 4/10\n",
            "Train Loss: 1.1456 Acc: 0.2968\n",
            "Val Loss: 3572.2594 Acc: 0.3333\n",
            "\n",
            "Epoch 5/10\n",
            "Train Loss: 1.1008 Acc: 0.3184\n",
            "Val Loss: 44.5827 Acc: 0.2222\n",
            "\n",
            "Epoch 6/10\n",
            "Train Loss: 1.1003 Acc: 0.3085\n",
            "Val Loss: 1.1028 Acc: 0.3333\n",
            "\n",
            "Epoch 7/10\n",
            "Train Loss: 1.0997 Acc: 0.3333\n",
            "Val Loss: 1.0988 Acc: 0.3333\n",
            "\n",
            "Epoch 8/10\n",
            "Train Loss: 1.0990 Acc: 0.3250\n",
            "Val Loss: 1.0988 Acc: 0.3333\n",
            "\n",
            "Epoch 9/10\n",
            "Train Loss: 1.0988 Acc: 0.3333\n",
            "Val Loss: 1.0987 Acc: 0.3333\n",
            "\n",
            "Epoch 10/10\n",
            "Train Loss: 1.0987 Acc: 0.3333\n",
            "Val Loss: 1.0987 Acc: 0.3333\n",
            "\n",
            "\n",
            "Testing parameters: {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 10, 'learning_rate': 0.1, 'weight_decay': 0.001}\n",
            "Epoch 1/10\n",
            "Train Loss: 1.3441 Acc: 0.4146\n",
            "Val Loss: 3732.6946 Acc: 0.3333\n",
            "\n",
            "Epoch 2/10\n",
            "Train Loss: 1.3222 Acc: 0.3217\n",
            "Val Loss: 145552966.0444 Acc: 0.3333\n",
            "\n",
            "Epoch 3/10\n",
            "Train Loss: 1.1076 Acc: 0.3184\n",
            "Val Loss: 484476.7486 Acc: 0.3333\n",
            "\n",
            "Epoch 4/10\n",
            "Train Loss: 1.1003 Acc: 0.3284\n",
            "Val Loss: 394.2298 Acc: 0.3333\n",
            "\n",
            "Epoch 5/10\n",
            "Train Loss: 1.1017 Acc: 0.3018\n",
            "Val Loss: 7.4714 Acc: 0.3333\n",
            "\n",
            "Epoch 6/10\n",
            "Train Loss: 1.1008 Acc: 0.3201\n",
            "Val Loss: 1.1183 Acc: 0.3333\n",
            "\n",
            "Epoch 7/10\n",
            "Train Loss: 1.1028 Acc: 0.3333\n",
            "Val Loss: 1.1007 Acc: 0.3333\n",
            "\n",
            "Epoch 8/10\n",
            "Train Loss: 1.0999 Acc: 0.3333\n",
            "Val Loss: 1.0993 Acc: 0.3333\n",
            "\n",
            "Epoch 9/10\n",
            "Train Loss: 1.0991 Acc: 0.3333\n",
            "Val Loss: 1.0988 Acc: 0.3333\n",
            "\n",
            "Epoch 10/10\n",
            "Train Loss: 1.0988 Acc: 0.3333\n",
            "Val Loss: 1.0988 Acc: 0.3333\n",
            "\n",
            "\n",
            "Testing parameters: {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 10, 'learning_rate': 0.01, 'weight_decay': 0}\n",
            "Epoch 1/10\n",
            "Train Loss: 0.9766 Acc: 0.4842\n",
            "Val Loss: 0.9448 Acc: 0.7222\n",
            "\n",
            "Epoch 2/10\n",
            "Train Loss: 0.7931 Acc: 0.6882\n",
            "Val Loss: 7.7449 Acc: 0.4333\n",
            "\n",
            "Epoch 3/10\n",
            "Train Loss: 1.0907 Acc: 0.4561\n",
            "Val Loss: 998.7757 Acc: 0.3333\n",
            "\n",
            "Epoch 4/10\n",
            "Train Loss: 1.1085 Acc: 0.4362\n",
            "Val Loss: 2.6045 Acc: 0.3333\n",
            "\n",
            "Epoch 5/10\n",
            "Train Loss: 1.0497 Acc: 0.4710\n",
            "Val Loss: 4.2970 Acc: 0.3667\n",
            "\n",
            "Epoch 6/10\n",
            "Train Loss: 0.9865 Acc: 0.5025\n",
            "Val Loss: 3.2709 Acc: 0.3444\n",
            "\n",
            "Epoch 7/10\n",
            "Train Loss: 0.9141 Acc: 0.5406\n",
            "Val Loss: 0.8723 Acc: 0.5111\n",
            "\n",
            "Epoch 8/10\n",
            "Train Loss: 0.9015 Acc: 0.4892\n",
            "Val Loss: 0.8728 Acc: 0.5556\n",
            "\n",
            "Epoch 9/10\n",
            "Train Loss: 0.8564 Acc: 0.5556\n",
            "Val Loss: 0.7217 Acc: 0.6889\n",
            "\n",
            "Epoch 10/10\n",
            "Train Loss: 0.8279 Acc: 0.5688\n",
            "Val Loss: 0.7146 Acc: 0.6444\n",
            "\n",
            "\n",
            "Testing parameters: {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 10, 'learning_rate': 0.01, 'weight_decay': 1e-05}\n",
            "Epoch 1/10\n",
            "Train Loss: 0.9777 Acc: 0.5224\n",
            "Val Loss: 0.8643 Acc: 0.7222\n",
            "\n",
            "Epoch 2/10\n",
            "Train Loss: 0.8344 Acc: 0.6501\n",
            "Val Loss: 4.9216 Acc: 0.3444\n",
            "\n",
            "Epoch 3/10\n",
            "Train Loss: 1.0221 Acc: 0.5207\n",
            "Val Loss: 12.7503 Acc: 0.3556\n",
            "\n",
            "Epoch 4/10\n",
            "Train Loss: 0.9925 Acc: 0.5025\n",
            "Val Loss: 13.1529 Acc: 0.4111\n",
            "\n",
            "Epoch 5/10\n",
            "Train Loss: 0.9801 Acc: 0.4942\n",
            "Val Loss: 16.0587 Acc: 0.3667\n",
            "\n",
            "Epoch 6/10\n",
            "Train Loss: 0.8764 Acc: 0.5688\n",
            "Val Loss: 1.0463 Acc: 0.5111\n",
            "\n",
            "Epoch 7/10\n",
            "Train Loss: 0.7654 Acc: 0.6103\n",
            "Val Loss: 1.2334 Acc: 0.5556\n",
            "\n",
            "Epoch 8/10\n",
            "Train Loss: 0.7256 Acc: 0.6683\n",
            "Val Loss: 0.6459 Acc: 0.6889\n",
            "\n",
            "Epoch 9/10\n",
            "Train Loss: 0.7327 Acc: 0.6750\n",
            "Val Loss: 0.5680 Acc: 0.7556\n",
            "\n",
            "Epoch 10/10\n",
            "Train Loss: 0.6582 Acc: 0.6949\n",
            "Val Loss: 0.5639 Acc: 0.7667\n",
            "\n",
            "\n",
            "Testing parameters: {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 10, 'learning_rate': 0.01, 'weight_decay': 0.0001}\n",
            "Epoch 1/10\n",
            "Train Loss: 0.9837 Acc: 0.5124\n",
            "Val Loss: 1.9175 Acc: 0.7444\n",
            "\n",
            "Epoch 2/10\n",
            "Train Loss: 0.8188 Acc: 0.6716\n",
            "Val Loss: 6.0149 Acc: 0.4889\n",
            "\n",
            "Epoch 3/10\n",
            "Train Loss: 1.0281 Acc: 0.5207\n",
            "Val Loss: 163.5917 Acc: 0.3444\n",
            "\n",
            "Epoch 4/10\n",
            "Train Loss: 1.0378 Acc: 0.4693\n",
            "Val Loss: 25.1825 Acc: 0.2556\n",
            "\n",
            "Epoch 5/10\n",
            "Train Loss: 1.0527 Acc: 0.4842\n",
            "Val Loss: 6.1835 Acc: 0.3333\n",
            "\n",
            "Epoch 6/10\n",
            "Train Loss: 0.9428 Acc: 0.5141\n",
            "Val Loss: 1.8261 Acc: 0.3556\n",
            "\n",
            "Epoch 7/10\n",
            "Train Loss: 0.9223 Acc: 0.5522\n",
            "Val Loss: 0.9603 Acc: 0.5778\n",
            "\n",
            "Epoch 8/10\n",
            "Train Loss: 0.8558 Acc: 0.5755\n",
            "Val Loss: 0.8475 Acc: 0.6222\n",
            "\n",
            "Epoch 9/10\n",
            "Train Loss: 0.7768 Acc: 0.6501\n",
            "Val Loss: 0.8084 Acc: 0.6222\n",
            "\n",
            "Epoch 10/10\n",
            "Train Loss: 0.7857 Acc: 0.6434\n",
            "Val Loss: 0.7622 Acc: 0.6556\n",
            "\n",
            "\n",
            "Testing parameters: {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 10, 'learning_rate': 0.01, 'weight_decay': 0.001}\n",
            "Epoch 1/10\n",
            "Train Loss: 0.9769 Acc: 0.5075\n",
            "Val Loss: 0.5101 Acc: 0.7667\n",
            "\n",
            "Epoch 2/10\n",
            "Train Loss: 0.8659 Acc: 0.6484\n",
            "Val Loss: 5.1063 Acc: 0.3889\n",
            "\n",
            "Epoch 3/10\n",
            "Train Loss: 1.0592 Acc: 0.4892\n",
            "Val Loss: 259.4321 Acc: 0.3333\n",
            "\n",
            "Epoch 4/10\n",
            "Train Loss: 1.0197 Acc: 0.4710\n",
            "Val Loss: 6.8270 Acc: 0.1778\n",
            "\n",
            "Epoch 5/10\n",
            "Train Loss: 0.9793 Acc: 0.5158\n",
            "Val Loss: 1.4623 Acc: 0.4444\n",
            "\n",
            "Epoch 6/10\n",
            "Train Loss: 0.9016 Acc: 0.5821\n",
            "Val Loss: 0.9214 Acc: 0.5000\n",
            "\n",
            "Epoch 7/10\n",
            "Train Loss: 0.8931 Acc: 0.5456\n",
            "Val Loss: 0.9112 Acc: 0.6556\n",
            "\n",
            "Epoch 8/10\n",
            "Train Loss: 0.8151 Acc: 0.5688\n",
            "Val Loss: 0.7051 Acc: 0.6333\n",
            "\n",
            "Epoch 9/10\n",
            "Train Loss: 0.7782 Acc: 0.6053\n",
            "Val Loss: 0.6958 Acc: 0.5778\n",
            "\n",
            "Epoch 10/10\n",
            "Train Loss: 0.7723 Acc: 0.6269\n",
            "Val Loss: 0.7053 Acc: 0.5667\n",
            "\n",
            "\n",
            "Testing parameters: {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 10, 'learning_rate': 0.001, 'weight_decay': 0}\n",
            "Epoch 1/10\n",
            "Train Loss: 1.0933 Acc: 0.3698\n",
            "Val Loss: 1.0581 Acc: 0.5667\n",
            "\n",
            "Epoch 2/10\n",
            "Train Loss: 0.9544 Acc: 0.6551\n",
            "Val Loss: 0.6378 Acc: 0.7222\n",
            "\n",
            "Epoch 3/10\n",
            "Train Loss: 0.5292 Acc: 0.7877\n",
            "Val Loss: 1.9074 Acc: 0.7333\n",
            "\n",
            "Epoch 4/10\n",
            "Train Loss: 0.5442 Acc: 0.8109\n",
            "Val Loss: 2.3096 Acc: 0.7000\n",
            "\n",
            "Epoch 5/10\n",
            "Train Loss: 0.5039 Acc: 0.8176\n",
            "Val Loss: 0.4654 Acc: 0.8333\n",
            "\n",
            "Epoch 6/10\n",
            "Train Loss: 0.3735 Acc: 0.8590\n",
            "Val Loss: 0.3845 Acc: 0.9000\n",
            "\n",
            "Epoch 7/10\n",
            "Train Loss: 0.3142 Acc: 0.8988\n",
            "Val Loss: 0.3988 Acc: 0.8778\n",
            "\n",
            "Epoch 8/10\n",
            "Train Loss: 0.2961 Acc: 0.8939\n",
            "Val Loss: 0.4109 Acc: 0.9111\n",
            "\n",
            "Epoch 9/10\n",
            "Train Loss: 0.1993 Acc: 0.9221\n",
            "Val Loss: 0.4066 Acc: 0.9111\n",
            "\n",
            "Epoch 10/10\n",
            "Train Loss: 0.2064 Acc: 0.9287\n",
            "Val Loss: 0.4066 Acc: 0.8889\n",
            "\n",
            "\n",
            "Testing parameters: {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 10, 'learning_rate': 0.001, 'weight_decay': 1e-05}\n",
            "Epoch 1/10\n",
            "Train Loss: 1.0890 Acc: 0.3748\n",
            "Val Loss: 1.0421 Acc: 0.5667\n",
            "\n",
            "Epoch 2/10\n",
            "Train Loss: 0.9539 Acc: 0.6650\n",
            "Val Loss: 0.6002 Acc: 0.7333\n",
            "\n",
            "Epoch 3/10\n",
            "Train Loss: 0.4939 Acc: 0.8060\n",
            "Val Loss: 0.9097 Acc: 0.7778\n",
            "\n",
            "Epoch 4/10\n",
            "Train Loss: 0.5410 Acc: 0.7828\n",
            "Val Loss: 0.5906 Acc: 0.7556\n",
            "\n",
            "Epoch 5/10\n",
            "Train Loss: 0.3683 Acc: 0.8541\n",
            "Val Loss: 0.6963 Acc: 0.8111\n",
            "\n",
            "Epoch 6/10\n",
            "Train Loss: 0.3278 Acc: 0.8823\n",
            "Val Loss: 0.8169 Acc: 0.8000\n",
            "\n",
            "Epoch 7/10\n",
            "Train Loss: 0.2807 Acc: 0.8922\n",
            "Val Loss: 0.4995 Acc: 0.8444\n",
            "\n",
            "Epoch 8/10\n",
            "Train Loss: 0.1948 Acc: 0.9254\n",
            "Val Loss: 0.4898 Acc: 0.8444\n",
            "\n",
            "Epoch 9/10\n",
            "Train Loss: 0.1799 Acc: 0.9320\n",
            "Val Loss: 0.4725 Acc: 0.8556\n",
            "\n",
            "Epoch 10/10\n",
            "Train Loss: 0.1638 Acc: 0.9420\n",
            "Val Loss: 0.4659 Acc: 0.8556\n",
            "\n",
            "\n",
            "Testing parameters: {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 10, 'learning_rate': 0.001, 'weight_decay': 0.0001}\n",
            "Epoch 1/10\n",
            "Train Loss: 1.0909 Acc: 0.3698\n",
            "Val Loss: 1.0310 Acc: 0.7333\n",
            "\n",
            "Epoch 2/10\n",
            "Train Loss: 0.9447 Acc: 0.6633\n",
            "Val Loss: 0.5192 Acc: 0.8111\n",
            "\n",
            "Epoch 3/10\n",
            "Train Loss: 0.5424 Acc: 0.8027\n",
            "Val Loss: 0.5690 Acc: 0.8333\n",
            "\n",
            "Epoch 4/10\n",
            "Train Loss: 0.4108 Acc: 0.8425\n",
            "Val Loss: 0.4270 Acc: 0.8333\n",
            "\n",
            "Epoch 5/10\n",
            "Train Loss: 0.3556 Acc: 0.8590\n",
            "Val Loss: 0.4339 Acc: 0.8778\n",
            "\n",
            "Epoch 6/10\n",
            "Train Loss: 0.3326 Acc: 0.8624\n",
            "Val Loss: 0.3870 Acc: 0.8778\n",
            "\n",
            "Epoch 7/10\n",
            "Train Loss: 0.2822 Acc: 0.9055\n",
            "Val Loss: 0.4158 Acc: 0.8778\n",
            "\n",
            "Epoch 8/10\n",
            "Train Loss: 0.1968 Acc: 0.9287\n",
            "Val Loss: 0.4291 Acc: 0.8778\n",
            "\n",
            "Epoch 9/10\n",
            "Train Loss: 0.1760 Acc: 0.9270\n",
            "Val Loss: 0.4496 Acc: 0.8778\n",
            "\n",
            "Epoch 10/10\n",
            "Train Loss: 0.2135 Acc: 0.9154\n",
            "Val Loss: 0.4599 Acc: 0.8778\n",
            "\n",
            "\n",
            "Testing parameters: {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 10, 'learning_rate': 0.001, 'weight_decay': 0.001}\n",
            "Epoch 1/10\n",
            "Train Loss: 1.0942 Acc: 0.3549\n",
            "Val Loss: 1.0422 Acc: 0.6000\n",
            "\n",
            "Epoch 2/10\n",
            "Train Loss: 0.9511 Acc: 0.6783\n",
            "Val Loss: 0.4828 Acc: 0.8000\n",
            "\n",
            "Epoch 3/10\n",
            "Train Loss: 0.5403 Acc: 0.8109\n",
            "Val Loss: 1.0011 Acc: 0.7667\n",
            "\n",
            "Epoch 4/10\n",
            "Train Loss: 0.5293 Acc: 0.8010\n",
            "Val Loss: 0.4470 Acc: 0.8556\n",
            "\n",
            "Epoch 5/10\n",
            "Train Loss: 0.3988 Acc: 0.8607\n",
            "Val Loss: 0.3034 Acc: 0.9222\n",
            "\n",
            "Epoch 6/10\n",
            "Train Loss: 0.3416 Acc: 0.8723\n",
            "Val Loss: 0.4358 Acc: 0.9111\n",
            "\n",
            "Epoch 7/10\n",
            "Train Loss: 0.2403 Acc: 0.9154\n",
            "Val Loss: 0.3776 Acc: 0.9000\n",
            "\n",
            "Epoch 8/10\n",
            "Train Loss: 0.2310 Acc: 0.9303\n",
            "Val Loss: 0.3564 Acc: 0.9000\n",
            "\n",
            "Epoch 9/10\n",
            "Train Loss: 0.1955 Acc: 0.9204\n",
            "Val Loss: 0.3596 Acc: 0.8889\n",
            "\n",
            "Epoch 10/10\n",
            "Train Loss: 0.1557 Acc: 0.9469\n",
            "Val Loss: 0.3629 Acc: 0.8889\n",
            "\n",
            "\n",
            "Testing parameters: {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 10, 'learning_rate': 0.1, 'weight_decay': 0}\n",
            "Epoch 1/10\n",
            "Train Loss: 1.1862 Acc: 0.4129\n",
            "Val Loss: 3945.3948 Acc: 0.3333\n",
            "\n",
            "Epoch 2/10\n",
            "Train Loss: 1.1866 Acc: 0.3632\n",
            "Val Loss: 837692.0889 Acc: 0.3444\n",
            "\n",
            "Epoch 3/10\n",
            "Train Loss: 1.1063 Acc: 0.3151\n",
            "Val Loss: 93705.2222 Acc: 0.3333\n",
            "\n",
            "Epoch 4/10\n",
            "Train Loss: 1.1050 Acc: 0.3383\n",
            "Val Loss: 302.6529 Acc: 0.3333\n",
            "\n",
            "Epoch 5/10\n",
            "Train Loss: 1.1015 Acc: 0.3085\n",
            "Val Loss: 18.4391 Acc: 0.2778\n",
            "\n",
            "Epoch 6/10\n",
            "Train Loss: 1.1005 Acc: 0.3250\n",
            "Val Loss: 2.7014 Acc: 0.2889\n",
            "\n",
            "Epoch 7/10\n",
            "Train Loss: 1.1000 Acc: 0.3333\n",
            "Val Loss: 1.3540 Acc: 0.3222\n",
            "\n",
            "Epoch 8/10\n",
            "Train Loss: 1.0992 Acc: 0.3333\n",
            "Val Loss: 1.2166 Acc: 0.3333\n",
            "\n",
            "Epoch 9/10\n",
            "Train Loss: 1.0988 Acc: 0.3333\n",
            "Val Loss: 1.1451 Acc: 0.3333\n",
            "\n",
            "Epoch 10/10\n",
            "Train Loss: 1.0987 Acc: 0.3333\n",
            "Val Loss: 1.1521 Acc: 0.3333\n",
            "\n",
            "\n",
            "Testing parameters: {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 10, 'learning_rate': 0.1, 'weight_decay': 1e-05}\n",
            "Epoch 1/10\n",
            "Train Loss: 1.1834 Acc: 0.4693\n",
            "Val Loss: 7820.4761 Acc: 0.3556\n",
            "\n",
            "Epoch 2/10\n",
            "Train Loss: 1.3313 Acc: 0.3350\n",
            "Val Loss: 146022252.0889 Acc: 0.3333\n",
            "\n",
            "Epoch 3/10\n",
            "Train Loss: 1.1091 Acc: 0.3201\n",
            "Val Loss: 27768.5345 Acc: 0.3333\n",
            "\n",
            "Epoch 4/10\n",
            "Train Loss: 1.1488 Acc: 0.3566\n",
            "Val Loss: 1144.5332 Acc: 0.3333\n",
            "\n",
            "Epoch 5/10\n",
            "Train Loss: 1.1110 Acc: 0.3333\n",
            "Val Loss: 1.0987 Acc: 0.3333\n",
            "\n",
            "Epoch 6/10\n",
            "Train Loss: 1.0999 Acc: 0.3167\n",
            "Val Loss: 1.0988 Acc: 0.3333\n",
            "\n",
            "Epoch 7/10\n",
            "Train Loss: 1.0993 Acc: 0.3333\n",
            "Val Loss: 1.0989 Acc: 0.3333\n",
            "\n",
            "Epoch 8/10\n",
            "Train Loss: 1.0988 Acc: 0.3333\n",
            "Val Loss: 1.0987 Acc: 0.3333\n",
            "\n",
            "Epoch 9/10\n",
            "Train Loss: 1.0989 Acc: 0.3367\n",
            "Val Loss: 1.0988 Acc: 0.3333\n",
            "\n",
            "Epoch 10/10\n",
            "Train Loss: 1.0988 Acc: 0.3333\n",
            "Val Loss: 1.0988 Acc: 0.3333\n",
            "\n",
            "\n",
            "Testing parameters: {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 10, 'learning_rate': 0.1, 'weight_decay': 0.0001}\n",
            "Epoch 1/10\n",
            "Train Loss: 1.2027 Acc: 0.4245\n",
            "Val Loss: 11729.9895 Acc: 0.3333\n",
            "\n",
            "Epoch 2/10\n",
            "Train Loss: 1.2930 Acc: 0.3284\n",
            "Val Loss: 78452906.6667 Acc: 0.3333\n",
            "\n",
            "Epoch 3/10\n",
            "Train Loss: 1.1626 Acc: 0.3250\n",
            "Val Loss: 8110.7546 Acc: 0.3333\n",
            "\n",
            "Epoch 4/10\n",
            "Train Loss: 1.0999 Acc: 0.3250\n",
            "Val Loss: 53.6939 Acc: 0.3333\n",
            "\n",
            "Epoch 5/10\n",
            "Train Loss: 1.1002 Acc: 0.3284\n",
            "Val Loss: 1.1425 Acc: 0.3222\n",
            "\n",
            "Epoch 6/10\n",
            "Train Loss: 1.0997 Acc: 0.3250\n",
            "Val Loss: 1.0987 Acc: 0.3333\n",
            "\n",
            "Epoch 7/10\n",
            "Train Loss: 1.0993 Acc: 0.3333\n",
            "Val Loss: 1.0987 Acc: 0.3333\n",
            "\n",
            "Epoch 8/10\n",
            "Train Loss: 1.0994 Acc: 0.3267\n",
            "Val Loss: 1.0987 Acc: 0.3333\n",
            "\n",
            "Epoch 9/10\n",
            "Train Loss: 1.0988 Acc: 0.3333\n",
            "Val Loss: 1.0978 Acc: 0.3333\n",
            "\n",
            "Epoch 10/10\n",
            "Train Loss: 1.0987 Acc: 0.3333\n",
            "Val Loss: 1.0934 Acc: 0.3333\n",
            "\n",
            "\n",
            "Testing parameters: {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 10, 'learning_rate': 0.1, 'weight_decay': 0.001}\n",
            "Epoch 1/10\n",
            "Train Loss: 1.2764 Acc: 0.4212\n",
            "Val Loss: 4638.0722 Acc: 0.3333\n",
            "\n",
            "Epoch 2/10\n",
            "Train Loss: 1.2039 Acc: 0.3317\n",
            "Val Loss: 407408139.3778 Acc: 0.3333\n",
            "\n",
            "Epoch 3/10\n",
            "Train Loss: 1.1138 Acc: 0.3284\n",
            "Val Loss: 1293405.4222 Acc: 0.3333\n",
            "\n",
            "Epoch 4/10\n",
            "Train Loss: 1.1025 Acc: 0.3317\n",
            "Val Loss: 1.0987 Acc: 0.3333\n",
            "\n",
            "Epoch 5/10\n",
            "Train Loss: 1.1042 Acc: 0.3317\n",
            "Val Loss: 1.0994 Acc: 0.3333\n",
            "\n",
            "Epoch 6/10\n",
            "Train Loss: 1.1002 Acc: 0.3201\n",
            "Val Loss: 1.0993 Acc: 0.3333\n",
            "\n",
            "Epoch 7/10\n",
            "Train Loss: 1.1010 Acc: 0.3085\n",
            "Val Loss: 1.0990 Acc: 0.3333\n",
            "\n",
            "Epoch 8/10\n",
            "Train Loss: 1.0992 Acc: 0.3333\n",
            "Val Loss: 1.0988 Acc: 0.3333\n",
            "\n",
            "Epoch 9/10\n",
            "Train Loss: 1.0991 Acc: 0.3333\n",
            "Val Loss: 1.0988 Acc: 0.3333\n",
            "\n",
            "Epoch 10/10\n",
            "Train Loss: 1.0988 Acc: 0.3333\n",
            "Val Loss: 1.0988 Acc: 0.3333\n",
            "\n",
            "\n",
            "Testing parameters: {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 10, 'learning_rate': 0.01, 'weight_decay': 0}\n",
            "Epoch 1/10\n",
            "Train Loss: 0.9815 Acc: 0.5041\n",
            "Val Loss: 1.1659 Acc: 0.6222\n",
            "\n",
            "Epoch 2/10\n",
            "Train Loss: 0.8557 Acc: 0.6501\n",
            "Val Loss: 2.2398 Acc: 0.3222\n",
            "\n",
            "Epoch 3/10\n",
            "Train Loss: 1.0943 Acc: 0.4776\n",
            "Val Loss: 94.6913 Acc: 0.2889\n",
            "\n",
            "Epoch 4/10\n",
            "Train Loss: 0.9620 Acc: 0.5589\n",
            "Val Loss: 17.4841 Acc: 0.3333\n",
            "\n",
            "Epoch 5/10\n",
            "Train Loss: 1.0564 Acc: 0.4643\n",
            "Val Loss: 661.6162 Acc: 0.3889\n",
            "\n",
            "Epoch 6/10\n",
            "Train Loss: 0.9787 Acc: 0.4992\n",
            "Val Loss: 2.1087 Acc: 0.4667\n",
            "\n",
            "Epoch 7/10\n",
            "Train Loss: 0.9322 Acc: 0.5539\n",
            "Val Loss: 0.9717 Acc: 0.5000\n",
            "\n",
            "Epoch 8/10\n",
            "Train Loss: 0.8394 Acc: 0.5837\n",
            "Val Loss: 0.7532 Acc: 0.6889\n",
            "\n",
            "Epoch 9/10\n",
            "Train Loss: 0.8304 Acc: 0.6003\n",
            "Val Loss: 0.6634 Acc: 0.7333\n",
            "\n",
            "Epoch 10/10\n",
            "Train Loss: 0.7587 Acc: 0.6401\n",
            "Val Loss: 0.6819 Acc: 0.6889\n",
            "\n",
            "\n",
            "Testing parameters: {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 10, 'learning_rate': 0.01, 'weight_decay': 1e-05}\n",
            "Epoch 1/10\n",
            "Train Loss: 0.9823 Acc: 0.5605\n",
            "Val Loss: 1.2698 Acc: 0.6667\n",
            "\n",
            "Epoch 2/10\n",
            "Train Loss: 0.7867 Acc: 0.6650\n",
            "Val Loss: 13.2305 Acc: 0.5222\n",
            "\n",
            "Epoch 3/10\n",
            "Train Loss: 1.0811 Acc: 0.4826\n",
            "Val Loss: 345.2872 Acc: 0.3333\n",
            "\n",
            "Epoch 4/10\n",
            "Train Loss: 1.0688 Acc: 0.4610\n",
            "Val Loss: 80.4990 Acc: 0.3444\n",
            "\n",
            "Epoch 5/10\n",
            "Train Loss: 1.0048 Acc: 0.5257\n",
            "Val Loss: 4.9414 Acc: 0.3444\n",
            "\n",
            "Epoch 6/10\n",
            "Train Loss: 0.9212 Acc: 0.5041\n",
            "Val Loss: 1.5220 Acc: 0.4333\n",
            "\n",
            "Epoch 7/10\n",
            "Train Loss: 0.9091 Acc: 0.5439\n",
            "Val Loss: 0.8785 Acc: 0.5667\n",
            "\n",
            "Epoch 8/10\n",
            "Train Loss: 0.8376 Acc: 0.5788\n",
            "Val Loss: 0.9908 Acc: 0.5778\n",
            "\n",
            "Epoch 9/10\n",
            "Train Loss: 0.8017 Acc: 0.6003\n",
            "Val Loss: 0.8327 Acc: 0.6222\n",
            "\n",
            "Epoch 10/10\n",
            "Train Loss: 0.8159 Acc: 0.6202\n",
            "Val Loss: 0.7908 Acc: 0.6333\n",
            "\n",
            "\n",
            "Testing parameters: {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 10, 'learning_rate': 0.01, 'weight_decay': 0.0001}\n",
            "Epoch 1/10\n",
            "Train Loss: 0.9948 Acc: 0.5373\n",
            "Val Loss: 0.6096 Acc: 0.7556\n",
            "\n",
            "Epoch 2/10\n",
            "Train Loss: 0.8608 Acc: 0.6683\n",
            "Val Loss: 2.0564 Acc: 0.4222\n",
            "\n",
            "Epoch 3/10\n",
            "Train Loss: 0.9943 Acc: 0.5406\n",
            "Val Loss: 14.2681 Acc: 0.4444\n",
            "\n",
            "Epoch 4/10\n",
            "Train Loss: 0.9764 Acc: 0.4975\n",
            "Val Loss: 4.1417 Acc: 0.3778\n",
            "\n",
            "Epoch 5/10\n",
            "Train Loss: 0.9457 Acc: 0.5108\n",
            "Val Loss: 2.3980 Acc: 0.4444\n",
            "\n",
            "Epoch 6/10\n",
            "Train Loss: 1.0072 Acc: 0.4892\n",
            "Val Loss: 1.0492 Acc: 0.4778\n",
            "\n",
            "Epoch 7/10\n",
            "Train Loss: 0.9432 Acc: 0.5207\n",
            "Val Loss: 0.9167 Acc: 0.5778\n",
            "\n",
            "Epoch 8/10\n",
            "Train Loss: 0.8707 Acc: 0.5738\n",
            "Val Loss: 0.6750 Acc: 0.6222\n",
            "\n",
            "Epoch 9/10\n",
            "Train Loss: 0.7907 Acc: 0.6269\n",
            "Val Loss: 0.6730 Acc: 0.6111\n",
            "\n",
            "Epoch 10/10\n",
            "Train Loss: 0.7795 Acc: 0.6368\n",
            "Val Loss: 0.6547 Acc: 0.6667\n",
            "\n",
            "\n",
            "Testing parameters: {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 10, 'learning_rate': 0.01, 'weight_decay': 0.001}\n",
            "Epoch 1/10\n",
            "Train Loss: 0.9771 Acc: 0.5207\n",
            "Val Loss: 0.6802 Acc: 0.7889\n",
            "\n",
            "Epoch 2/10\n",
            "Train Loss: 0.8105 Acc: 0.6849\n",
            "Val Loss: 23.4363 Acc: 0.2333\n",
            "\n",
            "Epoch 3/10\n",
            "Train Loss: 1.1223 Acc: 0.4262\n",
            "Val Loss: 467.0967 Acc: 0.3333\n",
            "\n",
            "Epoch 4/10\n",
            "Train Loss: 1.0541 Acc: 0.4411\n",
            "Val Loss: 13.0196 Acc: 0.3333\n",
            "\n",
            "Epoch 5/10\n",
            "Train Loss: 1.0788 Acc: 0.4179\n",
            "Val Loss: 2.1693 Acc: 0.3889\n",
            "\n",
            "Epoch 6/10\n",
            "Train Loss: 1.0013 Acc: 0.4478\n",
            "Val Loss: 1.0301 Acc: 0.5000\n",
            "\n",
            "Epoch 7/10\n",
            "Train Loss: 0.9091 Acc: 0.5274\n",
            "Val Loss: 0.9006 Acc: 0.5667\n",
            "\n",
            "Epoch 8/10\n",
            "Train Loss: 0.8820 Acc: 0.5158\n",
            "Val Loss: 0.8165 Acc: 0.5444\n",
            "\n",
            "Epoch 9/10\n",
            "Train Loss: 0.8513 Acc: 0.5340\n",
            "Val Loss: 0.7028 Acc: 0.6111\n",
            "\n",
            "Epoch 10/10\n",
            "Train Loss: 0.8541 Acc: 0.5473\n",
            "Val Loss: 0.6957 Acc: 0.6000\n",
            "\n",
            "\n",
            "Testing parameters: {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 10, 'learning_rate': 0.001, 'weight_decay': 0}\n",
            "Epoch 1/10\n",
            "Train Loss: 1.0901 Acc: 0.3897\n",
            "Val Loss: 1.0367 Acc: 0.6333\n",
            "\n",
            "Epoch 2/10\n",
            "Train Loss: 0.9731 Acc: 0.6302\n",
            "Val Loss: 0.6147 Acc: 0.7333\n",
            "\n",
            "Epoch 3/10\n",
            "Train Loss: 0.5156 Acc: 0.7877\n",
            "Val Loss: 0.7158 Acc: 0.8111\n",
            "\n",
            "Epoch 4/10\n",
            "Train Loss: 0.4267 Acc: 0.8557\n",
            "Val Loss: 0.4240 Acc: 0.8556\n",
            "\n",
            "Epoch 5/10\n",
            "Train Loss: 0.4202 Acc: 0.8391\n",
            "Val Loss: 0.7741 Acc: 0.7778\n",
            "\n",
            "Epoch 6/10\n",
            "Train Loss: 0.3845 Acc: 0.8375\n",
            "Val Loss: 0.4190 Acc: 0.8556\n",
            "\n",
            "Epoch 7/10\n",
            "Train Loss: 0.3041 Acc: 0.8889\n",
            "Val Loss: 0.4140 Acc: 0.9000\n",
            "\n",
            "Epoch 8/10\n",
            "Train Loss: 0.2338 Acc: 0.9138\n",
            "Val Loss: 0.3509 Acc: 0.9222\n",
            "\n",
            "Epoch 9/10\n",
            "Train Loss: 0.2440 Acc: 0.9088\n",
            "Val Loss: 0.3366 Acc: 0.9222\n",
            "\n",
            "Epoch 10/10\n",
            "Train Loss: 0.1961 Acc: 0.9221\n",
            "Val Loss: 0.3284 Acc: 0.9222\n",
            "\n",
            "\n",
            "Testing parameters: {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 10, 'learning_rate': 0.001, 'weight_decay': 1e-05}\n",
            "Epoch 1/10\n",
            "Train Loss: 1.0896 Acc: 0.3930\n",
            "Val Loss: 1.0426 Acc: 0.6333\n",
            "\n",
            "Epoch 2/10\n",
            "Train Loss: 0.9839 Acc: 0.6451\n",
            "Val Loss: 0.5474 Acc: 0.8111\n",
            "\n",
            "Epoch 3/10\n",
            "Train Loss: 0.5934 Acc: 0.7678\n",
            "Val Loss: 1.0232 Acc: 0.7444\n",
            "\n",
            "Epoch 4/10\n",
            "Train Loss: 0.4842 Acc: 0.8010\n",
            "Val Loss: 0.5334 Acc: 0.8444\n",
            "\n",
            "Epoch 5/10\n",
            "Train Loss: 0.3890 Acc: 0.8541\n",
            "Val Loss: 0.8052 Acc: 0.8222\n",
            "\n",
            "Epoch 6/10\n",
            "Train Loss: 0.3183 Acc: 0.8988\n",
            "Val Loss: 0.6266 Acc: 0.8556\n",
            "\n",
            "Epoch 7/10\n",
            "Train Loss: 0.2517 Acc: 0.8988\n",
            "Val Loss: 0.5490 Acc: 0.8333\n",
            "\n",
            "Epoch 8/10\n",
            "Train Loss: 0.2472 Acc: 0.9171\n",
            "Val Loss: 0.5438 Acc: 0.8222\n",
            "\n",
            "Epoch 9/10\n",
            "Train Loss: 0.1886 Acc: 0.9221\n",
            "Val Loss: 0.5330 Acc: 0.8556\n",
            "\n",
            "Epoch 10/10\n",
            "Train Loss: 0.1764 Acc: 0.9370\n",
            "Val Loss: 0.5693 Acc: 0.8444\n",
            "\n",
            "\n",
            "Testing parameters: {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 10, 'learning_rate': 0.001, 'weight_decay': 0.0001}\n",
            "Epoch 1/10\n",
            "Train Loss: 1.0958 Acc: 0.3698\n",
            "Val Loss: 1.0541 Acc: 0.4778\n",
            "\n",
            "Epoch 2/10\n",
            "Train Loss: 0.9805 Acc: 0.6119\n",
            "Val Loss: 0.5917 Acc: 0.8000\n",
            "\n",
            "Epoch 3/10\n",
            "Train Loss: 0.5707 Acc: 0.7761\n",
            "Val Loss: 0.7830 Acc: 0.8111\n",
            "\n",
            "Epoch 4/10\n",
            "Train Loss: 0.4542 Acc: 0.8259\n",
            "Val Loss: 0.6288 Acc: 0.8000\n",
            "\n",
            "Epoch 5/10\n",
            "Train Loss: 0.3792 Acc: 0.8657\n",
            "Val Loss: 0.4166 Acc: 0.8778\n",
            "\n",
            "Epoch 6/10\n",
            "Train Loss: 0.3246 Acc: 0.8723\n",
            "Val Loss: 0.4867 Acc: 0.8111\n",
            "\n",
            "Epoch 7/10\n",
            "Train Loss: 0.2596 Acc: 0.8955\n",
            "Val Loss: 0.5586 Acc: 0.8333\n",
            "\n",
            "Epoch 8/10\n",
            "Train Loss: 0.2020 Acc: 0.9121\n",
            "Val Loss: 0.5003 Acc: 0.8667\n",
            "\n",
            "Epoch 9/10\n",
            "Train Loss: 0.1777 Acc: 0.9237\n",
            "Val Loss: 0.4730 Acc: 0.8667\n",
            "\n",
            "Epoch 10/10\n",
            "Train Loss: 0.2044 Acc: 0.9337\n",
            "Val Loss: 0.4889 Acc: 0.8667\n",
            "\n",
            "\n",
            "Testing parameters: {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 10, 'learning_rate': 0.001, 'weight_decay': 0.001}\n",
            "Epoch 1/10\n",
            "Train Loss: 1.0934 Acc: 0.3698\n",
            "Val Loss: 1.0492 Acc: 0.6333\n",
            "\n",
            "Epoch 2/10\n",
            "Train Loss: 0.9731 Acc: 0.6119\n",
            "Val Loss: 0.5642 Acc: 0.8111\n",
            "\n",
            "Epoch 3/10\n",
            "Train Loss: 0.5538 Acc: 0.7993\n",
            "Val Loss: 0.3861 Acc: 0.8889\n",
            "\n",
            "Epoch 4/10\n",
            "Train Loss: 0.4634 Acc: 0.8342\n",
            "Val Loss: 1.1496 Acc: 0.6778\n",
            "\n",
            "Epoch 5/10\n",
            "Train Loss: 0.4811 Acc: 0.7977\n",
            "Val Loss: 0.5032 Acc: 0.8444\n",
            "\n",
            "Epoch 6/10\n",
            "Train Loss: 0.2942 Acc: 0.8740\n",
            "Val Loss: 0.4416 Acc: 0.8667\n",
            "\n",
            "Epoch 7/10\n",
            "Train Loss: 0.2371 Acc: 0.9154\n",
            "Val Loss: 0.3580 Acc: 0.8667\n",
            "\n",
            "Epoch 8/10\n",
            "Train Loss: 0.2367 Acc: 0.9038\n",
            "Val Loss: 0.3200 Acc: 0.8556\n",
            "\n",
            "Epoch 9/10\n",
            "Train Loss: 0.1770 Acc: 0.9337\n",
            "Val Loss: 0.2919 Acc: 0.8889\n",
            "\n",
            "Epoch 10/10\n",
            "Train Loss: 0.1997 Acc: 0.9154\n",
            "Val Loss: 0.2949 Acc: 0.8889\n",
            "\n",
            "\n",
            "Testing parameters: {'batch_size': 64, 'dropout_rate': 0.5, 'epochs': 10, 'learning_rate': 0.1, 'weight_decay': 0}\n",
            "Epoch 1/10\n",
            "Train Loss: 1.1986 Acc: 0.4179\n",
            "Val Loss: 448662.6736 Acc: 0.3333\n",
            "\n",
            "Epoch 2/10\n",
            "Train Loss: 1.4452 Acc: 0.3599\n",
            "Val Loss: 318169816.1778 Acc: 0.3333\n",
            "\n",
            "Epoch 3/10\n",
            "Train Loss: 1.1910 Acc: 0.3333\n",
            "Val Loss: 1.0986 Acc: 0.3333\n",
            "\n",
            "Epoch 4/10\n",
            "Train Loss: 1.1032 Acc: 0.3367\n",
            "Val Loss: 1.1001 Acc: 0.3333\n",
            "\n",
            "Epoch 5/10\n",
            "Train Loss: 1.1016 Acc: 0.3118\n",
            "Val Loss: 2.1177 Acc: 0.2889\n",
            "\n",
            "Epoch 6/10\n",
            "Train Loss: 1.1108 Acc: 0.3383\n",
            "Val Loss: 663.7139 Acc: 0.3333\n",
            "\n",
            "Epoch 7/10\n",
            "Train Loss: 1.1239 Acc: 0.3300\n",
            "Val Loss: 21.7199 Acc: 0.3222\n",
            "\n",
            "Epoch 8/10\n",
            "Train Loss: 1.1051 Acc: 0.3317\n",
            "Val Loss: 1.0987 Acc: 0.3333\n",
            "\n",
            "Epoch 9/10\n",
            "Train Loss: 1.0990 Acc: 0.3333\n",
            "Val Loss: 1.0988 Acc: 0.3333\n",
            "\n",
            "Epoch 10/10\n",
            "Train Loss: 1.0988 Acc: 0.3333\n",
            "Val Loss: 1.0988 Acc: 0.3333\n",
            "\n",
            "\n",
            "Testing parameters: {'batch_size': 64, 'dropout_rate': 0.5, 'epochs': 10, 'learning_rate': 0.1, 'weight_decay': 1e-05}\n",
            "Epoch 1/10\n",
            "Train Loss: 1.1832 Acc: 0.4594\n",
            "Val Loss: 566.1502 Acc: 0.3778\n",
            "\n",
            "Epoch 2/10\n",
            "Train Loss: 1.2762 Acc: 0.3466\n",
            "Val Loss: 104363185.4222 Acc: 0.3333\n",
            "\n",
            "Epoch 3/10\n",
            "Train Loss: 1.1028 Acc: 0.3002\n",
            "Val Loss: 1713473.0944 Acc: 0.3333\n",
            "\n",
            "Epoch 4/10\n",
            "Train Loss: 1.1040 Acc: 0.3118\n",
            "Val Loss: 6425.9504 Acc: 0.3333\n",
            "\n",
            "Epoch 5/10\n",
            "Train Loss: 1.0995 Acc: 0.3400\n",
            "Val Loss: 25.9802 Acc: 0.3333\n",
            "\n",
            "Epoch 6/10\n",
            "Train Loss: 1.1021 Acc: 0.3317\n",
            "Val Loss: 8.4667 Acc: 0.3333\n",
            "\n",
            "Epoch 7/10\n",
            "Train Loss: 1.1010 Acc: 0.3284\n",
            "Val Loss: 370.8548 Acc: 0.3556\n",
            "\n",
            "Epoch 8/10\n",
            "Train Loss: 1.0998 Acc: 0.3267\n",
            "Val Loss: 5.9911 Acc: 0.3333\n",
            "\n",
            "Epoch 9/10\n",
            "Train Loss: 1.0972 Acc: 0.3333\n",
            "Val Loss: 1.0989 Acc: 0.3333\n",
            "\n",
            "Epoch 10/10\n",
            "Train Loss: 1.1033 Acc: 0.3333\n",
            "Val Loss: 1.0989 Acc: 0.3333\n",
            "\n",
            "\n",
            "Testing parameters: {'batch_size': 64, 'dropout_rate': 0.5, 'epochs': 10, 'learning_rate': 0.1, 'weight_decay': 0.0001}\n",
            "Epoch 1/10\n",
            "Train Loss: 1.1998 Acc: 0.3997\n",
            "Val Loss: 14085.4384 Acc: 0.3333\n",
            "\n",
            "Epoch 2/10\n",
            "Train Loss: 1.1392 Acc: 0.3300\n",
            "Val Loss: 3731745.7778 Acc: 0.3333\n",
            "\n",
            "Epoch 3/10\n",
            "Train Loss: 1.1162 Acc: 0.3118\n",
            "Val Loss: 8502.3216 Acc: 0.3667\n",
            "\n",
            "Epoch 4/10\n",
            "Train Loss: 1.1000 Acc: 0.3134\n",
            "Val Loss: 181.6130 Acc: 0.3222\n",
            "\n",
            "Epoch 5/10\n",
            "Train Loss: 1.1138 Acc: 0.3267\n",
            "Val Loss: 5.4264 Acc: 0.3333\n",
            "\n",
            "Epoch 6/10\n",
            "Train Loss: 1.1022 Acc: 0.3333\n",
            "Val Loss: 1.1118 Acc: 0.3333\n",
            "\n",
            "Epoch 7/10\n",
            "Train Loss: 1.1008 Acc: 0.3151\n",
            "Val Loss: 1.0988 Acc: 0.3333\n",
            "\n",
            "Epoch 8/10\n",
            "Train Loss: 1.0994 Acc: 0.3350\n",
            "Val Loss: 1.0991 Acc: 0.3333\n",
            "\n",
            "Epoch 9/10\n",
            "Train Loss: 1.1071 Acc: 0.3317\n",
            "Val Loss: 1.0988 Acc: 0.3333\n",
            "\n",
            "Epoch 10/10\n",
            "Train Loss: 1.0987 Acc: 0.3333\n",
            "Val Loss: 1.0988 Acc: 0.3333\n",
            "\n",
            "\n",
            "Testing parameters: {'batch_size': 64, 'dropout_rate': 0.5, 'epochs': 10, 'learning_rate': 0.1, 'weight_decay': 0.001}\n",
            "Epoch 1/10\n",
            "Train Loss: 1.2598 Acc: 0.4362\n",
            "Val Loss: 4161.4947 Acc: 0.3333\n",
            "\n",
            "Epoch 2/10\n",
            "Train Loss: 1.2745 Acc: 0.3416\n",
            "Val Loss: 195862938.3111 Acc: 0.3333\n",
            "\n",
            "Epoch 3/10\n",
            "Train Loss: 1.1426 Acc: 0.3532\n",
            "Val Loss: 3530825.9667 Acc: 0.3333\n",
            "\n",
            "Epoch 4/10\n",
            "Train Loss: 1.1079 Acc: 0.3118\n",
            "Val Loss: 2290223.6389 Acc: 0.3333\n",
            "\n",
            "Epoch 5/10\n",
            "Train Loss: 1.1271 Acc: 0.3118\n",
            "Val Loss: 106940.3503 Acc: 0.3333\n",
            "\n",
            "Epoch 6/10\n",
            "Train Loss: 1.1003 Acc: 0.3350\n",
            "Val Loss: 974.8405 Acc: 0.3333\n",
            "\n",
            "Epoch 7/10\n",
            "Train Loss: 1.0995 Acc: 0.3333\n",
            "Val Loss: 4.0421 Acc: 0.3222\n",
            "\n",
            "Epoch 8/10\n",
            "Train Loss: 1.0992 Acc: 0.3101\n",
            "Val Loss: 1.0986 Acc: 0.3333\n",
            "\n",
            "Epoch 9/10\n",
            "Train Loss: 1.0987 Acc: 0.3333\n",
            "Val Loss: 1.0986 Acc: 0.3333\n",
            "\n",
            "Epoch 10/10\n",
            "Train Loss: 1.0987 Acc: 0.3333\n",
            "Val Loss: 1.0986 Acc: 0.3333\n",
            "\n",
            "\n",
            "Testing parameters: {'batch_size': 64, 'dropout_rate': 0.5, 'epochs': 10, 'learning_rate': 0.01, 'weight_decay': 0}\n",
            "Epoch 1/10\n",
            "Train Loss: 1.0172 Acc: 0.4859\n",
            "Val Loss: 0.8335 Acc: 0.7222\n",
            "\n",
            "Epoch 2/10\n",
            "Train Loss: 0.9192 Acc: 0.6186\n",
            "Val Loss: 1.6280 Acc: 0.6444\n",
            "\n",
            "Epoch 3/10\n",
            "Train Loss: 0.9594 Acc: 0.5406\n",
            "Val Loss: 5.6385 Acc: 0.4556\n",
            "\n",
            "Epoch 4/10\n",
            "Train Loss: 1.0045 Acc: 0.4776\n",
            "Val Loss: 8.6952 Acc: 0.3444\n",
            "\n",
            "Epoch 5/10\n",
            "Train Loss: 1.0167 Acc: 0.4362\n",
            "Val Loss: 22.0571 Acc: 0.3333\n",
            "\n",
            "Epoch 6/10\n",
            "Train Loss: 0.9494 Acc: 0.4942\n",
            "Val Loss: 4.3545 Acc: 0.4333\n",
            "\n",
            "Epoch 7/10\n",
            "Train Loss: 0.9212 Acc: 0.5307\n",
            "Val Loss: 1.3025 Acc: 0.4111\n",
            "\n",
            "Epoch 8/10\n",
            "Train Loss: 0.8785 Acc: 0.5605\n",
            "Val Loss: 0.7798 Acc: 0.6222\n",
            "\n",
            "Epoch 9/10\n",
            "Train Loss: 0.8169 Acc: 0.6136\n",
            "Val Loss: 0.6915 Acc: 0.6444\n",
            "\n",
            "Epoch 10/10\n",
            "Train Loss: 0.7801 Acc: 0.6368\n",
            "Val Loss: 0.6851 Acc: 0.7000\n",
            "\n",
            "\n",
            "Testing parameters: {'batch_size': 64, 'dropout_rate': 0.5, 'epochs': 10, 'learning_rate': 0.01, 'weight_decay': 1e-05}\n",
            "Epoch 1/10\n",
            "Train Loss: 0.9875 Acc: 0.5473\n",
            "Val Loss: 1.0304 Acc: 0.7333\n",
            "\n",
            "Epoch 2/10\n",
            "Train Loss: 0.8808 Acc: 0.6269\n",
            "Val Loss: 17.4471 Acc: 0.4778\n",
            "\n",
            "Epoch 3/10\n",
            "Train Loss: 1.0207 Acc: 0.5622\n",
            "Val Loss: 7.7058 Acc: 0.3333\n",
            "\n",
            "Epoch 4/10\n",
            "Train Loss: 1.0027 Acc: 0.5357\n",
            "Val Loss: 27.3980 Acc: 0.1667\n",
            "\n",
            "Epoch 5/10\n",
            "Train Loss: 0.9543 Acc: 0.5158\n",
            "Val Loss: 20.1749 Acc: 0.3556\n",
            "\n",
            "Epoch 6/10\n",
            "Train Loss: 0.9535 Acc: 0.5406\n",
            "Val Loss: 1.6886 Acc: 0.4111\n",
            "\n",
            "Epoch 7/10\n",
            "Train Loss: 0.8956 Acc: 0.5605\n",
            "Val Loss: 0.9246 Acc: 0.5333\n",
            "\n",
            "Epoch 8/10\n",
            "Train Loss: 0.7949 Acc: 0.6219\n",
            "Val Loss: 0.6454 Acc: 0.6667\n",
            "\n",
            "Epoch 9/10\n",
            "Train Loss: 0.7800 Acc: 0.6318\n",
            "Val Loss: 0.6015 Acc: 0.7333\n",
            "\n",
            "Epoch 10/10\n",
            "Train Loss: 0.7483 Acc: 0.6484\n",
            "Val Loss: 0.6099 Acc: 0.7222\n",
            "\n",
            "\n",
            "Testing parameters: {'batch_size': 64, 'dropout_rate': 0.5, 'epochs': 10, 'learning_rate': 0.01, 'weight_decay': 0.0001}\n",
            "Epoch 1/10\n",
            "Train Loss: 0.9881 Acc: 0.5456\n",
            "Val Loss: 0.9856 Acc: 0.6667\n",
            "\n",
            "Epoch 2/10\n",
            "Train Loss: 0.8505 Acc: 0.6650\n",
            "Val Loss: 3.0238 Acc: 0.4667\n",
            "\n",
            "Epoch 3/10\n",
            "Train Loss: 1.0247 Acc: 0.5274\n",
            "Val Loss: 843.5641 Acc: 0.3333\n",
            "\n",
            "Epoch 4/10\n",
            "Train Loss: 0.9887 Acc: 0.5224\n",
            "Val Loss: 35.1026 Acc: 0.3333\n",
            "\n",
            "Epoch 5/10\n",
            "Train Loss: 0.9774 Acc: 0.5307\n",
            "Val Loss: 2.9539 Acc: 0.4111\n",
            "\n",
            "Epoch 6/10\n",
            "Train Loss: 0.8793 Acc: 0.5804\n",
            "Val Loss: 5.7190 Acc: 0.4889\n",
            "\n",
            "Epoch 7/10\n",
            "Train Loss: 0.8824 Acc: 0.5904\n",
            "Val Loss: 0.6134 Acc: 0.6889\n",
            "\n",
            "Epoch 8/10\n",
            "Train Loss: 0.7887 Acc: 0.6418\n",
            "Val Loss: 0.5752 Acc: 0.7111\n",
            "\n",
            "Epoch 9/10\n",
            "Train Loss: 0.7254 Acc: 0.6816\n",
            "Val Loss: 0.5699 Acc: 0.7333\n",
            "\n",
            "Epoch 10/10\n",
            "Train Loss: 0.6914 Acc: 0.6982\n",
            "Val Loss: 0.5674 Acc: 0.7222\n",
            "\n",
            "\n",
            "Testing parameters: {'batch_size': 64, 'dropout_rate': 0.5, 'epochs': 10, 'learning_rate': 0.01, 'weight_decay': 0.001}\n",
            "Epoch 1/10\n",
            "Train Loss: 1.0143 Acc: 0.4992\n",
            "Val Loss: 0.7270 Acc: 0.7111\n",
            "\n",
            "Epoch 2/10\n",
            "Train Loss: 0.8621 Acc: 0.6783\n",
            "Val Loss: 2.6986 Acc: 0.4889\n",
            "\n",
            "Epoch 3/10\n",
            "Train Loss: 1.0200 Acc: 0.5423\n",
            "Val Loss: 124.8310 Acc: 0.3333\n",
            "\n",
            "Epoch 4/10\n",
            "Train Loss: 0.9972 Acc: 0.5672\n",
            "Val Loss: 11.6631 Acc: 0.3333\n",
            "\n",
            "Epoch 5/10\n",
            "Train Loss: 0.9917 Acc: 0.5307\n",
            "Val Loss: 1.2924 Acc: 0.6000\n",
            "\n",
            "Epoch 6/10\n",
            "Train Loss: 0.9635 Acc: 0.5124\n",
            "Val Loss: 1.3690 Acc: 0.5222\n",
            "\n",
            "Epoch 7/10\n",
            "Train Loss: 0.9020 Acc: 0.5489\n",
            "Val Loss: 0.8650 Acc: 0.5222\n",
            "\n",
            "Epoch 8/10\n",
            "Train Loss: 0.8418 Acc: 0.5821\n",
            "Val Loss: 0.7538 Acc: 0.6667\n",
            "\n",
            "Epoch 9/10\n",
            "Train Loss: 0.8018 Acc: 0.6186\n",
            "Val Loss: 0.6437 Acc: 0.7000\n",
            "\n",
            "Epoch 10/10\n",
            "Train Loss: 0.7792 Acc: 0.6335\n",
            "Val Loss: 0.6261 Acc: 0.7111\n",
            "\n",
            "\n",
            "Testing parameters: {'batch_size': 64, 'dropout_rate': 0.5, 'epochs': 10, 'learning_rate': 0.001, 'weight_decay': 0}\n",
            "Epoch 1/10\n",
            "Train Loss: 1.1008 Acc: 0.3367\n",
            "Val Loss: 1.0560 Acc: 0.6111\n",
            "\n",
            "Epoch 2/10\n",
            "Train Loss: 1.0145 Acc: 0.5771\n",
            "Val Loss: 0.7543 Acc: 0.6444\n",
            "\n",
            "Epoch 3/10\n",
            "Train Loss: 0.6037 Acc: 0.7828\n",
            "Val Loss: 0.7038 Acc: 0.7778\n",
            "\n",
            "Epoch 4/10\n",
            "Train Loss: 0.4714 Acc: 0.8043\n",
            "Val Loss: 0.5015 Acc: 0.8333\n",
            "\n",
            "Epoch 5/10\n",
            "Train Loss: 0.4386 Acc: 0.8458\n",
            "Val Loss: 0.2497 Acc: 0.8889\n",
            "\n",
            "Epoch 6/10\n",
            "Train Loss: 0.3173 Acc: 0.8773\n",
            "Val Loss: 0.3371 Acc: 0.8889\n",
            "\n",
            "Epoch 7/10\n",
            "Train Loss: 0.2264 Acc: 0.9254\n",
            "Val Loss: 0.4916 Acc: 0.8556\n",
            "\n",
            "Epoch 8/10\n",
            "Train Loss: 0.2490 Acc: 0.8939\n",
            "Val Loss: 0.5069 Acc: 0.8667\n",
            "\n",
            "Epoch 9/10\n",
            "Train Loss: 0.1699 Acc: 0.9270\n",
            "Val Loss: 0.4587 Acc: 0.8556\n",
            "\n",
            "Epoch 10/10\n",
            "Train Loss: 0.1866 Acc: 0.9221\n",
            "Val Loss: 0.3989 Acc: 0.8778\n",
            "\n",
            "\n",
            "Testing parameters: {'batch_size': 64, 'dropout_rate': 0.5, 'epochs': 10, 'learning_rate': 0.001, 'weight_decay': 1e-05}\n",
            "Epoch 1/10\n",
            "Train Loss: 1.0880 Acc: 0.4030\n",
            "Val Loss: 1.0497 Acc: 0.6222\n",
            "\n",
            "Epoch 2/10\n",
            "Train Loss: 1.0065 Acc: 0.5854\n",
            "Val Loss: 0.6541 Acc: 0.6889\n",
            "\n",
            "Epoch 3/10\n",
            "Train Loss: 0.6093 Acc: 0.7562\n",
            "Val Loss: 0.9989 Acc: 0.8000\n",
            "\n",
            "Epoch 4/10\n",
            "Train Loss: 0.5126 Acc: 0.7960\n",
            "Val Loss: 0.5786 Acc: 0.8222\n",
            "\n",
            "Epoch 5/10\n",
            "Train Loss: 0.4612 Acc: 0.8242\n",
            "Val Loss: 0.6142 Acc: 0.7667\n",
            "\n",
            "Epoch 6/10\n",
            "Train Loss: 0.3386 Acc: 0.8740\n",
            "Val Loss: 0.5575 Acc: 0.8222\n",
            "\n",
            "Epoch 7/10\n",
            "Train Loss: 0.2329 Acc: 0.9171\n",
            "Val Loss: 0.3497 Acc: 0.8667\n",
            "\n",
            "Epoch 8/10\n",
            "Train Loss: 0.1965 Acc: 0.9221\n",
            "Val Loss: 0.3225 Acc: 0.8889\n",
            "\n",
            "Epoch 9/10\n",
            "Train Loss: 0.1898 Acc: 0.9254\n",
            "Val Loss: 0.3246 Acc: 0.9000\n",
            "\n",
            "Epoch 10/10\n",
            "Train Loss: 0.2079 Acc: 0.9171\n",
            "Val Loss: 0.3606 Acc: 0.8778\n",
            "\n",
            "\n",
            "Testing parameters: {'batch_size': 64, 'dropout_rate': 0.5, 'epochs': 10, 'learning_rate': 0.001, 'weight_decay': 0.0001}\n",
            "Epoch 1/10\n",
            "Train Loss: 1.0996 Acc: 0.3267\n",
            "Val Loss: 1.0699 Acc: 0.5222\n",
            "\n",
            "Epoch 2/10\n",
            "Train Loss: 1.0035 Acc: 0.5821\n",
            "Val Loss: 0.6643 Acc: 0.7556\n",
            "\n",
            "Epoch 3/10\n",
            "Train Loss: 0.6267 Acc: 0.7529\n",
            "Val Loss: 0.5401 Acc: 0.8667\n",
            "\n",
            "Epoch 4/10\n",
            "Train Loss: 0.5358 Acc: 0.8010\n",
            "Val Loss: 0.5405 Acc: 0.8333\n",
            "\n",
            "Epoch 5/10\n",
            "Train Loss: 0.4543 Acc: 0.8209\n",
            "Val Loss: 0.4733 Acc: 0.8222\n",
            "\n",
            "Epoch 6/10\n",
            "Train Loss: 0.3073 Acc: 0.8839\n",
            "Val Loss: 0.4300 Acc: 0.8778\n",
            "\n",
            "Epoch 7/10\n",
            "Train Loss: 0.2440 Acc: 0.9005\n",
            "Val Loss: 0.3569 Acc: 0.8778\n",
            "\n",
            "Epoch 8/10\n",
            "Train Loss: 0.2284 Acc: 0.9121\n",
            "Val Loss: 0.3045 Acc: 0.8889\n",
            "\n",
            "Epoch 9/10\n",
            "Train Loss: 0.1816 Acc: 0.9303\n",
            "Val Loss: 0.3206 Acc: 0.8778\n",
            "\n",
            "Epoch 10/10\n",
            "Train Loss: 0.1723 Acc: 0.9254\n",
            "Val Loss: 0.3236 Acc: 0.8778\n",
            "\n",
            "\n",
            "Testing parameters: {'batch_size': 64, 'dropout_rate': 0.5, 'epochs': 10, 'learning_rate': 0.001, 'weight_decay': 0.001}\n",
            "Epoch 1/10\n",
            "Train Loss: 1.0995 Acc: 0.3449\n",
            "Val Loss: 1.0611 Acc: 0.5556\n",
            "\n",
            "Epoch 2/10\n",
            "Train Loss: 1.0060 Acc: 0.5837\n",
            "Val Loss: 0.6631 Acc: 0.8000\n",
            "\n",
            "Epoch 3/10\n",
            "Train Loss: 0.6410 Acc: 0.7595\n",
            "Val Loss: 0.6158 Acc: 0.8444\n",
            "\n",
            "Epoch 4/10\n",
            "Train Loss: 0.5442 Acc: 0.7993\n",
            "Val Loss: 1.1140 Acc: 0.7333\n",
            "\n",
            "Epoch 5/10\n",
            "Train Loss: 0.4438 Acc: 0.8292\n",
            "Val Loss: 0.5493 Acc: 0.8333\n",
            "\n",
            "Epoch 6/10\n",
            "Train Loss: 0.3325 Acc: 0.8706\n",
            "Val Loss: 0.4061 Acc: 0.8778\n",
            "\n",
            "Epoch 7/10\n",
            "Train Loss: 0.2885 Acc: 0.8955\n",
            "Val Loss: 0.5108 Acc: 0.8444\n",
            "\n",
            "Epoch 8/10\n",
            "Train Loss: 0.2500 Acc: 0.9005\n",
            "Val Loss: 0.4528 Acc: 0.8444\n",
            "\n",
            "Epoch 9/10\n",
            "Train Loss: 0.2019 Acc: 0.9204\n",
            "Val Loss: 0.4250 Acc: 0.8444\n",
            "\n",
            "Epoch 10/10\n",
            "Train Loss: 0.1935 Acc: 0.9337\n",
            "Val Loss: 0.4228 Acc: 0.8444\n",
            "\n",
            "\n",
            "Hyperparameter Tuning Results:\n",
            "     Learning Rate  Batch Size  Epochs  Dropout Rate  Weight Decay  \\\n",
            "0            0.100          16      10           0.1       0.00000   \n",
            "1            0.100          16      10           0.1       0.00001   \n",
            "2            0.100          16      10           0.1       0.00010   \n",
            "3            0.100          16      10           0.1       0.00100   \n",
            "4            0.010          16      10           0.1       0.00000   \n",
            "..             ...         ...     ...           ...           ...   \n",
            "103          0.010          64      10           0.5       0.00100   \n",
            "104          0.001          64      10           0.5       0.00000   \n",
            "105          0.001          64      10           0.5       0.00001   \n",
            "106          0.001          64      10           0.5       0.00010   \n",
            "107          0.001          64      10           0.5       0.00100   \n",
            "\n",
            "     Train Accuracy  Validation Accuracy  \n",
            "0          0.333333             0.366667  \n",
            "1          0.333333             0.366667  \n",
            "2          0.333333             0.333333  \n",
            "3          0.333333             0.333333  \n",
            "4          0.436153             0.577778  \n",
            "..              ...                  ...  \n",
            "103        0.633499             0.711111  \n",
            "104        0.922056             0.888889  \n",
            "105        0.917081             0.900000  \n",
            "106        0.925373             0.888889  \n",
            "107        0.933665             0.877778  \n",
            "\n",
            "[108 rows x 7 columns]\n",
            "\n",
            "Best Validation Accuracy: 0.9333\n",
            "Best Parameters: {'batch_size': 16, 'dropout_rate': 0.5, 'epochs': 10, 'learning_rate': 0.001, 'weight_decay': 1e-05}\n",
            "Best model saved as 'best_model_tuned.pth'\n"
          ]
        }
      ],
      "source": [
        "# Define streamlined hyperparameter grid\n",
        "param_grid = {\n",
        "    'learning_rate': [0.1, 0.01, 0.001],\n",
        "    'batch_size': [16, 32, 64],\n",
        "    'epochs': [10],\n",
        "    'dropout_rate': [0.1, 0.3, 0.5],\n",
        "    'weight_decay': [0, 1e-5, 1e-4, 1e-3]\n",
        "}\n",
        "\n",
        "# Hyperparameter tuning loop with table logging and final evaluation\n",
        "results = []\n",
        "best_val_acc = 0.0\n",
        "best_params = None\n",
        "best_model_state = None\n",
        "\n",
        "print(\"Starting Hyperparameter Tuning...\")\n",
        "for params in ParameterGrid(param_grid):\n",
        "    print(f\"\\nTesting parameters: {params}\")\n",
        "\n",
        "    # Load dataset with current batch size\n",
        "    dataloaders = {\n",
        "        'train': DataLoader(image_datasets['train'], batch_size=params['batch_size'], shuffle=True, num_workers=2),\n",
        "        'val': DataLoader(image_datasets['val'], batch_size=params['batch_size'], shuffle=False, num_workers=2)\n",
        "    }\n",
        "\n",
        "    # Load and configure model\n",
        "    model = models.efficientnet_b0(weights=models.EfficientNet_B0_Weights.IMAGENET1K_V1)\n",
        "    num_ftrs = model.classifier[1].in_features\n",
        "    model.classifier = nn.Sequential(\n",
        "        nn.Dropout(p=params['dropout_rate']),\n",
        "        nn.Linear(num_ftrs, 256),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(256, len(class_names))\n",
        "    )\n",
        "    model = model.to(device)\n",
        "\n",
        "    # Define loss, optimizer, and scheduler\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=params['learning_rate'], weight_decay=params['weight_decay'])\n",
        "    steps_per_epoch = len(dataloaders['train'])\n",
        "    total_steps = steps_per_epoch * params['epochs']  # Adjust total steps based on epochs\n",
        "    scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=params['learning_rate'], total_steps=total_steps)\n",
        "\n",
        "    # Train the model (using the original train_model function)\n",
        "    model, train_losses, train_accs, val_losses, val_accs = train_model(\n",
        "        model, criterion, optimizer, scheduler, num_epochs=params['epochs']\n",
        "    )\n",
        "\n",
        "    # Extract training accuracy from the last epoch\n",
        "    best_train_acc = train_accs[-1].item()\n",
        "\n",
        "    # Final evaluation using the best saved model\n",
        "    model.load_state_dict(torch.load('best_model.pth'))\n",
        "    model.eval()\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in dataloaders['val']:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    final_val_acc = correct / total\n",
        "\n",
        "    # Log results\n",
        "    result = {\n",
        "        'Learning Rate': params['learning_rate'],\n",
        "        'Batch Size': params['batch_size'],\n",
        "        'Epochs': params['epochs'],\n",
        "        'Dropout Rate': params['dropout_rate'],\n",
        "        'Weight Decay': params['weight_decay'],\n",
        "        'Train Accuracy': best_train_acc,\n",
        "        'Validation Accuracy': final_val_acc\n",
        "    }\n",
        "    results.append(result)\n",
        "\n",
        "    # Track the best model\n",
        "    if final_val_acc > best_val_acc:\n",
        "        best_val_acc = final_val_acc\n",
        "        best_params = params\n",
        "        best_model_state = model.state_dict()\n",
        "        torch.save(best_model_state, 'best_model_tuned.pth')\n",
        "\n",
        "# Create and display results table\n",
        "results_df = pd.DataFrame(results)\n",
        "results_df.to_csv('hyperparameter_tuning_results.csv', index=False)\n",
        "print(\"\\nHyperparameter Tuning Results:\")\n",
        "print(results_df)\n",
        "\n",
        "# Output best results\n",
        "print(f\"\\nBest Validation Accuracy: {best_val_acc:.4f}\")\n",
        "print(f\"Best Parameters: {best_params}\")\n",
        "print(\"Best model saved as 'best_model_tuned.pth'\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}